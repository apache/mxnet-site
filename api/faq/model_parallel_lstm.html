<!DOCTYPE html>
<html lang=" en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Parallel | Apache MXNet</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Model Parallel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A flexible and efficient library for deep learning." />
<meta property="og:description" content="A flexible and efficient library for deep learning." />
<link rel="canonical" href="https://mxnet.incubator.apache.org/api/faq/model_parallel_lstm" />
<meta property="og:url" content="https://mxnet.incubator.apache.org/api/faq/model_parallel_lstm" />
<meta property="og:site_name" content="Apache MXNet" />
<script type="application/ld+json">
{"url":"https://mxnet.incubator.apache.org/api/faq/model_parallel_lstm","headline":"Model Parallel","description":"A flexible and efficient library for deep learning.","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<script src="https://medium-widget.pixelpoint.io/widget.js"></script>
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://mxnet.incubator.apache.org/feed.xml" title="Apache MXNet" /><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

  <script src="/assets/js/clipboard.js"></script>
  <script src="/assets/js/copycode.js"></script>
</head>
<body><header class="site-header" role="banner">

  <script>
    $(document).ready(function () {

      // HEADER OPACITY LOGIC

      function opacity_header() {
        var value = "rgba(4,140,204," + ($(window).scrollTop() / 300 + 0.4) + ")"
        $('.site-header').css("background-color", value)
      }

      $(window).scroll(function () {
        opacity_header()
      })
      opacity_header();

      // MENU SELECTOR LOGIC
      $('.page-link').each( function () {
        if (window.location.href.includes(this.href)) {
          $(this).addClass("page-current");
        }
      });
    })
  </script>
  <div class="wrapper">
    <a class="site-title" rel="author" href="/"><img
            src="/assets/img/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/get_started">Get Started</a>
        <a class="page-link" href="/blog">Blog</a>
        <a class="page-link" href="/features">Features</a>
        <a class="page-link" href="/ecosystem">Ecosystem</a>
        <a class="page-link" href="/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
      </div>
    </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
    <script>

</script>
<article class="post">

    <header class="post-header wrapper">
        <h1 class="post-title">Model Parallel</h1>
        <h3></h3></header>

    <div class="post-content">
        <div class="wrapper">
            <div class="row">
    <div class="col-3 docs-side-bar">
        <h3 style="text-transform: capitalize; padding-left:10px">faq</h3>
        <ul>
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/add_op_in_backend">Exception Handling in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/caffe">Convert from Caffe to MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/cloud">MXNet on the Cloud</a></li>
              <!-- page-category -->
            
            
            <li><a href="/api/faq/distributed_training">Distributed Training in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/env_var">Environment Variables</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/floa16">Float16</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/model_parallel_lstm">Model Parallel</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/new_op">Create New Operators</a></li>
              <!-- page-category -->
            
            
            <li><a href="/api/faq/nnpack">NNPACK for Multi-Core CPU Support in MXNet</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/perf">Some Tips for Improving MXNet Performance</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/recordio">Create a Dataset Using RecordIO</a></li>
              <!-- page-category -->
            
            
            <li><a href="/api/faq/s3_integration">Use data from S3 for training</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/security">MXNet Security Best Practices</a></li>
              <!-- page-category -->
            
            
            <li><a href="/api/faq/smart_device">Deep Learning at the Edge</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
            
            
            <li><a href="/api/faq/visualize_graph">Visualize Neural Networks</a></li>
              <!-- page-category -->
            
            
            <li><a href="/api/faq/why_mxnet">Why MXNet came to be?</a></li>
              <!-- page-category -->
            
              <!-- page-category -->
            
              <!-- page-category -->
               <!-- resource-p -->
        </ul>
    </div>
    <div class="col-9">
        <h1 id="training-with-multiple-gpus-using-model-parallelism">Training with Multiple GPUs Using Model Parallelism</h1>

<p>Training deep learning models can be resource intensive.
Even with a powerful GPU, some models can take days or weeks to train.
Large long short-term memory (LSTM) recurrent neural networks
can be especially slow to train,
with each layer, at each time step, requiring eight matrix multiplications.
Fortunately, given cloud services like AWS,
machine learning practitioners often  have access
to multiple machines and multiple GPUs.
One key strength of <em>MXNet</em> is its ability to leverage
powerful heterogeneous hardware environments to achieve significant speedups.</p>

<p>There are two primary ways that we can spread a workload across multiple devices.
In a previous document, <a href="./multi_devices.md">we addressed data parallelism</a>,
an approach in which samples within a batch are divided among the available devices.
With data parallelism, each device stores a complete copy of the model.
Here, we explore <em>model parallelism</em>, a different approach.
Instead of splitting the batch among the devices, we partition the model itself.
Most commonly, we achieve model parallelism by assigning the parameters (and computation)
of different layers of the network to different devices.</p>

<p>In particular, we will focus on LSTM recurrent networks.
LSTMS are powerful sequence models, that have proven especially useful
for <a href="https://arxiv.org/pdf/1409.0473.pdf">natural language translation</a>, <a href="https://arxiv.org/abs/1512.02595">speech recognition</a>,
and working with <a href="https://arxiv.org/abs/1511.03677">time series data</a>.
For a general high-level introduction to LSTMs,
see the excellent <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">tutorial</a> by Christopher Olah.</p>

<h2 id="model-parallelism-using-multiple-gpus-as-a-pipeline">Model Parallelism: Using Multiple GPUs As a Pipeline</h2>

<p>Model parallelism in deep learning was first proposed
for the <em>extraordinarily large</em> convolutional layer in GoogleNet.
From this implementation, we take the idea of placing each layer on a separate GPU.
Using model parallelism in such a layer-wise fashion
provides the benefit that no GPU has to maintain all of the model parameters in memory.</p>

<p><img width="517" alt="screen shot 2016-05-06 at 10 13 16 pm" src="https://cloud.githubusercontent.com/assets/5545640/15089697/d6f4fca0-13d7-11e6-9331-7f94fcc7b4c6.png"></p>

<p>In the preceding figure, each LSTM layer is assigned to a different GPU.
After GPU 1 finishes computing layer 1 for the first sentence, it passes its output to GPU 2.
At the same time, GPU 1 fetches the next sentence and starts training.
This differs significantly from data parallelism.
Here, there is no contention to update the shared model at the end of each iteration,
and most of the communication happens when passing intermediate results between GPUs.</p>

<h2 id="workload-partitioning">Workload Partitioning</h2>

<p>Implementing model parallelism requires knowledge of the training task.
Here are some general heuristics that we find useful:</p>

<ul>
<li>To minimize communication time, place neighboring layers on the same GPUs.</li>
<li>Be careful to balance the workload between GPUs.</li>
<li>Remember that different kinds of layers have different computation-memory properties.</li>
</ul>

<p><img width="449" alt="screen shot 2016-05-07 at 1 51 02 am" src="https://cloud.githubusercontent.com/assets/5545640/15090455/37a30ab0-13f6-11e6-863b-efe2b10ec2e6.png"></p>

<p>Let&#39;s take a quick look at the two pipelines in the preceding diagram.
They both have eight layers with a decoder and an encoder layer.
Based on our first principle, it&#39;s unwise to place all neighboring layers on separate GPUs.
We also want to balance the workload across GPUs.
Although the LSTM layers consume less memory than the decoder/encoder layers, they consume more computation time because of the dependency of the unrolled LSTM.
Thus, the partition on the left will be faster than the one on the right
because the workload is more evenly distributed.</p>

    </div>
</div>
        </div>
    </div>

</article>

</main><footer class="site-footer h-card">
    <data class="u-url" href="/%0A%20%20%20%20/"></data>

    <div class="wrapper">


        <div class="row">
            <div class="col-4">
                <h4>Resources</h4>
                <ul class="contact-list">
                    <li><a class="u-email" href="mailto:dev@mxnet.apache.org">Dev list</a></li>
                    <li><a class="u-email" href="mailto:user@mxnet.apache.org">User mailing list</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="https://issues.apache.org/jira/projects/MXNET/issues">Jira Tracker</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/labels/Roadmap">Github Roadmap</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>
<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="/assets/img/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span
                        style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright Â© 2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>




</body>

</html>
