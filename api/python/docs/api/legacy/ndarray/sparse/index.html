<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <style>
    .dropdown {
        position: relative;
        display: inline-block;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #f9f9f9;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
        padding: 12px 16px;
        z-index: 1;
        text-align: left;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown-option:hover {
        color: #FF4500;
    }

    .dropdown-option-active {
        color: #FF4500;
        font-weight: lighter;
    }

    .dropdown-option {
        color: #000000;
        font-weight: lighter;
    }

    .dropdown-header {
        color: #FFFFFF;
        display: inline-flex;
    }

    .dropdown-caret {
        width: 18px;
        height: 54px;
    }

    .dropdown-caret-path {
        fill: #FFFFFF;
    }
    </style>
    
    <title>ndarray.sparse &#8212; Apache MXNet  documentation</title>

    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mxnet.css" />
    <link rel="stylesheet" href="../../../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/feedback.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/google_analytics.js"></script>
    <script src="../../../../_static/autodoc.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../../../_static/sphinx_materialdesign_theme.js"></script>
    <link rel="shortcut icon" href="../../../../_static/mxnet-icon.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="ndarray.utils" href="../utils/index.html" />
    <link rel="prev" title="ndarray.register" href="../register/index.html" /> 
  </head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
      <a class="site-title" rel="author" href="/"><img
            src="../../../../_static/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/get_started">Get Started</a>
        <a class="page-link" href="/blog">Blog</a>
        <a class="page-link" href="/features">Features</a>
        <a class="page-link" href="/ecosystem">Ecosystem</a>
        <a class="page-link page-current" href="/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
        <div class="dropdown">
          <span class="dropdown-header">master
            <svg class="dropdown-caret" viewBox="0 0 32 32" class="icon icon-caret-bottom" aria-hidden="true"><path class="dropdown-caret-path" d="M24 11.305l-7.997 11.39L8 11.305z"></path></svg>
          </span>
          <div class="dropdown-content">
            <a class="dropdown-option-active" href="/versions/master/">master</a><br>
            <a class="dropdown-option" href="/versions/1.7.0/">1.7.0</a><br>
            <a class="dropdown-option" href="/versions/1.6.0/">1.6.0</a><br>
            <a class="dropdown-option" href="/versions/1.5.0/">1.5.0</a><br>
            <a class="dropdown-option" href="/versions/1.4.1/">1.4.1</a><br>
            <a class="dropdown-option" href="/versions/1.3.1/">1.3.1</a><br>
            <a class="dropdown-option" href="/versions/1.2.1/">1.2.1</a><br>
            <a class="dropdown-option" href="/versions/1.1.0/">1.1.0</a><br>
            <a class="dropdown-option" href="/versions/1.0.0/">1.0.0</a><br>
            <a class="dropdown-option" href="/versions/0.12.1/">0.12.1</a><br>
            <a class="dropdown-option" href="/versions/0.11.0/">0.11.0</a>
          </div>
        </div>
      </div>
    </nav>
  </div>
</header>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../../index.html">Python API</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="../../index.html">Legacy</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="../index.html">mxnet.ndarray</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">ndarray.sparse</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-github"
    href="https://github.com/apache/mxnet/edit/master/docs/python_docs/python/api/legacy/ndarray/sparse/index.rst" class="mdl-button mdl-js-button mdl-button--icon">
<i class="material-icons">edit</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-github">
Edit on Github
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/0-introduction.html">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/1-nparray.html">Step 1: Manipulate data with NP on MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/2-create-nn.html">Step 2: Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/3-autograd.html">Step 3: Automatic differentiation with autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/4-components.html">Step 4: Necessary components that are not in the network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html">Step 5: <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#Using-your-own-data-with-custom-Datasets">Using your own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#New-in-MXNet-2.0:-faster-C++-backend-dataloaders">New in MXNet 2.0: faster C++ backend dataloaders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/6-train-nn.html">Step 6: Train a Neural Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/7-use-gpus.html">Step 7: Load and Run a NN using GPU</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/index.html">Data Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/data_augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html">Gluon <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-custom-Datasets">Using own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader">Appendix: Upgrading from Module <code class="docutils literal notranslate"><span class="pre">DataIter</span></code> to Gluon <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/info_gan.html">Image similarity search with InfoGAN</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/legacy/index.html">Legacy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/np/index.html">What is NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/np/cheat-sheet.html">The NP on MXNet cheat sheet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/np/np-vs-numpy.html">Differences between NP on MXNet and NumPy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with ONEDNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/inference/image_classification_jetson.html">Image Classication using pretrained ResNet-50 model on Jetson module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/using_rtc">Using RTC for CUDA kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../np/index.html">mxnet.np</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../np/arrays.html">Array objects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../np/arrays.ndarray.html">The N-dimensional array (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/arrays.indexing.html">Indexing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../np/routines.html">Routines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.array-creation.html">Array creation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.eye.html">mxnet.np.eye</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.empty.html">mxnet.np.empty</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.full.html">mxnet.np.full</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.identity.html">mxnet.np.identity</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ones.html">mxnet.np.ones</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ones_like.html">mxnet.np.ones_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.zeros.html">mxnet.np.zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.zeros_like.html">mxnet.np.zeros_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.array.html">mxnet.np.array</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.copy.html">mxnet.np.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arange.html">mxnet.np.arange</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linspace.html">mxnet.np.linspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.logspace.html">mxnet.np.logspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.meshgrid.html">mxnet.np.meshgrid</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tril.html">mxnet.np.tril</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.array-manipulation.html">Array manipulation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ravel.html">mxnet.np.ravel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.swapaxes.html">mxnet.np.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.transpose.html">mxnet.np.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.moveaxis.html">mxnet.np.moveaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rollaxis.html">mxnet.np.rollaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.expand_dims.html">mxnet.np.expand_dims</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.squeeze.html">mxnet.np.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.broadcast_to.html">mxnet.np.broadcast_to</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.broadcast_arrays.html">mxnet.np.broadcast_arrays</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_1d.html">mxnet.np.atleast_1d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_2d.html">mxnet.np.atleast_2d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_3d.html">mxnet.np.atleast_3d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.concatenate.html">mxnet.np.concatenate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.stack.html">mxnet.np.stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dstack.html">mxnet.np.dstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vstack.html">mxnet.np.vstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.column_stack.html">mxnet.np.column_stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hstack.html">mxnet.np.hstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.split.html">mxnet.np.split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hsplit.html">mxnet.np.hsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vsplit.html">mxnet.np.vsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.array_split.html">mxnet.np.array_split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dsplit.html">mxnet.np.dsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tile.html">mxnet.np.tile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.repeat.html">mxnet.np.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.unique.html">mxnet.np.unique</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.delete.html">mxnet.np.delete</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.insert.html">mxnet.np.insert</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.append.html">mxnet.np.append</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.resize.html">mxnet.np.resize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trim_zeros.html">mxnet.np.trim_zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flip.html">mxnet.np.flip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.roll.html">mxnet.np.roll</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rot90.html">mxnet.np.rot90</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fliplr.html">mxnet.np.fliplr</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flipud.html">mxnet.np.flipud</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.io.html">Input and output</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.genfromtxt.html">mxnet.np.genfromtxt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.tolist.html">mxnet.np.ndarray.tolist</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.set_printoptions.html">mxnet.np.set_printoptions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.linalg.html">Linear algebra (<code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.linalg</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dot.html">mxnet.np.dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vdot.html">mxnet.np.vdot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.inner.html">mxnet.np.inner</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.outer.html">mxnet.np.outer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tensordot.html">mxnet.np.tensordot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.einsum.html">mxnet.np.einsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.multi_dot.html">mxnet.np.linalg.multi_dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.matmul.html">mxnet.np.matmul</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.matrix_power.html">mxnet.np.linalg.matrix_power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.kron.html">mxnet.np.kron</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.svd.html">mxnet.np.linalg.svd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.cholesky.html">mxnet.np.linalg.cholesky</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.qr.html">mxnet.np.linalg.qr</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eig.html">mxnet.np.linalg.eig</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigh.html">mxnet.np.linalg.eigh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigvals.html">mxnet.np.linalg.eigvals</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigvalsh.html">mxnet.np.linalg.eigvalsh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.norm.html">mxnet.np.linalg.norm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trace.html">mxnet.np.trace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.cond.html">mxnet.np.linalg.cond</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.det.html">mxnet.np.linalg.det</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.matrix_rank.html">mxnet.np.linalg.matrix_rank</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.slogdet.html">mxnet.np.linalg.slogdet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.solve.html">mxnet.np.linalg.solve</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.tensorsolve.html">mxnet.np.linalg.tensorsolve</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.lstsq.html">mxnet.np.linalg.lstsq</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.inv.html">mxnet.np.linalg.inv</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.pinv.html">mxnet.np.linalg.pinv</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.tensorinv.html">mxnet.np.linalg.tensorinv</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.math.html">Mathematical functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sin.html">mxnet.np.sin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cos.html">mxnet.np.cos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tan.html">mxnet.np.tan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arcsin.html">mxnet.np.arcsin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arccos.html">mxnet.np.arccos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctan.html">mxnet.np.arctan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.degrees.html">mxnet.np.degrees</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.radians.html">mxnet.np.radians</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hypot.html">mxnet.np.hypot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctan2.html">mxnet.np.arctan2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.deg2rad.html">mxnet.np.deg2rad</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rad2deg.html">mxnet.np.rad2deg</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.unwrap.html">mxnet.np.unwrap</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sinh.html">mxnet.np.sinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cosh.html">mxnet.np.cosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tanh.html">mxnet.np.tanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arcsinh.html">mxnet.np.arcsinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arccosh.html">mxnet.np.arccosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctanh.html">mxnet.np.arctanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rint.html">mxnet.np.rint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fix.html">mxnet.np.fix</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.floor.html">mxnet.np.floor</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ceil.html">mxnet.np.ceil</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trunc.html">mxnet.np.trunc</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.around.html">mxnet.np.around</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.round_.html">mxnet.np.round_</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sum.html">mxnet.np.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.prod.html">mxnet.np.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cumsum.html">mxnet.np.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanprod.html">mxnet.np.nanprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nansum.html">mxnet.np.nansum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cumprod.html">mxnet.np.cumprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nancumprod.html">mxnet.np.nancumprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nancumsum.html">mxnet.np.nancumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.diff.html">mxnet.np.diff</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ediff1d.html">mxnet.np.ediff1d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cross.html">mxnet.np.cross</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trapz.html">mxnet.np.trapz</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.exp.html">mxnet.np.exp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.expm1.html">mxnet.np.expm1</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log.html">mxnet.np.log</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log10.html">mxnet.np.log10</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log2.html">mxnet.np.log2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log1p.html">mxnet.np.log1p</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.i0.html">mxnet.np.i0</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ldexp.html">mxnet.np.ldexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.signbit.html">mxnet.np.signbit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.copysign.html">mxnet.np.copysign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.frexp.html">mxnet.np.frexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.spacing.html">mxnet.np.spacing</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.lcm.html">mxnet.np.lcm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.add.html">mxnet.np.add</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reciprocal.html">mxnet.np.reciprocal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.negative.html">mxnet.np.negative</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.divide.html">mxnet.np.divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.power.html">mxnet.np.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.subtract.html">mxnet.np.subtract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.mod.html">mxnet.np.mod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.multiply.html">mxnet.np.multiply</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.true_divide.html">mxnet.np.true_divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.remainder.html">mxnet.np.remainder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.positive.html">mxnet.np.positive</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.float_power.html">mxnet.np.float_power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmod.html">mxnet.np.fmod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.modf.html">mxnet.np.modf</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.divmod.html">mxnet.np.divmod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.clip.html">mxnet.np.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sqrt.html">mxnet.np.sqrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cbrt.html">mxnet.np.cbrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.square.html">mxnet.np.square</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.absolute.html">mxnet.np.absolute</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sign.html">mxnet.np.sign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.maximum.html">mxnet.np.maximum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.minimum.html">mxnet.np.minimum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fabs.html">mxnet.np.fabs</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.heaviside.html">mxnet.np.heaviside</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmax.html">mxnet.np.fmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmin.html">mxnet.np.fmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nan_to_num.html">mxnet.np.nan_to_num</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.interp.html">mxnet.np.interp</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/random/index.html">np.random</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.choice.html">mxnet.np.random.choice</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.shuffle.html">mxnet.np.random.shuffle</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.normal.html">mxnet.np.random.normal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.uniform.html">mxnet.np.random.uniform</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.rand.html">mxnet.np.random.rand</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.randint.html">mxnet.np.random.randint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.beta.html">mxnet.np.random.beta</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.chisquare.html">mxnet.np.random.chisquare</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.exponential.html">mxnet.np.random.exponential</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.f.html">mxnet.np.random.f</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.gamma.html">mxnet.np.random.gamma</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.gumbel.html">mxnet.np.random.gumbel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.laplace.html">mxnet.np.random.laplace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.logistic.html">mxnet.np.random.logistic</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.lognormal.html">mxnet.np.random.lognormal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.multinomial.html">mxnet.np.random.multinomial</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.multivariate_normal.html">mxnet.np.random.multivariate_normal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.pareto.html">mxnet.np.random.pareto</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.power.html">mxnet.np.random.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.rayleigh.html">mxnet.np.random.rayleigh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.weibull.html">mxnet.np.random.weibull</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.sort.html">Sorting, searching, and counting</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.sort.html">mxnet.np.ndarray.sort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sort.html">mxnet.np.sort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.lexsort.html">mxnet.np.lexsort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argsort.html">mxnet.np.argsort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.msort.html">mxnet.np.msort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.partition.html">mxnet.np.partition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argpartition.html">mxnet.np.argpartition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argmax.html">mxnet.np.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argmin.html">mxnet.np.argmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanargmax.html">mxnet.np.nanargmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanargmin.html">mxnet.np.nanargmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argwhere.html">mxnet.np.argwhere</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nonzero.html">mxnet.np.nonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flatnonzero.html">mxnet.np.flatnonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.where.html">mxnet.np.where</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.searchsorted.html">mxnet.np.searchsorted</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.extract.html">mxnet.np.extract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.count_nonzero.html">mxnet.np.count_nonzero</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.statistics.html">Statistics</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.min.html">mxnet.np.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.max.html">mxnet.np.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.amin.html">mxnet.np.amin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.amax.html">mxnet.np.amax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmin.html">mxnet.np.nanmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmax.html">mxnet.np.nanmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ptp.html">mxnet.np.ptp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.percentile.html">mxnet.np.percentile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanpercentile.html">mxnet.np.nanpercentile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.quantile.html">mxnet.np.quantile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanquantile.html">mxnet.np.nanquantile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.mean.html">mxnet.np.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.std.html">mxnet.np.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.var.html">mxnet.np.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.median.html">mxnet.np.median</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.average.html">mxnet.np.average</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmedian.html">mxnet.np.nanmedian</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanstd.html">mxnet.np.nanstd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanvar.html">mxnet.np.nanvar</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.corrcoef.html">mxnet.np.corrcoef</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.correlate.html">mxnet.np.correlate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cov.html">mxnet.np.cov</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram.html">mxnet.np.histogram</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram2d.html">mxnet.np.histogram2d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogramdd.html">mxnet.np.histogramdd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.bincount.html">mxnet.np.bincount</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram_bin_edges.html">mxnet.np.histogram_bin_edges</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.digitize.html">mxnet.np.digitize</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../npx/index.html">NPX: NumPy Neural Network Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.set_np.html">mxnet.npx.set_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.reset_np.html">mxnet.npx.reset_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.cpu.html">mxnet.npx.cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.cpu_pinned.html">mxnet.npx.cpu_pinned</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gpu.html">mxnet.npx.gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gpu_memory_info.html">mxnet.npx.gpu_memory_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.current_context.html">mxnet.npx.current_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.num_gpus.html">mxnet.npx.num_gpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.activation.html">mxnet.npx.activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_norm.html">mxnet.npx.batch_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.convolution.html">mxnet.npx.convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.dropout.html">mxnet.npx.dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.embedding.html">mxnet.npx.embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.fully_connected.html">mxnet.npx.fully_connected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.layer_norm.html">mxnet.npx.layer_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.pooling.html">mxnet.npx.pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.rnn.html">mxnet.npx.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.leaky_relu.html">mxnet.npx.leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_detection.html">mxnet.npx.multibox_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_prior.html">mxnet.npx.multibox_prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_target.html">mxnet.npx.multibox_target</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.roi_pooling.html">mxnet.npx.roi_pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.sigmoid.html">mxnet.npx.sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.relu.html">mxnet.npx.relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.smooth_l1.html">mxnet.npx.smooth_l1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.softmax.html">mxnet.npx.softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.log_softmax.html">mxnet.npx.log_softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.topk.html">mxnet.npx.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.waitall.html">mxnet.npx.waitall</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.load.html">mxnet.npx.load</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.save.html">mxnet.npx.save</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.one_hot.html">mxnet.npx.one_hot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.pick.html">mxnet.npx.pick</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.reshape_like.html">mxnet.npx.reshape_like</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_flatten.html">mxnet.npx.batch_flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_dot.html">mxnet.npx.batch_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gamma.html">mxnet.npx.gamma</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.sequence_mask.html">mxnet.npx.sequence_mask</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/metric/index.html">gluon.metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html">KVStore: Communication for Distributed Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#horovod">Horovod</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.Horovod.html">mxnet.kvstore.Horovod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#byteps">BytePS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.BytePS.html">mxnet.kvstore.BytePS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#kvstore-interface">KVStore Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStore.html">mxnet.kvstore.KVStore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStoreBase.html">mxnet.kvstore.KVStoreBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStoreServer.html">mxnet.kvstore.KVStoreServer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html">Legacy</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../io/index.html">mxnet.io</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">mxnet.ndarray</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../image/index.html">ndarray.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../op/index.html">ndarray.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../random/index.html">ndarray.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../register/index.html">ndarray.register</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">ndarray.sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/symbol.html">symbol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../context/index.html">mxnet.context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../runtime/index.html">mxnet.runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.Feature.html">mxnet.runtime.Feature</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.Features.html">mxnet.runtime.Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.feature_list.html">mxnet.runtime.feature_list</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../util/index.html">mxnet.util</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">
<header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/0-introduction.html">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/1-nparray.html">Step 1: Manipulate data with NP on MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/2-create-nn.html">Step 2: Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/3-autograd.html">Step 3: Automatic differentiation with autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/4-components.html">Step 4: Necessary components that are not in the network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html">Step 5: <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#Using-your-own-data-with-custom-Datasets">Using your own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/5-datasets.html#New-in-MXNet-2.0:-faster-C++-backend-dataloaders">New in MXNet 2.0: faster C++ backend dataloaders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/6-train-nn.html">Step 6: Train a Neural Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/crash-course/7-use-gpus.html">Step 7: Load and Run a NN using GPU</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/index.html">Data Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/data_augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html">Gluon <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-custom-Datasets">Using own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader">Appendix: Upgrading from Module <code class="docutils literal notranslate"><span class="pre">DataIter</span></code> to Gluon <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/info_gan.html">Image similarity search with InfoGAN</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/legacy/index.html">Legacy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../tutorials/packages/legacy/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/np/index.html">What is NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/np/cheat-sheet.html">The NP on MXNet cheat sheet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/np/np-vs-numpy.html">Differences between NP on MXNet and NumPy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with ONEDNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/inference/image_classification_jetson.html">Image Classication using pretrained ResNet-50 model on Jetson module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/using_rtc">Using RTC for CUDA kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../np/index.html">mxnet.np</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../np/arrays.html">Array objects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../np/arrays.ndarray.html">The N-dimensional array (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/arrays.indexing.html">Indexing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../np/routines.html">Routines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.array-creation.html">Array creation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.eye.html">mxnet.np.eye</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.empty.html">mxnet.np.empty</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.full.html">mxnet.np.full</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.identity.html">mxnet.np.identity</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ones.html">mxnet.np.ones</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ones_like.html">mxnet.np.ones_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.zeros.html">mxnet.np.zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.zeros_like.html">mxnet.np.zeros_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.array.html">mxnet.np.array</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.copy.html">mxnet.np.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arange.html">mxnet.np.arange</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linspace.html">mxnet.np.linspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.logspace.html">mxnet.np.logspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.meshgrid.html">mxnet.np.meshgrid</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tril.html">mxnet.np.tril</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.array-manipulation.html">Array manipulation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ravel.html">mxnet.np.ravel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.swapaxes.html">mxnet.np.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.transpose.html">mxnet.np.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.moveaxis.html">mxnet.np.moveaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rollaxis.html">mxnet.np.rollaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.expand_dims.html">mxnet.np.expand_dims</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.squeeze.html">mxnet.np.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.broadcast_to.html">mxnet.np.broadcast_to</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.broadcast_arrays.html">mxnet.np.broadcast_arrays</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_1d.html">mxnet.np.atleast_1d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_2d.html">mxnet.np.atleast_2d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.atleast_3d.html">mxnet.np.atleast_3d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.concatenate.html">mxnet.np.concatenate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.stack.html">mxnet.np.stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dstack.html">mxnet.np.dstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vstack.html">mxnet.np.vstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.column_stack.html">mxnet.np.column_stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hstack.html">mxnet.np.hstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.split.html">mxnet.np.split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hsplit.html">mxnet.np.hsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vsplit.html">mxnet.np.vsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.array_split.html">mxnet.np.array_split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dsplit.html">mxnet.np.dsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tile.html">mxnet.np.tile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.repeat.html">mxnet.np.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.unique.html">mxnet.np.unique</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.delete.html">mxnet.np.delete</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.insert.html">mxnet.np.insert</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.append.html">mxnet.np.append</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.resize.html">mxnet.np.resize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trim_zeros.html">mxnet.np.trim_zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flip.html">mxnet.np.flip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.roll.html">mxnet.np.roll</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rot90.html">mxnet.np.rot90</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fliplr.html">mxnet.np.fliplr</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flipud.html">mxnet.np.flipud</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.io.html">Input and output</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.genfromtxt.html">mxnet.np.genfromtxt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.tolist.html">mxnet.np.ndarray.tolist</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.set_printoptions.html">mxnet.np.set_printoptions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.linalg.html">Linear algebra (<code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.linalg</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.dot.html">mxnet.np.dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.vdot.html">mxnet.np.vdot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.inner.html">mxnet.np.inner</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.outer.html">mxnet.np.outer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tensordot.html">mxnet.np.tensordot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.einsum.html">mxnet.np.einsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.multi_dot.html">mxnet.np.linalg.multi_dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.matmul.html">mxnet.np.matmul</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.matrix_power.html">mxnet.np.linalg.matrix_power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.kron.html">mxnet.np.kron</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.svd.html">mxnet.np.linalg.svd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.cholesky.html">mxnet.np.linalg.cholesky</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.qr.html">mxnet.np.linalg.qr</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eig.html">mxnet.np.linalg.eig</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigh.html">mxnet.np.linalg.eigh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigvals.html">mxnet.np.linalg.eigvals</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.eigvalsh.html">mxnet.np.linalg.eigvalsh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.norm.html">mxnet.np.linalg.norm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trace.html">mxnet.np.trace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.cond.html">mxnet.np.linalg.cond</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.det.html">mxnet.np.linalg.det</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.matrix_rank.html">mxnet.np.linalg.matrix_rank</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.slogdet.html">mxnet.np.linalg.slogdet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.solve.html">mxnet.np.linalg.solve</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.tensorsolve.html">mxnet.np.linalg.tensorsolve</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.lstsq.html">mxnet.np.linalg.lstsq</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.inv.html">mxnet.np.linalg.inv</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.pinv.html">mxnet.np.linalg.pinv</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.linalg.tensorinv.html">mxnet.np.linalg.tensorinv</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.math.html">Mathematical functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sin.html">mxnet.np.sin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cos.html">mxnet.np.cos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tan.html">mxnet.np.tan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arcsin.html">mxnet.np.arcsin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arccos.html">mxnet.np.arccos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctan.html">mxnet.np.arctan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.degrees.html">mxnet.np.degrees</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.radians.html">mxnet.np.radians</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.hypot.html">mxnet.np.hypot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctan2.html">mxnet.np.arctan2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.deg2rad.html">mxnet.np.deg2rad</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rad2deg.html">mxnet.np.rad2deg</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.unwrap.html">mxnet.np.unwrap</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sinh.html">mxnet.np.sinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cosh.html">mxnet.np.cosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.tanh.html">mxnet.np.tanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arcsinh.html">mxnet.np.arcsinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arccosh.html">mxnet.np.arccosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.arctanh.html">mxnet.np.arctanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.rint.html">mxnet.np.rint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fix.html">mxnet.np.fix</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.floor.html">mxnet.np.floor</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ceil.html">mxnet.np.ceil</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trunc.html">mxnet.np.trunc</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.around.html">mxnet.np.around</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.round_.html">mxnet.np.round_</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sum.html">mxnet.np.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.prod.html">mxnet.np.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cumsum.html">mxnet.np.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanprod.html">mxnet.np.nanprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nansum.html">mxnet.np.nansum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cumprod.html">mxnet.np.cumprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nancumprod.html">mxnet.np.nancumprod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nancumsum.html">mxnet.np.nancumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.diff.html">mxnet.np.diff</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ediff1d.html">mxnet.np.ediff1d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cross.html">mxnet.np.cross</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.trapz.html">mxnet.np.trapz</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.exp.html">mxnet.np.exp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.expm1.html">mxnet.np.expm1</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log.html">mxnet.np.log</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log10.html">mxnet.np.log10</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log2.html">mxnet.np.log2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.log1p.html">mxnet.np.log1p</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.i0.html">mxnet.np.i0</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ldexp.html">mxnet.np.ldexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.signbit.html">mxnet.np.signbit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.copysign.html">mxnet.np.copysign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.frexp.html">mxnet.np.frexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.spacing.html">mxnet.np.spacing</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.lcm.html">mxnet.np.lcm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.add.html">mxnet.np.add</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.reciprocal.html">mxnet.np.reciprocal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.negative.html">mxnet.np.negative</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.divide.html">mxnet.np.divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.power.html">mxnet.np.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.subtract.html">mxnet.np.subtract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.mod.html">mxnet.np.mod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.multiply.html">mxnet.np.multiply</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.true_divide.html">mxnet.np.true_divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.remainder.html">mxnet.np.remainder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.positive.html">mxnet.np.positive</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.float_power.html">mxnet.np.float_power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmod.html">mxnet.np.fmod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.modf.html">mxnet.np.modf</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.divmod.html">mxnet.np.divmod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.clip.html">mxnet.np.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sqrt.html">mxnet.np.sqrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cbrt.html">mxnet.np.cbrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.square.html">mxnet.np.square</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.absolute.html">mxnet.np.absolute</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sign.html">mxnet.np.sign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.maximum.html">mxnet.np.maximum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.minimum.html">mxnet.np.minimum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fabs.html">mxnet.np.fabs</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.heaviside.html">mxnet.np.heaviside</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmax.html">mxnet.np.fmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.fmin.html">mxnet.np.fmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nan_to_num.html">mxnet.np.nan_to_num</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.interp.html">mxnet.np.interp</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/random/index.html">np.random</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.choice.html">mxnet.np.random.choice</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.shuffle.html">mxnet.np.random.shuffle</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.normal.html">mxnet.np.random.normal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.uniform.html">mxnet.np.random.uniform</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.rand.html">mxnet.np.random.rand</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.randint.html">mxnet.np.random.randint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.beta.html">mxnet.np.random.beta</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.chisquare.html">mxnet.np.random.chisquare</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.exponential.html">mxnet.np.random.exponential</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.f.html">mxnet.np.random.f</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.gamma.html">mxnet.np.random.gamma</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.gumbel.html">mxnet.np.random.gumbel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.laplace.html">mxnet.np.random.laplace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.logistic.html">mxnet.np.random.logistic</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.lognormal.html">mxnet.np.random.lognormal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.multinomial.html">mxnet.np.random.multinomial</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.multivariate_normal.html">mxnet.np.random.multivariate_normal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.pareto.html">mxnet.np.random.pareto</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.power.html">mxnet.np.random.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.rayleigh.html">mxnet.np.random.rayleigh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/random/generated/mxnet.np.random.weibull.html">mxnet.np.random.weibull</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.sort.html">Sorting, searching, and counting</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ndarray.sort.html">mxnet.np.ndarray.sort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.sort.html">mxnet.np.sort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.lexsort.html">mxnet.np.lexsort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argsort.html">mxnet.np.argsort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.msort.html">mxnet.np.msort</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.partition.html">mxnet.np.partition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argpartition.html">mxnet.np.argpartition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argmax.html">mxnet.np.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argmin.html">mxnet.np.argmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanargmax.html">mxnet.np.nanargmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanargmin.html">mxnet.np.nanargmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.argwhere.html">mxnet.np.argwhere</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nonzero.html">mxnet.np.nonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.flatnonzero.html">mxnet.np.flatnonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.where.html">mxnet.np.where</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.searchsorted.html">mxnet.np.searchsorted</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.extract.html">mxnet.np.extract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.count_nonzero.html">mxnet.np.count_nonzero</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../np/routines.statistics.html">Statistics</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.min.html">mxnet.np.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.max.html">mxnet.np.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.amin.html">mxnet.np.amin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.amax.html">mxnet.np.amax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmin.html">mxnet.np.nanmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmax.html">mxnet.np.nanmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.ptp.html">mxnet.np.ptp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.percentile.html">mxnet.np.percentile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanpercentile.html">mxnet.np.nanpercentile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.quantile.html">mxnet.np.quantile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanquantile.html">mxnet.np.nanquantile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.mean.html">mxnet.np.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.std.html">mxnet.np.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.var.html">mxnet.np.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.median.html">mxnet.np.median</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.average.html">mxnet.np.average</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanmedian.html">mxnet.np.nanmedian</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanstd.html">mxnet.np.nanstd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.nanvar.html">mxnet.np.nanvar</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.corrcoef.html">mxnet.np.corrcoef</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.correlate.html">mxnet.np.correlate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.cov.html">mxnet.np.cov</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram.html">mxnet.np.histogram</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram2d.html">mxnet.np.histogram2d</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogramdd.html">mxnet.np.histogramdd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.bincount.html">mxnet.np.bincount</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.histogram_bin_edges.html">mxnet.np.histogram_bin_edges</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../np/generated/mxnet.np.digitize.html">mxnet.np.digitize</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../npx/index.html">NPX: NumPy Neural Network Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.set_np.html">mxnet.npx.set_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.reset_np.html">mxnet.npx.reset_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.cpu.html">mxnet.npx.cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.cpu_pinned.html">mxnet.npx.cpu_pinned</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gpu.html">mxnet.npx.gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gpu_memory_info.html">mxnet.npx.gpu_memory_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.current_context.html">mxnet.npx.current_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.num_gpus.html">mxnet.npx.num_gpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.activation.html">mxnet.npx.activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_norm.html">mxnet.npx.batch_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.convolution.html">mxnet.npx.convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.dropout.html">mxnet.npx.dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.embedding.html">mxnet.npx.embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.fully_connected.html">mxnet.npx.fully_connected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.layer_norm.html">mxnet.npx.layer_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.pooling.html">mxnet.npx.pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.rnn.html">mxnet.npx.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.leaky_relu.html">mxnet.npx.leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_detection.html">mxnet.npx.multibox_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_prior.html">mxnet.npx.multibox_prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.multibox_target.html">mxnet.npx.multibox_target</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.roi_pooling.html">mxnet.npx.roi_pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.sigmoid.html">mxnet.npx.sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.relu.html">mxnet.npx.relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.smooth_l1.html">mxnet.npx.smooth_l1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.softmax.html">mxnet.npx.softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.log_softmax.html">mxnet.npx.log_softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.topk.html">mxnet.npx.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.waitall.html">mxnet.npx.waitall</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.load.html">mxnet.npx.load</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.save.html">mxnet.npx.save</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.one_hot.html">mxnet.npx.one_hot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.pick.html">mxnet.npx.pick</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.reshape_like.html">mxnet.npx.reshape_like</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_flatten.html">mxnet.npx.batch_flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.batch_dot.html">mxnet.npx.batch_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.gamma.html">mxnet.npx.gamma</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../npx/generated/mxnet.npx.sequence_mask.html">mxnet.npx.sequence_mask</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/metric/index.html">gluon.metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html">KVStore: Communication for Distributed Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#horovod">Horovod</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.Horovod.html">mxnet.kvstore.Horovod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#byteps">BytePS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.BytePS.html">mxnet.kvstore.BytePS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore/index.html#kvstore-interface">KVStore Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStore.html">mxnet.kvstore.KVStore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStoreBase.html">mxnet.kvstore.KVStoreBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../kvstore/generated/mxnet.kvstore.KVStoreServer.html">mxnet.kvstore.KVStoreServer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html">Legacy</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../io/index.html">mxnet.io</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">mxnet.ndarray</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../image/index.html">ndarray.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../op/index.html">ndarray.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../random/index.html">ndarray.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../register/index.html">ndarray.register</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">ndarray.sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/symbol.html">symbol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../context/index.html">mxnet.context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../runtime/index.html">mxnet.runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.Feature.html">mxnet.runtime.Feature</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.Features.html">mxnet.runtime.Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../runtime/generated/mxnet.runtime.feature_list.html">mxnet.runtime.feature_list</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../util/index.html">mxnet.util</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="module-mxnet.ndarray.sparse">
<span id="ndarray-sparse"></span><h1>ndarray.sparse<a class="headerlink" href="#module-mxnet.ndarray.sparse" title="Permalink to this headline"></a></h1>
<p>Sparse NDArray API of MXNet.</p>
<p><strong>Functions</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.csr_matrix" title="mxnet.ndarray.sparse.csr_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">csr_matrix</span></code></a>(arg1[,shape,ctx,dtype])</p></td>
<td><p>Creates a <cite>CSRNDArray</cite>, an 2D array with compressed sparse row (CSR) format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.row_sparse_array" title="mxnet.ndarray.sparse.row_sparse_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">row_sparse_array</span></code></a>(arg1[,shape,ctx,dtype])</p></td>
<td><p>Creates a <cite>RowSparseNDArray</cite>, a multidimensional row sparse array with a set of     tensor slices at given indices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.add" title="mxnet.ndarray.sparse.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a>(lhs,rhs)</p></td>
<td><p>Returns element-wise sum of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.subtract" title="mxnet.ndarray.sparse.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract</span></code></a>(lhs,rhs)</p></td>
<td><p>Returns element-wise difference of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.multiply" title="mxnet.ndarray.sparse.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply</span></code></a>(lhs,rhs)</p></td>
<td><p>Returns element-wise product of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.divide" title="mxnet.ndarray.sparse.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide</span></code></a>(lhs,rhs)</p></td>
<td><p>Returns element-wise division of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.ElementWiseSum" title="mxnet.ndarray.sparse.ElementWiseSum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElementWiseSum</span></code></a>(*args,**kwargs)</p></td>
<td><p>Adds all input arguments element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.Embedding" title="mxnet.ndarray.sparse.Embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Embedding</span></code></a>([data,weight,input_dim,])</p></td>
<td><p>Maps integer indices to vector representations (embeddings).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.FullyConnected" title="mxnet.ndarray.sparse.FullyConnected"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FullyConnected</span></code></a>([data,weight,bias,])</p></td>
<td><p>Applies a linear transformation: <span class="math notranslate nohighlight">\(Y = XW^T + b\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.LinearRegressionOutput" title="mxnet.ndarray.sparse.LinearRegressionOutput"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearRegressionOutput</span></code></a>([data,label,])</p></td>
<td><p>Computes and optimizes for squared loss during backward propagation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.LogisticRegressionOutput" title="mxnet.ndarray.sparse.LogisticRegressionOutput"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticRegressionOutput</span></code></a>([data,label,])</p></td>
<td><p>Applies a logistic function to the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.MAERegressionOutput" title="mxnet.ndarray.sparse.MAERegressionOutput"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAERegressionOutput</span></code></a>([data,label,])</p></td>
<td><p>Computes mean absolute error of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.abs" title="mxnet.ndarray.sparse.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise absolute value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.adagrad_update" title="mxnet.ndarray.sparse.adagrad_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adagrad_update</span></code></a>([weight,grad,history,lr,])</p></td>
<td><p>Update function for AdaGrad optimizer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.adam_update" title="mxnet.ndarray.sparse.adam_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adam_update</span></code></a>([weight,grad,mean,var,lr,])</p></td>
<td><p>Update function for Adam optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.add_n" title="mxnet.ndarray.sparse.add_n"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_n</span></code></a>(*args,**kwargs)</p></td>
<td><p>Adds all input arguments element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arccos" title="mxnet.ndarray.sparse.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise inverse cosine of the input array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arccosh" title="mxnet.ndarray.sparse.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the element-wise inverse hyperbolic cosine of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arcsin" title="mxnet.ndarray.sparse.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise inverse sine of the input array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arcsinh" title="mxnet.ndarray.sparse.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the element-wise inverse hyperbolic sine of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arctan" title="mxnet.ndarray.sparse.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise inverse tangent of the input array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.arctanh" title="mxnet.ndarray.sparse.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the element-wise inverse hyperbolic tangent of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_add" title="mxnet.ndarray.sparse.broadcast_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_add</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise sum of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_div" title="mxnet.ndarray.sparse.broadcast_div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_div</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise division of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_minus" title="mxnet.ndarray.sparse.broadcast_minus"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_minus</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise difference of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_mul" title="mxnet.ndarray.sparse.broadcast_mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_mul</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise product of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_plus" title="mxnet.ndarray.sparse.broadcast_plus"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_plus</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise sum of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.broadcast_sub" title="mxnet.ndarray.sparse.broadcast_sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_sub</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Returns element-wise difference of the input arrays with broadcasting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.cast_storage" title="mxnet.ndarray.sparse.cast_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cast_storage</span></code></a>([data,stype,out,name])</p></td>
<td><p>Casts tensor storage type to the new type.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.cbrt" title="mxnet.ndarray.sparse.cbrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cbrt</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise cube-root value of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.ceil" title="mxnet.ndarray.sparse.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise ceiling of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.clip" title="mxnet.ndarray.sparse.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip</span></code></a>([data,a_min,a_max,out,name])</p></td>
<td><p>Clips (limits) the values in an array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.concat" title="mxnet.ndarray.sparse.concat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">concat</span></code></a>(*data,**kwargs)</p></td>
<td><p>Joins input arrays along a given axis.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.cos" title="mxnet.ndarray.sparse.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos</span></code></a>([data,out,name])</p></td>
<td><p>Computes the element-wise cosine of the input array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.cosh" title="mxnet.ndarray.sparse.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the hyperbolic cosine  of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.degrees" title="mxnet.ndarray.sparse.degrees"><code class="xref py py-obj docutils literal notranslate"><span class="pre">degrees</span></code></a>([data,out,name])</p></td>
<td><p>Converts each element of the input array from radians to degrees.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.digamma" title="mxnet.ndarray.sparse.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise log derivative of the gamma function of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.dot" title="mxnet.ndarray.sparse.dot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dot</span></code></a>([lhs,rhs,transpose_a,transpose_b,])</p></td>
<td><p>Dot product of two arrays.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.elemwise_add" title="mxnet.ndarray.sparse.elemwise_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elemwise_add</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Adds arguments element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.elemwise_div" title="mxnet.ndarray.sparse.elemwise_div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elemwise_div</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Divides arguments element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.elemwise_mul" title="mxnet.ndarray.sparse.elemwise_mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elemwise_mul</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Multiplies arguments element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.elemwise_sub" title="mxnet.ndarray.sparse.elemwise_sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elemwise_sub</span></code></a>([lhs,rhs,out,name])</p></td>
<td><p>Subtracts arguments element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.exp" title="mxnet.ndarray.sparse.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise exponential value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.expm1" title="mxnet.ndarray.sparse.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1</span></code></a>([data,out,name])</p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">exp(x)</span> <span class="pre">-</span> <span class="pre">1</span></code> computed element-wise on the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.fix" title="mxnet.ndarray.sparse.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise rounded value to the nearest integer towards zero of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.floor" title="mxnet.ndarray.sparse.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise floor of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.ftrl_update" title="mxnet.ndarray.sparse.ftrl_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ftrl_update</span></code></a>([weight,grad,z,n,lr,])</p></td>
<td><p>Update function for Ftrl optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.gamma" title="mxnet.ndarray.sparse.gamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gamma</span></code></a>([data,out,name])</p></td>
<td><p>Returns the gamma function (extension of the factorial function to the reals), computed element-wise on the input array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.gammaln" title="mxnet.ndarray.sparse.gammaln"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gammaln</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise log of the absolute value of the gamma function of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.log" title="mxnet.ndarray.sparse.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise Natural logarithmic value of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.log10" title="mxnet.ndarray.sparse.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise Base-10 logarithmic value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.log1p" title="mxnet.ndarray.sparse.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise <code class="docutils literal notranslate"><span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">x)</span></code> value of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.log2" title="mxnet.ndarray.sparse.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise Base-2 logarithmic value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.log_sigmoid" title="mxnet.ndarray.sparse.log_sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_sigmoid</span></code></a>([data,out,name])</p></td>
<td><p>Computes log_sigmoid of x element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.make_loss" title="mxnet.ndarray.sparse.make_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_loss</span></code></a>([data,out,name])</p></td>
<td><p>Make your own loss function in network construction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.mean" title="mxnet.ndarray.sparse.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a>([data,axis,keepdims,exclude,out,name])</p></td>
<td><p>Computes the mean of array elements over given axes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.mish" title="mxnet.ndarray.sparse.mish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mish</span></code></a>([data,out,name])</p></td>
<td><p>Computes mish of x element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.negative" title="mxnet.ndarray.sparse.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative</span></code></a>([data,out,name])</p></td>
<td><p>Numerical negative of the argument, element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.norm" title="mxnet.ndarray.sparse.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code></a>([data,ord,axis,out_dtype,keepdims,])</p></td>
<td><p>Computes the norm on an NDArray.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.radians" title="mxnet.ndarray.sparse.radians"><code class="xref py py-obj docutils literal notranslate"><span class="pre">radians</span></code></a>([data,out,name])</p></td>
<td><p>Converts each element of the input array from degrees to radians.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.relu" title="mxnet.ndarray.sparse.relu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">relu</span></code></a>([data,out,name])</p></td>
<td><p>Computes rectified linear activation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.retain" title="mxnet.ndarray.sparse.retain"><code class="xref py py-obj docutils literal notranslate"><span class="pre">retain</span></code></a>([data,indices,out,name])</p></td>
<td><p>Pick rows specified by user input index array from a row sparse matrix and save them in the output sparse matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.rint" title="mxnet.ndarray.sparse.rint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rint</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise rounded value to the nearest integer of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.round" title="mxnet.ndarray.sparse.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise rounded value to the nearest integer of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.rsqrt" title="mxnet.ndarray.sparse.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise inverse square-root value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sgd_mom_update" title="mxnet.ndarray.sparse.sgd_mom_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgd_mom_update</span></code></a>([weight,grad,mom,lr,])</p></td>
<td><p>Momentum update function for Stochastic Gradient Descent (SGD) optimizer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sgd_update" title="mxnet.ndarray.sparse.sgd_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgd_update</span></code></a>([weight,grad,lr,wd,])</p></td>
<td><p>Update function for Stochastic Gradient Descent (SGD) optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sigmoid" title="mxnet.ndarray.sparse.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid</span></code></a>([data,out,name])</p></td>
<td><p>Computes sigmoid of x element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sign" title="mxnet.ndarray.sparse.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise sign of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sin" title="mxnet.ndarray.sparse.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin</span></code></a>([data,out,name])</p></td>
<td><p>Computes the element-wise sine of the input array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sinh" title="mxnet.ndarray.sparse.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the hyperbolic sine of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.slice" title="mxnet.ndarray.sparse.slice"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slice</span></code></a>([data,begin,end,step,out,name])</p></td>
<td><p>Slices a region of the array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sqrt" title="mxnet.ndarray.sparse.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise square-root value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.square" title="mxnet.ndarray.sparse.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square</span></code></a>([data,out,name])</p></td>
<td><p>Returns element-wise squared value of the input.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.stop_gradient" title="mxnet.ndarray.sparse.stop_gradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stop_gradient</span></code></a>([data,out,name])</p></td>
<td><p>Stops gradient computation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.sum" title="mxnet.ndarray.sparse.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum</span></code></a>([data,axis,keepdims,exclude,out,name])</p></td>
<td><p>Computes the sum of array elements over given axes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.tan" title="mxnet.ndarray.sparse.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan</span></code></a>([data,out,name])</p></td>
<td><p>Computes the element-wise tangent of the input array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.tanh" title="mxnet.ndarray.sparse.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh</span></code></a>([data,out,name])</p></td>
<td><p>Returns the hyperbolic tangent of the input array, computed element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.trunc" title="mxnet.ndarray.sparse.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc</span></code></a>([data,out,name])</p></td>
<td><p>Return the element-wise truncated value of the input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.where" title="mxnet.ndarray.sparse.where"><code class="xref py py-obj docutils literal notranslate"><span class="pre">where</span></code></a>([condition,x,y,out,name])</p></td>
<td><p>Return the elements, either from x or y, depending on the condition.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.zeros_like" title="mxnet.ndarray.sparse.zeros_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros_like</span></code></a>([data,out,name])</p></td>
<td><p>Return an array of zeros with the same shape, type and storage type as the input array.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray" title="mxnet.ndarray.sparse.BaseSparseNDArray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BaseSparseNDArray</span></code></a></p></td>
<td><p>The base class of an NDArray stored in a sparse storage format.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CSRNDArray</span></code></a></p></td>
<td><p>A sparse representation of 2D NDArray in the Compressed Sparse Row format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RowSparseNDArray</span></code></a></p></td>
<td><p>A sparse representation of a set of NDArray row slices at given indices.</p></td>
</tr>
</tbody>
</table>
<dl class="function">
<dt id="mxnet.ndarray.sparse.csr_matrix">
<code class="sig-name descname">csr_matrix</code><span class="sig-paren">(</span><em class="sig-param">arg1</em>, <em class="sig-param">shape=None</em>, <em class="sig-param">ctx=None</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#csr_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.csr_matrix" title="Permalink to this definition"></a></dt>
<dd><p>Creates a <cite>CSRNDArray</cite>, an 2D array with compressed sparse row (CSR) format.</p>
<p>The CSRNDArray can be instantiated in several ways:</p>
<ul class="simple">
<li><dl class="simple">
<dt>csr_matrix(D):</dt><dd><dl class="simple">
<dt>to construct a CSRNDArray with a dense 2D array <code class="docutils literal notranslate"><span class="pre">D</span></code></dt><dd><ul>
<li><p><strong>D</strong> (<em>array_like</em>) - An object exposing the array interface, an object whose             <cite>__array__</cite> method returns an array, or any (nested) sequence.</p></li>
<li><p><strong>ctx</strong> (<em>Context, optional</em>) - Device context             (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is <code class="docutils literal notranslate"><span class="pre">D.dtype</span></code> if <code class="docutils literal notranslate"><span class="pre">D</span></code> is an NDArray or numpy.ndarray,             float32 otherwise.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>csr_matrix(S)</dt><dd><dl class="simple">
<dt>to construct a CSRNDArray with a sparse 2D array <code class="docutils literal notranslate"><span class="pre">S</span></code></dt><dd><ul>
<li><p><strong>S</strong> (<em>CSRNDArray or scipy.sparse.csr.csr_matrix</em>) - A sparse matrix.</p></li>
<li><p><strong>ctx</strong> (<em>Context, optional</em>) - Device context             (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is <code class="docutils literal notranslate"><span class="pre">S.dtype</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>csr_matrix((M, N))</dt><dd><dl class="simple">
<dt>to construct an empty CSRNDArray with shape <code class="docutils literal notranslate"><span class="pre">(M,</span> <span class="pre">N)</span></code></dt><dd><ul>
<li><p><strong>M</strong> (<em>int</em>) - Number of rows in the matrix</p></li>
<li><p><strong>N</strong> (<em>int</em>) - Number of columns in the matrix</p></li>
<li><p><strong>ctx</strong> (<em>Context, optional</em>) - Device context             (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is float32.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>csr_matrix((data, indices, indptr))</dt><dd><dl class="simple">
<dt>to construct a CSRNDArray based on the definition of compressed sparse row format         using three separate arrays,         where the column indices for row i are stored in <code class="docutils literal notranslate"><span class="pre">indices[indptr[i]:indptr[i+1]]</span></code>         and their corresponding values are stored in <code class="docutils literal notranslate"><span class="pre">data[indptr[i]:indptr[i+1]]</span></code>.         The column indices for a given row are expected to be <strong>sorted in ascending order.</strong>         Duplicate column entries for the same row are not allowed.</dt><dd><ul>
<li><p><strong>data</strong> (<em>array_like</em>) - An object exposing the array interface, which             holds all the non-zero entries of the matrix in row-major order.</p></li>
<li><p><strong>indices</strong> (<em>array_like</em>) - An object exposing the array interface, which             stores the column index for each non-zero element in <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>indptr</strong> (<em>array_like</em>) - An object exposing the array interface, which             stores the offset into <code class="docutils literal notranslate"><span class="pre">data</span></code> of the first non-zero element number of each             row of the matrix.</p></li>
<li><p><strong>shape</strong> (<em>tuple of int, optional</em>) - The shape of the array. The default             shape is inferred from the indices and indptr arrays.</p></li>
<li><p><strong>ctx</strong> (<em>Context, optional</em>) - Device context             (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is <code class="docutils literal notranslate"><span class="pre">data.dtype</span></code> if <code class="docutils literal notranslate"><span class="pre">data</span></code> is an NDArray or numpy.ndarray,             float32 otherwise.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>csr_matrix((data, (row, col)))</dt><dd><dl class="simple">
<dt>to construct a CSRNDArray based on the COOrdinate format         using three seperate arrays,         where <code class="docutils literal notranslate"><span class="pre">row[i]</span></code> is the row index of the element,         <code class="docutils literal notranslate"><span class="pre">col[i]</span></code> is the column index of the element         and <code class="docutils literal notranslate"><span class="pre">data[i]</span></code> is the data corresponding to the element. All the missing         elements in the input are taken to be zeroes.</dt><dd><ul>
<li><p><strong>data</strong> (<em>array_like</em>) - An object exposing the array interface, which             holds all the non-zero entries of the matrix in COO format.</p></li>
<li><p><strong>row</strong> (<em>array_like</em>) - An object exposing the array interface, which             stores the row index for each non zero element in <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>col</strong> (<em>array_like</em>) - An object exposing the array interface, which             stores the col index for each non zero element in <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>shape</strong> (<em>tuple of int, optional</em>) - The shape of the array. The default             shape is inferred from the <code class="docutils literal notranslate"><span class="pre">row</span></code> and <code class="docutils literal notranslate"><span class="pre">col</span></code> arrays.</p></li>
<li><p><strong>ctx</strong> (<em>Context, optional</em>) - Device context             (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is float32.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arg1</strong> (<em>tuple of int</em><em>, </em><em>tuple of array_like</em><em>, </em><em>array_like</em><em>, </em><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray"><em>CSRNDArray</em></a><em>, </em><em>scipy.sparse.csr_matrix</em><em>,     </em><em>scipy.sparse.coo_matrix</em><em>, </em><em>tuple of int</em><em> or </em><em>tuple of array_like</em>)  The argument to help instantiate the csr matrix. See above for further details.</p></li>
<li><p><strong>shape</strong> (<em>tuple of int</em><em>, </em><em>optional</em>)  The shape of the csr matrix.</p></li>
<li><p><strong>ctx</strong> (<a class="reference internal" href="../../../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a><em>, </em><em>optional</em>)  Device context (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em> or </em><em>numpy.dtype</em><em>, </em><em>optional</em>)  The data type of the output array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>CSRNDArray</cite> with the <cite>csr</cite> storage representation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray">CSRNDArray</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  0.],</span>
<span class="go">       [ 2.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  3.]], dtype=float32)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray"><code class="xref py py-func docutils literal notranslate"><span class="pre">CSRNDArray()</span></code></a></dt><dd><p>MXNet NDArray in compressed sparse row format.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.row_sparse_array">
<code class="sig-name descname">row_sparse_array</code><span class="sig-paren">(</span><em class="sig-param">arg1</em>, <em class="sig-param">shape=None</em>, <em class="sig-param">ctx=None</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#row_sparse_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.row_sparse_array" title="Permalink to this definition"></a></dt>
<dd><p>Creates a <cite>RowSparseNDArray</cite>, a multidimensional row sparse array with a set of     tensor slices at given indices.</p>
<p>The RowSparseNDArray can be instantiated in several ways:</p>
<ul class="simple">
<li><dl class="simple">
<dt>row_sparse_array(D):</dt><dd><p>to construct a RowSparseNDArray with a dense ndarray <code class="docutils literal notranslate"><span class="pre">D</span></code>
-  <strong>D</strong> (<em>array_like</em>) - An object exposing the array interface, an object whose         <cite>__array__</cite> method returns an array, or any (nested) sequence.
- <strong>ctx</strong> (<em>Context, optional</em>) - Device context         (default is the current default context).
- <strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.         The default dtype is <code class="docutils literal notranslate"><span class="pre">D.dtype</span></code> if <code class="docutils literal notranslate"><span class="pre">D</span></code> is an NDArray or numpy.ndarray,         float32 otherwise.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>row_sparse_array(S)</dt><dd><p>to construct a RowSparseNDArray with a sparse ndarray <code class="docutils literal notranslate"><span class="pre">S</span></code>
-  <strong>S</strong> (<em>RowSparseNDArray</em>) - A sparse ndarray.
- <strong>ctx</strong> (<em>Context, optional</em>) - Device context         (default is the current default context).
- <strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.         The default dtype is <code class="docutils literal notranslate"><span class="pre">S.dtype</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>row_sparse_array((D0, D1 .. Dn))</dt><dd><p>to construct an empty RowSparseNDArray with shape <code class="docutils literal notranslate"><span class="pre">(D0,</span> <span class="pre">D1,</span> <span class="pre">...</span> <span class="pre">Dn)</span></code>
-  <strong>D0, D1 .. Dn</strong> (<em>int</em>) - The shape of the ndarray
- <strong>ctx</strong> (<em>Context, optional</em>) - Device context         (default is the current default context).
- <strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.             The default dtype is float32.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>row_sparse_array((data, indices))</dt><dd><p>to construct a RowSparseNDArray based on the definition of row sparse format         using two separate arrays,         where the <cite>indices</cite> stores the indices of the row slices with non-zeros,
while the values are stored in <cite>data</cite>. The corresponding NDArray <code class="docutils literal notranslate"><span class="pre">dense</span></code>
represented by RowSparseNDArray <code class="docutils literal notranslate"><span class="pre">rsp</span></code> has         <code class="docutils literal notranslate"><span class="pre">dense[rsp.indices[i],</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">rsp.data[i,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span></code>
The row indices for are expected to be <strong>sorted in ascending order.</strong>         - <strong>data</strong> (<em>array_like</em>) - An object exposing the array interface, which         holds all the non-zero row slices of the array.
- <strong>indices</strong> (<em>array_like</em>) - An object exposing the array interface, which         stores the row index for each row slice with non-zero elements.
- <strong>shape</strong> (<em>tuple of int, optional</em>) - The shape of the array. The default         shape is inferred from the indices and indptr arrays.
- <strong>ctx</strong> (<em>Context, optional</em>) - Device context         (default is the current default context).
- <strong>dtype</strong> (<em>str or numpy.dtype, optional</em>) - The data type of the output array.         The default dtype is float32.</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arg1</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>numpy.ndarray</em><em>, </em><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray"><em>RowSparseNDArray</em></a><em>, </em><em>tuple of int</em><em> or </em><em>tuple of array_like</em>)  The argument to help instantiate the row sparse ndarray. See above for further details.</p></li>
<li><p><strong>shape</strong> (<em>tuple of int</em><em>, </em><em>optional</em>)  The shape of the row sparse ndarray. (Default value = None)</p></li>
<li><p><strong>ctx</strong> (<a class="reference internal" href="../../../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a><em>, </em><em>optional</em>)  Device context (default is the current default context).</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em> or </em><em>numpy.dtype</em><em>, </em><em>optional</em>)  The data type of the output array. (Default value = None)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An <cite>RowSparseNDArray</cite> with the <cite>row_sparse</cite> storage representation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray">RowSparseNDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">row_sparse_array</span><span class="p">(([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  0.],</span>
<span class="go">       [ 1.,  2.],</span>
<span class="go">       [ 0.,  0.],</span>
<span class="go">       [ 0.,  0.],</span>
<span class="go">       [ 3.,  4.],</span>
<span class="go">       [ 0.,  0.]], dtype=float32)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray"><code class="xref py py-func docutils literal notranslate"><span class="pre">RowSparseNDArray()</span></code></a></dt><dd><p>MXNet NDArray in row sparse format.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="class">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray">
<em class="property">class </em><code class="sig-name descname">BaseSparseNDArray</code><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.ndarray.ndarray.NDArray</span></code></p>
<p>The base class of an NDArray stored in a sparse storage format.</p>
<p>See CSRNDArray and RowSparseNDArray for more details.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.asnumpy" title="mxnet.ndarray.sparse.BaseSparseNDArray.asnumpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asnumpy</span></code></a>()</p></td>
<td><p>Return a dense <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> object with value copied from this array</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.astype" title="mxnet.ndarray.sparse.BaseSparseNDArray.astype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">astype</span></code></a>(dtype[,copy])</p></td>
<td><p>Return a copy of the array after casting to a specified type.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.check_format" title="mxnet.ndarray.sparse.BaseSparseNDArray.check_format"><code class="xref py py-obj docutils literal notranslate"><span class="pre">check_format</span></code></a>([full_check])</p></td>
<td><p>Check whether the NDArray format is valid.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.copyto" title="mxnet.ndarray.sparse.BaseSparseNDArray.copyto"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copyto</span></code></a>(other)</p></td>
<td><p>Copies the value of this array to another array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.reshape" title="mxnet.ndarray.sparse.BaseSparseNDArray.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reshape</span></code></a>(*shape,**kwargs)</p></td>
<td><p>Returns a <strong>view</strong> of this array with a new shape without altering any data.</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray.size" title="mxnet.ndarray.sparse.BaseSparseNDArray.size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">size</span></code></a></p></td>
<td><p>Number of elements in the array.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.asnumpy">
<code class="sig-name descname">asnumpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray.asnumpy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.asnumpy" title="Permalink to this definition"></a></dt>
<dd><p>Return a dense <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> object with value copied from this array</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.astype">
<code class="sig-name descname">astype</code><span class="sig-paren">(</span><em class="sig-param">dtype</em>, <em class="sig-param">copy=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray.astype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.astype" title="Permalink to this definition"></a></dt>
<dd><p>Return a copy of the array after casting to a specified type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<em>numpy.dtype</em><em> or </em><em>str</em>)  The type of the returned array.</p></li>
<li><p><strong>copy</strong> (<em>bool</em>)  Default <cite>True</cite>. By default, astype always returns a newly
allocated ndarray on the same context. If this is set to
<cite>False</cite>, and the dtype requested is the same as the ndarrays
dtype, the ndarray is returned instead of a copy.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">&lt;type &#39;numpy.int32&#39;&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.check_format">
<code class="sig-name descname">check_format</code><span class="sig-paren">(</span><em class="sig-param">full_check=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray.check_format"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.check_format" title="Permalink to this definition"></a></dt>
<dd><p>Check whether the NDArray format is valid.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>full_check</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>True</cite>, rigorous check, O(N) operations. Otherwise
basic check, O(1) operations (default True).</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.copyto">
<code class="sig-name descname">copyto</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray.copyto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.copyto" title="Permalink to this definition"></a></dt>
<dd><p>Copies the value of this array to another array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray"><em>CSRNDArray</em></a><em> or </em><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray"><em>RowSparseNDArray</em></a><em> or </em><a class="reference internal" href="../../../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a>)  The destination array or context.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The copied array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray">CSRNDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray">RowSparseNDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.reshape">
<code class="sig-name descname">reshape</code><span class="sig-paren">(</span><em class="sig-param">*shape</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#BaseSparseNDArray.reshape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.reshape" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <strong>view</strong> of this array with a new shape without altering any data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<em>tuple of int</em><em>, or </em><em>n ints</em>)  <p>The new shape should not change the array size, namely
<code class="docutils literal notranslate"><span class="pre">np.prod(new_shape)</span></code> should be equal to <code class="docutils literal notranslate"><span class="pre">np.prod(self.shape)</span></code>.
Some dimensions of the shape can take special values from the set {0, -1, -2, -3, -4}.
The significance of each is explained below:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code>  copy this dimension from the input to the output shape.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-1</span></code> infers the dimension of the output shape by using the remainder of the
input dimensions keeping the size of the new array same as that of the input array.
At most one dimension of shape can be -1.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">24</span><span class="p">,)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-2</span></code> copy all/remainder of the input dimensions to the output shape.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-3</span></code> use the product of two consecutive dimensions of the input shape as the
output dimension.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">-4</span></code> split one dimension of the input into two dimensions passed subsequent to
-4 in shape (can contain -1).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">-</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>If the argument <cite>reverse</cite> is set to 1, then the special values are inferred from right
to left.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">without</span> <span class="n">reverse</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="k">for</span> <span class="nb">input</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">output</span> <span class="n">shape</span> <span class="n">would</span> <span class="n">be</span>                 <span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="k">with</span> <span class="n">reverse</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output</span> <span class="n">shape</span> <span class="n">will</span> <span class="n">be</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
</p></li>
<li><p><strong>reverse</strong> (<em>bool</em><em>, </em><em>default False</em>)  If true then the special values are inferred from right to left. Only supported as
keyword argument.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An array with desired shape that shares data with this array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.],</span>
<span class="go">       [ 3.,  4.,  5.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.],</span>
<span class="go">       [ 2.,  3.],</span>
<span class="go">       [ 4.,  5.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.],</span>
<span class="go">       [ 2.,  3.],</span>
<span class="go">       [ 4.,  5.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.],</span>
<span class="go">       [ 2.,  3.],</span>
<span class="go">       [ 4.,  5.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([ 0.  1.  2.  3.  4.  5.], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[-1., -1., -1.],</span>
<span class="go">       [-1., -1., -1.]], dtype=float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.BaseSparseNDArray.size">
<em class="property">property </em><code class="sig-name descname">size</code><a class="headerlink" href="#mxnet.ndarray.sparse.BaseSparseNDArray.size" title="Permalink to this definition"></a></dt>
<dd><p>Number of elements in the array.</p>
<p>Equivalent to the product of the arrays dimensions.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span>
<span class="go">30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">30</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.ndarray.sparse.CSRNDArray">
<em class="property">class </em><code class="sig-name descname">CSRNDArray</code><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#CSRNDArray"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray" title="mxnet.ndarray.sparse.BaseSparseNDArray"><code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.ndarray.sparse.BaseSparseNDArray</span></code></a></p>
<p>A sparse representation of 2D NDArray in the Compressed Sparse Row format.</p>
<p>A CSRNDArray represents an NDArray as three separate arrays: <cite>data</cite>,
<cite>indptr</cite> and <cite>indices</cite>. It uses the CSR representation where the column indices for
row i are stored in <code class="docutils literal notranslate"><span class="pre">indices[indptr[i]:indptr[i+1]]</span></code> and their corresponding values are stored
in <code class="docutils literal notranslate"><span class="pre">data[indptr[i]:indptr[i+1]]</span></code>.</p>
<p>The column indices for a given row are expected to be sorted in ascending order.
Duplicate column entries for the same row are not allowed.</p>
<p class="rubric">Example</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.asscipy" title="mxnet.ndarray.sparse.CSRNDArray.asscipy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asscipy</span></code></a>()</p></td>
<td><p>Returns a <code class="docutils literal notranslate"><span class="pre">scipy.sparse.csr.csr_matrix</span></code> object with value copied from this array</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.copyto" title="mxnet.ndarray.sparse.CSRNDArray.copyto"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copyto</span></code></a>(other)</p></td>
<td><p>Copies the value of this array to another array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.tostype" title="mxnet.ndarray.sparse.CSRNDArray.tostype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tostype</span></code></a>(stype)</p></td>
<td><p>Return a copy of the array with chosen storage type.</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.data" title="mxnet.ndarray.sparse.CSRNDArray.data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">data</span></code></a></p></td>
<td><p>A deep copy NDArray of the data array of the CSRNDArray.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.indices" title="mxnet.ndarray.sparse.CSRNDArray.indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">indices</span></code></a></p></td>
<td><p>A deep copy NDArray of the indices array of the CSRNDArray.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray.indptr" title="mxnet.ndarray.sparse.CSRNDArray.indptr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">indptr</span></code></a></p></td>
<td><p>A deep copy NDArray of the indptr array of the CSRNDArray.</p></td>
</tr>
</tbody>
</table>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([ 1.,  2.,  3.], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([1, 0, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([0, 1, 2, 2, 3])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#mxnet.ndarray.sparse.csr_matrix" title="mxnet.ndarray.sparse.csr_matrix"><code class="xref py py-class docutils literal notranslate"><span class="pre">csr_matrix</span></code></a></dt><dd><p>Several ways to construct a CSRNDArray</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.asscipy">
<code class="sig-name descname">asscipy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#CSRNDArray.asscipy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.asscipy" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <code class="docutils literal notranslate"><span class="pre">scipy.sparse.csr.csr_matrix</span></code> object with value copied from this array</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asscipy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">&lt;type &#39;scipy.sparse.csr.csr_matrix&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">&lt;2x3 sparse matrix of type &#39;&lt;type &#39;numpy.float32&#39;&gt;&#39;</span>
<span class="go">with 0 stored elements in Compressed Sparse Row format&gt;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.copyto">
<code class="sig-name descname">copyto</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#CSRNDArray.copyto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.copyto" title="Permalink to this definition"></a></dt>
<dd><p>Copies the value of this array to another array.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">other</span></code> is a <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">CSRNDArray</span></code> object, then <code class="docutils literal notranslate"><span class="pre">other.shape</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.shape</span></code> should be the same. This function copies the value from
<code class="docutils literal notranslate"><span class="pre">self</span></code> to <code class="docutils literal notranslate"><span class="pre">other</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">other</span></code> is a context, a new <code class="docutils literal notranslate"><span class="pre">CSRNDArray</span></code> will be first created on
the target context, and the value of <code class="docutils literal notranslate"><span class="pre">self</span></code> is copied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray"><em>CSRNDArray</em></a><em> or </em><a class="reference internal" href="../../../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a>)  The destination array or context.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The copied array. If <code class="docutils literal notranslate"><span class="pre">other</span></code> is an <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">CSRNDArray</span></code>, then the return
value and <code class="docutils literal notranslate"><span class="pre">other</span></code> will point to the same <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">CSRNDArray</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray">CSRNDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.data">
<em class="property">property </em><code class="sig-name descname">data</code><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.data" title="Permalink to this definition"></a></dt>
<dd><p>A deep copy NDArray of the data array of the CSRNDArray.
This generates a deep copy of the <cite>data</cite> of the current <cite>csr</cite> matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This CSRNDArrays data array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.indices">
<em class="property">property </em><code class="sig-name descname">indices</code><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.indices" title="Permalink to this definition"></a></dt>
<dd><p>A deep copy NDArray of the indices array of the CSRNDArray.
This generates a deep copy of the column indices of the current <cite>csr</cite> matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This CSRNDArrays indices array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.indptr">
<em class="property">property </em><code class="sig-name descname">indptr</code><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.indptr" title="Permalink to this definition"></a></dt>
<dd><p>A deep copy NDArray of the indptr array of the CSRNDArray.
This generates a deep copy of the <cite>indptr</cite> of the current <cite>csr</cite> matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This CSRNDArrays indptr array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.CSRNDArray.tostype">
<code class="sig-name descname">tostype</code><span class="sig-paren">(</span><em class="sig-param">stype</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#CSRNDArray.tostype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.CSRNDArray.tostype" title="Permalink to this definition"></a></dt>
<dd><p>Return a copy of the array with chosen storage type.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A copy of the array with the chosen storage stype</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.CSRNDArray" title="mxnet.ndarray.sparse.CSRNDArray">CSRNDArray</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray">
<em class="property">class </em><code class="sig-name descname">RowSparseNDArray</code><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#RowSparseNDArray"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#mxnet.ndarray.sparse.BaseSparseNDArray" title="mxnet.ndarray.sparse.BaseSparseNDArray"><code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.ndarray.sparse.BaseSparseNDArray</span></code></a></p>
<p>A sparse representation of a set of NDArray row slices at given indices.</p>
<p>A RowSparseNDArray represents a multidimensional NDArray using two separate arrays: <cite>data</cite> and
<cite>indices</cite>. The number of dimensions has to be at least 2.</p>
<ul class="simple">
<li><p>data: an NDArray of any dtype with shape [D0, D1, , Dn].</p></li>
<li><p>indices: a 1-D int64 NDArray with shape [D0] with values sorted in ascending order.</p></li>
</ul>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray.copyto" title="mxnet.ndarray.sparse.RowSparseNDArray.copyto"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copyto</span></code></a>(other)</p></td>
<td><p>Copies the value of this array to another array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray.retain" title="mxnet.ndarray.sparse.RowSparseNDArray.retain"><code class="xref py py-obj docutils literal notranslate"><span class="pre">retain</span></code></a>(*args,**kwargs)</p></td>
<td><p>Convenience fluent method for <a class="reference internal" href="#mxnet.ndarray.sparse.retain" title="mxnet.ndarray.sparse.retain"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray.tostype" title="mxnet.ndarray.sparse.RowSparseNDArray.tostype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tostype</span></code></a>(stype)</p></td>
<td><p>Return a copy of the array with chosen storage type.</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray.data" title="mxnet.ndarray.sparse.RowSparseNDArray.data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">data</span></code></a></p></td>
<td><p>A deep copy NDArray of the data array of the RowSparseNDArray.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray.indices" title="mxnet.ndarray.sparse.RowSparseNDArray.indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">indices</span></code></a></p></td>
<td><p>A deep copy NDArray of the indices array of the RowSparseNDArray.</p></td>
</tr>
</tbody>
</table>
<p>The <cite>indices</cite> stores the indices of the row slices with non-zeros,
while the values are stored in <cite>data</cite>. The corresponding NDArray <code class="docutils literal notranslate"><span class="pre">dense</span></code>
represented by RowSparseNDArray <code class="docutils literal notranslate"><span class="pre">rsp</span></code> has</p>
<p><code class="docutils literal notranslate"><span class="pre">dense[rsp.indices[i],</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">rsp.data[i,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">:,</span> <span class="pre">...]</span></code></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  2., 3.],</span>
<span class="go">       [ 0.,  0., 0.],</span>
<span class="go">       [ 4.,  0., 5.],</span>
<span class="go">       [ 0.,  0., 0.],</span>
<span class="go">       [ 0.,  0., 0.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsp</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsp</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([0, 2], dtype=int64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  2., 3.],</span>
<span class="go">       [ 4.,  0., 5.]], dtype=float32)</span>
</pre></div>
</div>
<p>A RowSparseNDArray is typically used to represent non-zero row slices of a large NDArray
of shape [LARGE0, D1, .. , Dn] where LARGE0 &gt;&gt; D0 and most row slices are zeros.</p>
<p>RowSparseNDArray is used principally in the definition of gradients for operations
that have sparse gradients (e.g. sparse dot and sparse embedding).</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#mxnet.ndarray.sparse.row_sparse_array" title="mxnet.ndarray.sparse.row_sparse_array"><code class="xref py py-class docutils literal notranslate"><span class="pre">row_sparse_array</span></code></a></dt><dd><p>Several ways to construct a RowSparseNDArray</p>
</dd>
</dl>
</div>
<dl class="method">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray.copyto">
<code class="sig-name descname">copyto</code><span class="sig-paren">(</span><em class="sig-param">other</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#RowSparseNDArray.copyto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray.copyto" title="Permalink to this definition"></a></dt>
<dd><p>Copies the value of this array to another array.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">other</span></code> is a <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">RowSparseNDArray</span></code> object, then <code class="docutils literal notranslate"><span class="pre">other.shape</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.shape</span></code> should be the same. This function copies the value from
<code class="docutils literal notranslate"><span class="pre">self</span></code> to <code class="docutils literal notranslate"><span class="pre">other</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">other</span></code> is a context, a new <code class="docutils literal notranslate"><span class="pre">RowSparseNDArray</span></code> will be first created on
the target context, and the value of <code class="docutils literal notranslate"><span class="pre">self</span></code> is copied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray"><em>RowSparseNDArray</em></a><em> or </em><a class="reference internal" href="../../../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a>)  The destination array or context.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The copied array. If <code class="docutils literal notranslate"><span class="pre">other</span></code> is an <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">RowSparseNDArray</span></code>, then the
return value and <code class="docutils literal notranslate"><span class="pre">other</span></code> will point to the same <code class="docutils literal notranslate"><span class="pre">NDArray</span></code> or <code class="docutils literal notranslate"><span class="pre">RowSparseNDArray</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray">RowSparseNDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray.data">
<em class="property">property </em><code class="sig-name descname">data</code><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray.data" title="Permalink to this definition"></a></dt>
<dd><p>A deep copy NDArray of the data array of the RowSparseNDArray.
This generates a deep copy of the <cite>data</cite> of the current <cite>row_sparse</cite> matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This RowSparseNDArrays data array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray.indices">
<em class="property">property </em><code class="sig-name descname">indices</code><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray.indices" title="Permalink to this definition"></a></dt>
<dd><p>A deep copy NDArray of the indices array of the RowSparseNDArray.
This generates a deep copy of the row indices of the current <cite>row_sparse</cite> matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>This RowSparseNDArrays indices array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray.retain">
<code class="sig-name descname">retain</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#RowSparseNDArray.retain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray.retain" title="Permalink to this definition"></a></dt>
<dd><p>Convenience fluent method for <a class="reference internal" href="#mxnet.ndarray.sparse.retain" title="mxnet.ndarray.sparse.retain"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain()</span></code></a>.</p>
<p>The arguments are the same as for <a class="reference internal" href="#mxnet.ndarray.sparse.retain" title="mxnet.ndarray.sparse.retain"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain()</span></code></a>, with
this array as data.</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.ndarray.sparse.RowSparseNDArray.tostype">
<code class="sig-name descname">tostype</code><span class="sig-paren">(</span><em class="sig-param">stype</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#RowSparseNDArray.tostype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.RowSparseNDArray.tostype" title="Permalink to this definition"></a></dt>
<dd><p>Return a copy of the array with chosen storage type.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A copy of the array with the chosen storage stype</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or <a class="reference internal" href="#mxnet.ndarray.sparse.RowSparseNDArray" title="mxnet.ndarray.sparse.RowSparseNDArray">RowSparseNDArray</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">lhs</em>, <em class="sig-param">rhs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.add" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise sum of the input arrays with broadcasting.</p>
<p>Equivalent to <code class="docutils literal notranslate"><span class="pre">lhs</span> <span class="pre">+</span> <span class="pre">rhs</span></code>, <code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_add(lhs,</span> <span class="pre">rhs)</span></code> and
<code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_plus(lhs,</span> <span class="pre">rhs)</span></code> when shapes of lhs and rhs do not
match. If lhs.shape == rhs.shape, this is equivalent to
<code class="docutils literal notranslate"><span class="pre">mx.nd.elemwise_add(lhs,</span> <span class="pre">rhs)</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the corresponding dimensions of two arrays have the same size or one of them has size 1,
then the arrays are broadcastable to a common shape.abs</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  First array to be added.</p></li>
<li><p><strong>rhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  Second array to be added.
If <code class="docutils literal notranslate"><span class="pre">lhs.shape</span> <span class="pre">!=</span> <span class="pre">rhs.shape</span></code>, they must be
broadcastable to a common shape.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The element-wise sum of the input arrays.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">c</span><span class="o">+</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.subtract">
<code class="sig-name descname">subtract</code><span class="sig-paren">(</span><em class="sig-param">lhs</em>, <em class="sig-param">rhs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#subtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.subtract" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise difference of the input arrays with broadcasting.</p>
<p>Equivalent to <code class="docutils literal notranslate"><span class="pre">lhs</span> <span class="pre">-</span> <span class="pre">rhs</span></code>, <code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_sub(lhs,</span> <span class="pre">rhs)</span></code> and
<code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_minus(lhs,</span> <span class="pre">rhs)</span></code> when shapes of lhs and rhs do not
match. If lhs.shape == rhs.shape, this is equivalent to
<code class="docutils literal notranslate"><span class="pre">mx.nd.elemwise_sub(lhs,</span> <span class="pre">rhs)</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the corresponding dimensions of two arrays have the same size or one of them has size 1,
then the arrays are broadcastable to a common shape.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  First array to be subtracted.</p></li>
<li><p><strong>rhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  Second array to be subtracted.
If <code class="docutils literal notranslate"><span class="pre">lhs.shape</span> <span class="pre">!=</span> <span class="pre">rhs.shape</span></code>, they must be
broadcastable to a common shape.__spec__</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The element-wise difference of the input arrays.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">c</span><span class="o">-</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.]], dtype=float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.multiply">
<code class="sig-name descname">multiply</code><span class="sig-paren">(</span><em class="sig-param">lhs</em>, <em class="sig-param">rhs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#multiply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.multiply" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise product of the input arrays with broadcasting.</p>
<blockquote>
<div><p>Equivalent to <code class="docutils literal notranslate"><span class="pre">lhs</span> <span class="pre">*</span> <span class="pre">rhs</span></code> and <code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_mul(lhs,</span> <span class="pre">rhs)</span></code>
when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,
this is equivalent to <code class="docutils literal notranslate"><span class="pre">mx.nd.elemwise_mul(lhs,</span> <span class="pre">rhs)</span></code></p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the corresponding dimensions of two arrays have the same size or one of them has size 1,
then the arrays are broadcastable to a common shape.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  First array to be multiplied.</p></li>
<li><p><strong>rhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  Second array to be multiplied.
If <code class="docutils literal notranslate"><span class="pre">lhs.shape</span> <span class="pre">!=</span> <span class="pre">rhs.shape</span></code>, they must be
broadcastable to a common shape.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The element-wise multiplication of the input arrays.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  1.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.],</span>
<span class="go">       [ 1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([ 0.,  1.,  2.], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  0.,  0.],</span>
<span class="go">       [ 1.,  1.,  1.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.],</span>
<span class="go">       [ 0.,  1.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.],</span>
<span class="go">       [ 0.,  1.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.],</span>
<span class="go">       [ 0.,  1.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 0.,  1.,  2.],</span>
<span class="go">       [ 0.,  1.,  2.]], dtype=float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.divide">
<code class="sig-name descname">divide</code><span class="sig-paren">(</span><em class="sig-param">lhs</em>, <em class="sig-param">rhs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../../_modules/mxnet/ndarray/sparse.html#divide"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.ndarray.sparse.divide" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise division of the input arrays with broadcasting.</p>
<p>Equivalent to <code class="docutils literal notranslate"><span class="pre">lhs</span> <span class="pre">/</span> <span class="pre">rhs</span></code> and <code class="docutils literal notranslate"><span class="pre">mx.nd.broadcast_div(lhs,</span> <span class="pre">rhs)</span></code>
when shapes of lhs and rhs do not match. If lhs.shape == rhs.shape,
this is equivalent to <code class="docutils literal notranslate"><span class="pre">mx.nd.elemwise_div(lhs,</span> <span class="pre">rhs)</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the corresponding dimensions of two arrays have the same size or one of them has size 1,
then the arrays are broadcastable to a common shape.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  First array in division.</p></li>
<li><p><strong>rhs</strong> (<em>scalar</em><em> or </em><em>mxnet.ndarray.sparse.array</em>)  Second array in division.
The arrays to be divided. If <code class="docutils literal notranslate"><span class="pre">lhs.shape</span> <span class="pre">!=</span> <span class="pre">rhs.shape</span></code>, they must be
broadcastable to a common shape.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The element-wise division of the input arrays.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">*</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">tostype</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  6.,  6.],</span>
<span class="go">       [ 6.,  6.,  6.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.],</span>
<span class="go">       [ 2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([ 1.,  2.,  3.], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">/</span><span class="mi">2</span>
<span class="go">&lt;NDArray 2x3 @cpu(0)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 2.,  2.,  2.],</span>
<span class="go">       [ 2.,  2.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  6.,  6.],</span>
<span class="go">       [ 3.,  3.,  3.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  6.,  6.],</span>
<span class="go">       [ 3.,  3.,  3.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  3.,  2.],</span>
<span class="go">       [ 6.,  3.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sprase</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  3.,  2.],</span>
<span class="go">       [ 6.,  3.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 1.,  2.,  3.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  3.,  2.],</span>
<span class="go">       [ 6.,  3.,  2.]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="go">array([[ 6.,  3.,  2.],</span>
<span class="go">       [ 6.,  3.,  2.]], dtype=float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.ElementWiseSum">
<code class="sig-name descname">ElementWiseSum</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.ElementWiseSum" title="Permalink to this definition"></a></dt>
<dd><p>Adds all input arguments element-wise.</p>
<div class="math notranslate nohighlight">
\[add\_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\]</div>
<p><code class="docutils literal notranslate"><span class="pre">add_n</span></code> is potentially more efficient than calling <code class="docutils literal notranslate"><span class="pre">add</span></code> by <cite>n</cite> times.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">add_n</span></code> output depends on storage types of inputs</p>
<ul class="simple">
<li><p>add_n(row_sparse, row_sparse, ..) = row_sparse</p></li>
<li><p>add_n(default, csr, default) = default</p></li>
<li><p>add_n(any input combinations longer than 4 (&gt;4) with at least one default type) = default</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">add_n</span></code> falls all inputs back to default storage and generates default storage</p></li>
</ul>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_sum.cc:L157</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>[</em><em>]</em>)  Positional input arguments</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.Embedding">
<code class="sig-name descname">Embedding</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">weight=None</em>, <em class="sig-param">input_dim=_Null</em>, <em class="sig-param">output_dim=_Null</em>, <em class="sig-param">dtype=_Null</em>, <em class="sig-param">sparse_grad=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.Embedding" title="Permalink to this definition"></a></dt>
<dd><p>Maps integer indices to vector representations (embeddings).</p>
<p>This operator maps words to real-valued vectors in a high-dimensional space,
called word embeddings. These embeddings can capture semantic and syntactic properties of the words.
For example, it has been noted that in the learned embedding spaces, similar words tend
to be close to each other and dissimilar words far apart.</p>
<p>For an input array of shape (d1, , dK),
the shape of an output array is (d1, , dK, output_dim).
All the input values should be integers in the range [0, input_dim).</p>
<p>If the input_dim is ip0 and output_dim is op0, then shape of the embedding weight matrix must be
(ip0, op0).</p>
<p>When sparse_grad is False, if any index mentioned is too large, it is replaced by the index that
addresses the last vector in an embedding matrix.
When sparse_grad is True, an error will be raised if invalid indices are found.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">5</span>

<span class="o">//</span> <span class="n">Each</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">weight</span> <span class="n">matrix</span> <span class="n">y</span> <span class="n">represents</span> <span class="n">a</span> <span class="n">word</span><span class="o">.</span> <span class="n">So</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">,</span><span class="n">w3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">2.</span><span class="p">,</span>   <span class="mf">3.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">],</span>
     <span class="p">[</span>  <span class="mf">5.</span><span class="p">,</span>   <span class="mf">6.</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">,</span>   <span class="mf">8.</span><span class="p">,</span>   <span class="mf">9.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">10.</span><span class="p">,</span>  <span class="mf">11.</span><span class="p">,</span>  <span class="mf">12.</span><span class="p">,</span>  <span class="mf">13.</span><span class="p">,</span>  <span class="mf">14.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">15.</span><span class="p">,</span>  <span class="mf">16.</span><span class="p">,</span>  <span class="mf">17.</span><span class="p">,</span>  <span class="mf">18.</span><span class="p">,</span>  <span class="mf">19.</span><span class="p">]]</span>

<span class="o">//</span> <span class="n">Input</span> <span class="n">array</span> <span class="n">x</span> <span class="n">represents</span> <span class="n">n</span><span class="o">-</span><span class="n">grams</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="n">gram</span><span class="p">)</span><span class="o">.</span> <span class="n">So</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w1</span><span class="p">,</span><span class="n">w3</span><span class="p">),</span> <span class="p">(</span><span class="n">w0</span><span class="p">,</span><span class="n">w2</span><span class="p">)]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>

<span class="o">//</span> <span class="n">Mapped</span> <span class="nb">input</span> <span class="n">x</span> <span class="n">to</span> <span class="n">its</span> <span class="n">vector</span> <span class="n">representation</span> <span class="n">y</span><span class="o">.</span>
<span class="n">Embedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[[</span>  <span class="mf">5.</span><span class="p">,</span>   <span class="mf">6.</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">,</span>   <span class="mf">8.</span><span class="p">,</span>   <span class="mf">9.</span><span class="p">],</span>
                          <span class="p">[</span> <span class="mf">15.</span><span class="p">,</span>  <span class="mf">16.</span><span class="p">,</span>  <span class="mf">17.</span><span class="p">,</span>  <span class="mf">18.</span><span class="p">,</span>  <span class="mf">19.</span><span class="p">]],</span>

                         <span class="p">[[</span>  <span class="mf">0.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">,</span>   <span class="mf">2.</span><span class="p">,</span>   <span class="mf">3.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">],</span>
                          <span class="p">[</span> <span class="mf">10.</span><span class="p">,</span>  <span class="mf">11.</span><span class="p">,</span>  <span class="mf">12.</span><span class="p">,</span>  <span class="mf">13.</span><span class="p">,</span>  <span class="mf">14.</span><span class="p">]]]</span>
</pre></div>
</div>
<p>The storage type of weight can be either row_sparse or default.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If sparse_grad is set to True, the storage type of gradient w.r.t weights will be
row_sparse. Only a subset of optimizers support sparse gradients, including SGD, AdaGrad
and Adam. Note that by default lazy updates is turned on, which may perform differently
from standard updates. For more details, please check the Optimization API at:
<a class="reference external" href="https://mxnet.incubator.apache.org/api/python/optimization/optimization.html">https://mxnet.incubator.apache.org/api/python/optimization/optimization.html</a></p>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/indexing_op.cc:L602</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array to the embedding operator.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The embedding weight matrix.</p></li>
<li><p><strong>input_dim</strong> (<em>long</em><em>, </em><em>required</em>)  Vocabulary size of the input indices.</p></li>
<li><p><strong>output_dim</strong> (<em>long</em><em>, </em><em>required</em>)  Dimension of the embedding vectors.</p></li>
<li><p><strong>dtype</strong> (<em>{'bfloat16'</em><em>, </em><em>'float16'</em><em>, </em><em>'float32'</em><em>, </em><em>'float64'</em><em>, </em><em>'int32'</em><em>, </em><em>'int64'</em><em>, </em><em>'int8'</em><em>, </em><em>'uint8'}</em><em>,</em><em>optional</em><em>, </em><em>default='float32'</em>)  Data type of weight.</p></li>
<li><p><strong>sparse_grad</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Compute row sparse gradient in the backward calculation. If set to True, the grads storage type is row_sparse.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.FullyConnected">
<code class="sig-name descname">FullyConnected</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">weight=None</em>, <em class="sig-param">bias=None</em>, <em class="sig-param">num_hidden=_Null</em>, <em class="sig-param">no_bias=_Null</em>, <em class="sig-param">flatten=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.FullyConnected" title="Permalink to this definition"></a></dt>
<dd><p>Applies a linear transformation: <span class="math notranslate nohighlight">\(Y = XW^T + b\)</span>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">flatten</span></code> is set to be true, then the shapes are:</p>
<ul class="simple">
<li><p><strong>data</strong>: <cite>(batch_size, x1, x2, , xn)</cite></p></li>
<li><p><strong>weight</strong>: <cite>(num_hidden, x1 * x2 *  * xn)</cite></p></li>
<li><p><strong>bias</strong>: <cite>(num_hidden,)</cite></p></li>
<li><p><strong>out</strong>: <cite>(batch_size, num_hidden)</cite></p></li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">flatten</span></code> is set to be false, then the shapes are:</p>
<ul class="simple">
<li><p><strong>data</strong>: <cite>(x1, x2, , xn, input_dim)</cite></p></li>
<li><p><strong>weight</strong>: <cite>(num_hidden, input_dim)</cite></p></li>
<li><p><strong>bias</strong>: <cite>(num_hidden,)</cite></p></li>
<li><p><strong>out</strong>: <cite>(x1, x2, , xn, num_hidden)</cite></p></li>
</ul>
<p>The learnable parameters include both <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">no_bias</span></code> is set to be true, then the <code class="docutils literal notranslate"><span class="pre">bias</span></code> term is ignored.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sparse support for FullyConnected is limited to forward evaluation with <cite>row_sparse</cite>
weight and bias, where the length of <cite>weight.indices</cite> and <cite>bias.indices</cite> must be equal
to <cite>num_hidden</cite>. This could be useful for model inference with <cite>row_sparse</cite> weights
trained with importance sampling or noise contrastive estimation.</p>
<p>To compute linear transformation with csr sparse data, sparse.dot is recommended instead
of sparse.FullyConnected.</p>
</div>
<p>Defined in /work/mxnet/src/operator/nn/fully_connected.cc:L284</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input data.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight matrix.</p></li>
<li><p><strong>bias</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Bias parameter.</p></li>
<li><p><strong>num_hidden</strong> (<em>int</em><em>, </em><em>required</em>)  Number of hidden nodes of the output.</p></li>
<li><p><strong>no_bias</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Whether to disable bias parameter.</p></li>
<li><p><strong>flatten</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Whether to collapse all but the first axis of the input data tensor.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.LinearRegressionOutput">
<code class="sig-name descname">LinearRegressionOutput</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">label=None</em>, <em class="sig-param">grad_scale=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.LinearRegressionOutput" title="Permalink to this definition"></a></dt>
<dd><p>Computes and optimizes for squared loss during backward propagation.
Just outputs <code class="docutils literal notranslate"><span class="pre">data</span></code> during forward propagation.</p>
<p>If <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value of the i-th sample, and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding target value,
then the squared loss estimated over <span class="math notranslate nohighlight">\(n\)</span> samples is defined as</p>
<p><span class="math notranslate nohighlight">\(\text{SquaredLoss}(\textbf{Y}, \hat{\textbf{Y}} ) = \frac{1}{n} \sum_{i=0}^{n-1} \lVert  \textbf{y}_i - \hat{\textbf{y}}_i  \rVert_2\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the LinearRegressionOutput as the final output layer of a net.</p>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">label</span></code> can be <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">csr</span></code></p>
<ul class="simple">
<li><p>LinearRegressionOutput(default, default) = default</p></li>
<li><p>LinearRegressionOutput(default, csr) = default</p></li>
</ul>
<p>By default, gradients of this loss function are scaled by factor <cite>1/m</cite>, where m is the number of regression outputs of a training example.
The parameter <cite>grad_scale</cite> can be used to change this scale to <cite>grad_scale/m</cite>.</p>
<p>Defined in /work/mxnet/src/operator/regression_output.cc:L92</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input data to the function.</p></li>
<li><p><strong>label</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input label to the function.</p></li>
<li><p><strong>grad_scale</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Scale the gradient by a float factor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.LogisticRegressionOutput">
<code class="sig-name descname">LogisticRegressionOutput</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">label=None</em>, <em class="sig-param">grad_scale=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.LogisticRegressionOutput" title="Permalink to this definition"></a></dt>
<dd><p>Applies a logistic function to the input.</p>
<p>The logistic function, also known as the sigmoid function, is computed as
<span class="math notranslate nohighlight">\(\frac{1}{1+exp(-\textbf{x})}\)</span>.</p>
<p>Commonly, the sigmoid is used to squash the real-valued output of a linear model
<span class="math notranslate nohighlight">\(wTx+b\)</span> into the [0,1] range so that it can be interpreted as a probability.
It is suitable for binary classification or probability prediction tasks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the LogisticRegressionOutput as the final output layer of a net.</p>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">label</span></code> can be <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">csr</span></code></p>
<ul class="simple">
<li><p>LogisticRegressionOutput(default, default) = default</p></li>
<li><p>LogisticRegressionOutput(default, csr) = default</p></li>
</ul>
<p>The loss function used is the Binary Cross Entropy Loss:</p>
<p><span class="math notranslate nohighlight">\(-{(y\log(p) + (1 - y)\log(1 - p))}\)</span></p>
<p>Where <cite>y</cite> is the ground truth probability of positive outcome for a given example, and <cite>p</cite> the probability predicted by the model. By default, gradients of this loss function are scaled by factor <cite>1/m</cite>, where m is the number of regression outputs of a training example.
The parameter <cite>grad_scale</cite> can be used to change this scale to <cite>grad_scale/m</cite>.</p>
<p>Defined in /work/mxnet/src/operator/regression_output.cc:L152</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input data to the function.</p></li>
<li><p><strong>label</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input label to the function.</p></li>
<li><p><strong>grad_scale</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Scale the gradient by a float factor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.MAERegressionOutput">
<code class="sig-name descname">MAERegressionOutput</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">label=None</em>, <em class="sig-param">grad_scale=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.MAERegressionOutput" title="Permalink to this definition"></a></dt>
<dd><p>Computes mean absolute error of the input.</p>
<p>MAE is a risk metric corresponding to the expected value of the absolute error.</p>
<p>If <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the predicted value of the i-th sample, and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding target value,
then the mean absolute error (MAE) estimated over <span class="math notranslate nohighlight">\(n\)</span> samples is defined as</p>
<p><span class="math notranslate nohighlight">\(\text{MAE}(\textbf{Y}, \hat{\textbf{Y}} ) = \frac{1}{n} \sum_{i=0}^{n-1} \lVert \textbf{y}_i - \hat{\textbf{y}}_i \rVert_1\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use the MAERegressionOutput as the final output layer of a net.</p>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">label</span></code> can be <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">csr</span></code></p>
<ul class="simple">
<li><p>MAERegressionOutput(default, default) = default</p></li>
<li><p>MAERegressionOutput(default, csr) = default</p></li>
</ul>
<p>By default, gradients of this loss function are scaled by factor <cite>1/m</cite>, where m is the number of regression outputs of a training example.
The parameter <cite>grad_scale</cite> can be used to change this scale to <cite>grad_scale/m</cite>.</p>
<p>Defined in /work/mxnet/src/operator/regression_output.cc:L120</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input data to the function.</p></li>
<li><p><strong>label</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input label to the function.</p></li>
<li><p><strong>grad_scale</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Scale the gradient by a float factor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.abs">
<code class="sig-name descname">abs</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.abs" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise absolute value of the input.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">abs</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">abs</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>abs(default) = default</p></li>
<li><p>abs(row_sparse) = row_sparse</p></li>
<li><p>abs(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L782</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.adagrad_update">
<code class="sig-name descname">adagrad_update</code><span class="sig-paren">(</span><em class="sig-param">weight=None</em>, <em class="sig-param">grad=None</em>, <em class="sig-param">history=None</em>, <em class="sig-param">lr=_Null</em>, <em class="sig-param">epsilon=_Null</em>, <em class="sig-param">wd=_Null</em>, <em class="sig-param">rescale_grad=_Null</em>, <em class="sig-param">clip_gradient=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.adagrad_update" title="Permalink to this definition"></a></dt>
<dd><p>Update function for AdaGrad optimizer.</p>
<p>Referenced from <em>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</em>,
and available at <a class="reference external" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>.</p>
<p>Updates are applied by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rescaled_grad</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">rescale_grad</span><span class="p">,</span> <span class="n">clip_gradient</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">history</span> <span class="o">+</span> <span class="n">square</span><span class="p">(</span><span class="n">rescaled_grad</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">rescaled_grad</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">history</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that non-zero values for the weight decay option are not supported.</p>
<p>Defined in /work/mxnet/src/operator/optimizer_op.cc:L902</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight</p></li>
<li><p><strong>grad</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Gradient</p></li>
<li><p><strong>history</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  History</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>required</em>)  Learning rate</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1.00000001e-07</em>)  epsilon</p></li>
<li><p><strong>wd</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  weight decay</p></li>
<li><p><strong>rescale_grad</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Rescale gradient to grad = rescale_grad*grad.</p></li>
<li><p><strong>clip_gradient</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=-1</em>)  Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient &lt;= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.adam_update">
<code class="sig-name descname">adam_update</code><span class="sig-paren">(</span><em class="sig-param">weight=None</em>, <em class="sig-param">grad=None</em>, <em class="sig-param">mean=None</em>, <em class="sig-param">var=None</em>, <em class="sig-param">lr=_Null</em>, <em class="sig-param">beta1=_Null</em>, <em class="sig-param">beta2=_Null</em>, <em class="sig-param">epsilon=_Null</em>, <em class="sig-param">wd=_Null</em>, <em class="sig-param">rescale_grad=_Null</em>, <em class="sig-param">clip_gradient=_Null</em>, <em class="sig-param">lazy_update=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.adam_update" title="Permalink to this definition"></a></dt>
<dd><p>Update function for Adam optimizer. Adam is seen as a generalization
of AdaGrad.</p>
<p>Adam update consists of the following steps, where g represents gradient and m, v
are 1st and 2nd order moment estimates (mean and variance).</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_t = \nabla J(W_{t-1})\\
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
W_t = W_{t-1} - \alpha \frac{ m_t }{ \sqrt{ v_t } + \epsilon }\end{split}\]</div>
<p>It updates the weights using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">grad</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">w</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>However, if grads storage type is <code class="docutils literal notranslate"><span class="pre">row_sparse</span></code>, <code class="docutils literal notranslate"><span class="pre">lazy_update</span></code> is True and the storage
type of weight is the same as those of m and v,
only the row slices whose indices appear in grad.indices are updated (for w, m and v):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
    <span class="n">m</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>
    <span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">w</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">])</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/optimizer_op.cc:L681</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight</p></li>
<li><p><strong>grad</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Gradient</p></li>
<li><p><strong>mean</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Moving mean</p></li>
<li><p><strong>var</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Moving variance</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>required</em>)  Learning rate</p></li>
<li><p><strong>beta1</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0.899999976</em>)  The decay rate for the 1st moment estimates.</p></li>
<li><p><strong>beta2</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0.999000013</em>)  The decay rate for the 2nd moment estimates.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=9.99999994e-09</em>)  A small constant for numerical stability.</p></li>
<li><p><strong>wd</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.</p></li>
<li><p><strong>rescale_grad</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Rescale gradient to grad = rescale_grad*grad.</p></li>
<li><p><strong>clip_gradient</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=-1</em>)  Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient &lt;= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).</p></li>
<li><p><strong>lazy_update</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  If true, lazy updates are applied if gradients stype is row_sparse and all of w, m and v have the same stype</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.add_n">
<code class="sig-name descname">add_n</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.add_n" title="Permalink to this definition"></a></dt>
<dd><p>Adds all input arguments element-wise.</p>
<div class="math notranslate nohighlight">
\[add\_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\]</div>
<p><code class="docutils literal notranslate"><span class="pre">add_n</span></code> is potentially more efficient than calling <code class="docutils literal notranslate"><span class="pre">add</span></code> by <cite>n</cite> times.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">add_n</span></code> output depends on storage types of inputs</p>
<ul class="simple">
<li><p>add_n(row_sparse, row_sparse, ..) = row_sparse</p></li>
<li><p>add_n(default, csr, default) = default</p></li>
<li><p>add_n(any input combinations longer than 4 (&gt;4) with at least one default type) = default</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">add_n</span></code> falls all inputs back to default storage and generates default storage</p></li>
</ul>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_sum.cc:L157</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>[</em><em>]</em>)  Positional input arguments</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arccos">
<code class="sig-name descname">arccos</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arccos" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise inverse cosine of the input array.</p>
<p>The input should be in range <cite>[-1, 1]</cite>.
The output is in the closed interval <span class="math notranslate nohighlight">\([0, \pi]\)</span></p>
<div class="math notranslate nohighlight">
\[arccos([-1, -.707, 0, .707, 1]) = [\pi, 3\pi/4, \pi/2, \pi/4, 0]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arccos</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L233</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arccosh">
<code class="sig-name descname">arccosh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arccosh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the element-wise inverse hyperbolic cosine of the input array, computed element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arccosh</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L535</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arcsin">
<code class="sig-name descname">arcsin</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arcsin" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise inverse sine of the input array.</p>
<p>The input should be in the range <cite>[-1, 1]</cite>.
The output is in the closed interval of [<span class="math notranslate nohighlight">\(-\pi/2\)</span>, <span class="math notranslate nohighlight">\(\pi/2\)</span>].</p>
<div class="math notranslate nohighlight">
\[arcsin([-1, -.707, 0, .707, 1]) = [-\pi/2, -\pi/4, 0, \pi/4, \pi/2]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arcsin</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>arcsin(default) = default</p></li>
<li><p>arcsin(row_sparse) = row_sparse</p></li>
<li><p>arcsin(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L187</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arcsinh">
<code class="sig-name descname">arcsinh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arcsinh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the element-wise inverse hyperbolic sine of the input array, computed element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arcsinh</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>arcsinh(default) = default</p></li>
<li><p>arcsinh(row_sparse) = row_sparse</p></li>
<li><p>arcsinh(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L494</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arctan">
<code class="sig-name descname">arctan</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arctan" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise inverse tangent of the input array.</p>
<p>The output is in the closed interval <span class="math notranslate nohighlight">\([-\pi/2, \pi/2]\)</span></p>
<div class="math notranslate nohighlight">
\[arctan([-1, 0, 1]) = [-\pi/4, 0, \pi/4]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arctan</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>arctan(default) = default</p></li>
<li><p>arctan(row_sparse) = row_sparse</p></li>
<li><p>arctan(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L282</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.arctanh">
<code class="sig-name descname">arctanh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.arctanh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the element-wise inverse hyperbolic tangent of the input array, computed element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">arctanh</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>arctanh(default) = default</p></li>
<li><p>arctanh(row_sparse) = row_sparse</p></li>
<li><p>arctanh(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L579</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_add">
<code class="sig-name descname">broadcast_add</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_add" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise sum of the input arrays with broadcasting.</p>
<p><cite>broadcast_plus</cite> is an alias to the function <cite>broadcast_add</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">]]</span>

<span class="n">broadcast_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>

<span class="n">broadcast_plus</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_add(csr, dense(1D)) = dense
broadcast_add(dense(1D), csr) = dense</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L58</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_div">
<code class="sig-name descname">broadcast_div</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_div" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise division of the input arrays with broadcasting.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">2.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">3.</span><span class="p">]]</span>

<span class="n">broadcast_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_div(csr, dense(1D)) = csr</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L187</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_minus">
<code class="sig-name descname">broadcast_minus</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_minus" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise difference of the input arrays with broadcasting.</p>
<p><cite>broadcast_minus</cite> is an alias to the function <cite>broadcast_sub</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">]]</span>

<span class="n">broadcast_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>

<span class="n">broadcast_minus</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_sub/minus(csr, dense(1D)) = dense
broadcast_sub/minus(dense(1D), csr) = dense</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L106</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_mul">
<code class="sig-name descname">broadcast_mul</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_mul" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise product of the input arrays with broadcasting.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">]]</span>

<span class="n">broadcast_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_mul(csr, dense(1D)) = csr</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L146</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_plus">
<code class="sig-name descname">broadcast_plus</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_plus" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise sum of the input arrays with broadcasting.</p>
<p><cite>broadcast_plus</cite> is an alias to the function <cite>broadcast_add</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">]]</span>

<span class="n">broadcast_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>

<span class="n">broadcast_plus</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_add(csr, dense(1D)) = dense
broadcast_add(dense(1D), csr) = dense</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L58</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.broadcast_sub">
<code class="sig-name descname">broadcast_sub</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.broadcast_sub" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise difference of the input arrays with broadcasting.</p>
<p><cite>broadcast_minus</cite> is an alias to the function <cite>broadcast_sub</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">]]</span>

<span class="n">broadcast_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>

<span class="n">broadcast_minus</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                         <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Supported sparse operations:</p>
<blockquote>
<div><p>broadcast_sub/minus(csr, dense(1D)) = dense
broadcast_sub/minus(dense(1D), csr) = dense</p>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_binary_broadcast_op_basic.cc:L106</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  First input to the function</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Second input to the function</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.cast_storage">
<code class="sig-name descname">cast_storage</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">stype=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.cast_storage" title="Permalink to this definition"></a></dt>
<dd><p>Casts tensor storage type to the new type.</p>
<p>When an NDArray with default storage type is cast to csr or row_sparse storage,
the result is compact, which means:</p>
<ul class="simple">
<li><p>for csr, zero values will not be retained</p></li>
<li><p>for row_sparse, row slices of all zeros will not be retained</p></li>
</ul>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">cast_storage</span></code> output depends on stype parameter:</p>
<ul class="simple">
<li><p>cast_storage(csr, default) = default</p></li>
<li><p>cast_storage(row_sparse, default) = default</p></li>
<li><p>cast_storage(default, csr) = csr</p></li>
<li><p>cast_storage(default, row_sparse) = row_sparse</p></li>
<li><p>cast_storage(csr, csr) = csr</p></li>
<li><p>cast_storage(row_sparse, row_sparse) = row_sparse</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dense</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>

<span class="c1"># cast to row_sparse storage type</span>
<span class="n">rsp</span> <span class="o">=</span> <span class="n">cast_storage</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>
<span class="n">rsp</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">rsp</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">]]</span>

<span class="c1"># cast to csr storage type</span>
<span class="n">csr</span> <span class="o">=</span> <span class="n">cast_storage</span><span class="p">(</span><span class="n">dense</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">)</span>
<span class="n">csr</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">csr</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">]</span>
<span class="n">csr</span><span class="o">.</span><span class="n">indptr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/cast_storage.cc:L71</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input.</p></li>
<li><p><strong>stype</strong> (<em>{'csr'</em><em>, </em><em>'default'</em><em>, </em><em>'row_sparse'}</em><em>, </em><em>required</em>)  Output storage type.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.cbrt">
<code class="sig-name descname">cbrt</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.cbrt" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise cube-root value of the input.</p>
<div class="math notranslate nohighlight">
\[cbrt(x) = \sqrt[3]{x}\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cbrt</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">125</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">cbrt</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>cbrt(default) = default</p></li>
<li><p>cbrt(row_sparse) = row_sparse</p></li>
<li><p>cbrt(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_pow.cc:L270</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.ceil">
<code class="sig-name descname">ceil</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.ceil" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise ceiling of the input.</p>
<p>The ceil of the scalar x is the smallest integer i, such that i &gt;= x.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceil</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">ceil</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>ceil(default) = default</p></li>
<li><p>ceil(row_sparse) = row_sparse</p></li>
<li><p>ceil(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L877</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.clip">
<code class="sig-name descname">clip</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">a_min=_Null</em>, <em class="sig-param">a_max=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.clip" title="Permalink to this definition"></a></dt>
<dd><p>Clips (limits) the values in an array.
Given an interval, values outside the interval are clipped to the interval edges.
Clipping <code class="docutils literal notranslate"><span class="pre">x</span></code> between <cite>a_min</cite> and <cite>a_max</cite> would be
.. math:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a_min</span><span class="p">,</span> <span class="n">a_max</span><span class="p">)</span> <span class="o">=</span> \<span class="nb">max</span><span class="p">(</span>\<span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a_max</span><span class="p">),</span> <span class="n">a_min</span><span class="p">))</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">clip</span></code> output depends on storage types of inputs and the a_min, a_max parameter values:
* clip(default) = default
* clip(row_sparse, a_min &lt;= 0, a_max &gt;= 0) = row_sparse
* clip(csr, a_min &lt;= 0, a_max &gt;= 0) = csr
* clip(row_sparse, a_min &lt; 0, a_max &lt; 0) = default
* clip(row_sparse, a_min &gt; 0, a_max &gt; 0) = default
* clip(csr, a_min &lt; 0, a_max &lt; 0) = csr
* clip(csr, a_min &gt; 0, a_max &gt; 0) = csr</p>
<p>Defined in /work/mxnet/src/operator/tensor/matrix_op.cc:L708</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Input array.</p></li>
<li><p><strong>a_min</strong> (<em>float</em><em>, </em><em>required</em>)  Minimum value</p></li>
<li><p><strong>a_max</strong> (<em>float</em><em>, </em><em>required</em>)  Maximum value</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.concat">
<code class="sig-name descname">concat</code><span class="sig-paren">(</span><em class="sig-param">*data</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.concat" title="Permalink to this definition"></a></dt>
<dd><p>Joins input arrays along a given axis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>Concat</cite> is deprecated. Use <cite>concat</cite> instead.</p>
</div>
<p>The dimensions of the input arrays should be the same except the axis along
which they will be concatenated.
The dimension of the output array along the concatenated axis will be equal
to the sum of the corresponding dimensions of the input arrays.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">concat</span></code> output depends on storage types of inputs</p>
<ul class="simple">
<li><p>concat(csr, csr, , csr, dim=0) = csr</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">concat</span></code> generates output with default storage</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">]]</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">]]</span>

<span class="n">concat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
                       <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">]]</span>

<span class="n">Note</span> <span class="n">that</span> <span class="n">you</span> <span class="n">cannot</span> <span class="n">concat</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span> <span class="n">along</span> <span class="n">dimension</span> <span class="mi">1</span> <span class="n">since</span> <span class="n">dimension</span>
<span class="mi">0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">the</span> <span class="n">same</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">arrays</span><span class="o">.</span>

<span class="n">concat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/nn/concat.cc:L386</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>[</em><em>]</em>)  List of arrays to concatenate</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default='1'</em>)  the dimension to be concated.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.cos">
<code class="sig-name descname">cos</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.cos" title="Permalink to this definition"></a></dt>
<dd><p>Computes the element-wise cosine of the input array.</p>
<p>The input should be in radians (<span class="math notranslate nohighlight">\(2\pi\)</span> rad equals 360 degrees).</p>
<div class="math notranslate nohighlight">
\[cos([0, \pi/4, \pi/2]) = [1, 0.707, 0]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">cos</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L90</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.cosh">
<code class="sig-name descname">cosh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.cosh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the hyperbolic cosine  of the input array, computed element-wise.</p>
<div class="math notranslate nohighlight">
\[cosh(x) = 0.5\times(exp(x) + exp(-x))\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">cosh</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L409</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.degrees">
<code class="sig-name descname">degrees</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.degrees" title="Permalink to this definition"></a></dt>
<dd><p>Converts each element of the input array from radians to degrees.</p>
<div class="math notranslate nohighlight">
\[degrees([0, \pi/2, \pi, 3\pi/2, 2\pi]) = [0, 90, 180, 270, 360]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">degrees</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>degrees(default) = default</p></li>
<li><p>degrees(row_sparse) = row_sparse</p></li>
<li><p>degrees(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L332</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.digamma">
<code class="sig-name descname">digamma</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.digamma" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise log derivative of the gamma function of the input.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">digamma</span></code> output is always dense</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.dot">
<code class="sig-name descname">dot</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">transpose_a=_Null</em>, <em class="sig-param">transpose_b=_Null</em>, <em class="sig-param">forward_stype=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.dot" title="Permalink to this definition"></a></dt>
<dd><p>Dot product of two arrays.</p>
<p><code class="docutils literal notranslate"><span class="pre">dot</span></code>s behavior depends on the input array dimensions:</p>
<ul>
<li><p>1-D arrays: inner product of vectors</p></li>
<li><p>2-D arrays: matrix multiplication</p></li>
<li><p>N-D arrays: a sum product over the last axis of the first input and the first
axis of the second input</p>
<p>For example, given 3-D <code class="docutils literal notranslate"><span class="pre">x</span></code> with shape <cite>(n,m,k)</cite> and <code class="docutils literal notranslate"><span class="pre">y</span></code> with shape <cite>(k,r,s)</cite>, the
result array will have shape <cite>(n,m,r,s)</cite>. It is computed by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,:]</span><span class="o">*</span><span class="n">y</span><span class="p">[:,</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">])</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span><span class="o">*</span><span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</li>
</ul>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">dot</span></code> output depends on storage types of inputs, transpose option and
forward_stype option for output storage type. Implemented sparse operations include:</p>
<ul class="simple">
<li><p>dot(default, default, transpose_a=True/False, transpose_b=True/False) = default</p></li>
<li><p>dot(csr, default, transpose_a=True) = default</p></li>
<li><p>dot(csr, default, transpose_a=True) = row_sparse</p></li>
<li><p>dot(csr, default) = default</p></li>
<li><p>dot(csr, row_sparse) = default</p></li>
<li><p>dot(default, csr) = csr (CPU only)</p></li>
<li><p>dot(default, csr, forward_stype=default) = default</p></li>
<li><p>dot(default, csr, transpose_b=True, forward_stype=default) = default</p></li>
</ul>
<p>If the combination of input storage types and forward_stype does not match any of the
above patterns, <code class="docutils literal notranslate"><span class="pre">dot</span></code> will fallback and generate output with default storage.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the storage type of the lhs is csr, the storage type of gradient w.r.t rhs will be
row_sparse. Only a subset of optimizers support sparse gradients, including SGD, AdaGrad
and Adam. Note that by default lazy updates is turned on, which may perform differently
from standard updates. For more details, please check the Optimization API at:
<a class="reference external" href="https://mxnet.incubator.apache.org/api/python/optimization/optimization.html">https://mxnet.incubator.apache.org/api/python/optimization/optimization.html</a></p>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/dot.cc:L81</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The second input</p></li>
<li><p><strong>transpose_a</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  If true then transpose the first input before dot.</p></li>
<li><p><strong>transpose_b</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  If true then transpose the second input before dot.</p></li>
<li><p><strong>forward_stype</strong> (<em>{None</em><em>, </em><em>'csr'</em><em>, </em><em>'default'</em><em>, </em><em>'row_sparse'}</em><em>,</em><em>optional</em><em>, </em><em>default='None'</em>)  The desired storage type of the forward output given by user, if thecombination of input storage types and this hint does not matchany implemented ones, the dot operator will perform fallback operationand still produce an output of the desired storage type.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.elemwise_add">
<code class="sig-name descname">elemwise_add</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.elemwise_add" title="Permalink to this definition"></a></dt>
<dd><p>Adds arguments element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">elemwise_add</span></code> output depends on storage types of inputs</p>
<blockquote>
<div><ul class="simple">
<li><p>elemwise_add(row_sparse, row_sparse) = row_sparse</p></li>
<li><p>elemwise_add(csr, csr) = csr</p></li>
<li><p>elemwise_add(default, csr) = default</p></li>
<li><p>elemwise_add(csr, default) = default</p></li>
<li><p>elemwise_add(default, rsp) = default</p></li>
<li><p>elemwise_add(rsp, default) = default</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">elemwise_add</span></code> generates output with default storage</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  first input</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  second input</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.elemwise_div">
<code class="sig-name descname">elemwise_div</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.elemwise_div" title="Permalink to this definition"></a></dt>
<dd><p>Divides arguments element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">elemwise_div</span></code> output is always dense</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  first input</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  second input</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.elemwise_mul">
<code class="sig-name descname">elemwise_mul</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.elemwise_mul" title="Permalink to this definition"></a></dt>
<dd><p>Multiplies arguments element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">elemwise_mul</span></code> output depends on storage types of inputs</p>
<blockquote>
<div><ul class="simple">
<li><p>elemwise_mul(default, default) = default</p></li>
<li><p>elemwise_mul(row_sparse, row_sparse) = row_sparse</p></li>
<li><p>elemwise_mul(default, row_sparse) = row_sparse</p></li>
<li><p>elemwise_mul(row_sparse, default) = row_sparse</p></li>
<li><p>elemwise_mul(csr, csr) = csr</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">elemwise_mul</span></code> generates output with default storage</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  first input</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  second input</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.elemwise_sub">
<code class="sig-name descname">elemwise_sub</code><span class="sig-paren">(</span><em class="sig-param">lhs=None</em>, <em class="sig-param">rhs=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.elemwise_sub" title="Permalink to this definition"></a></dt>
<dd><p>Subtracts arguments element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">elemwise_sub</span></code> output depends on storage types of inputs</p>
<blockquote>
<div><ul class="simple">
<li><p>elemwise_sub(row_sparse, row_sparse) = row_sparse</p></li>
<li><p>elemwise_sub(csr, csr) = csr</p></li>
<li><p>elemwise_sub(default, csr) = default</p></li>
<li><p>elemwise_sub(csr, default) = default</p></li>
<li><p>elemwise_sub(default, rsp) = default</p></li>
<li><p>elemwise_sub(rsp, default) = default</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">elemwise_sub</span></code> generates output with default storage</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  first input</p></li>
<li><p><strong>rhs</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  second input</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.exp">
<code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.exp" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise exponential value of the input.</p>
<div class="math notranslate nohighlight">
\[exp(x) = e^x \approx 2.718^x\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.71828175</span><span class="p">,</span> <span class="mf">7.38905621</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">exp</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L64</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.expm1">
<code class="sig-name descname">expm1</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.expm1" title="Permalink to this definition"></a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">exp(x)</span> <span class="pre">-</span> <span class="pre">1</span></code> computed element-wise on the input.</p>
<p>This function provides greater precision than <code class="docutils literal notranslate"><span class="pre">exp(x)</span> <span class="pre">-</span> <span class="pre">1</span></code> for small values of <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">expm1</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>expm1(default) = default</p></li>
<li><p>expm1(row_sparse) = row_sparse</p></li>
<li><p>expm1(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L244</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.fix">
<code class="sig-name descname">fix</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.fix" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise rounded value to the nearest integer towards zero of the input.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fix</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">fix</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>fix(default) = default</p></li>
<li><p>fix(row_sparse) = row_sparse</p></li>
<li><p>fix(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L934</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.floor">
<code class="sig-name descname">floor</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.floor" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise floor of the input.</p>
<p>The floor of the scalar x is the largest integer i, such that i &lt;= x.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">floor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">floor</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>floor(default) = default</p></li>
<li><p>floor(row_sparse) = row_sparse</p></li>
<li><p>floor(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L896</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.ftrl_update">
<code class="sig-name descname">ftrl_update</code><span class="sig-paren">(</span><em class="sig-param">weight=None</em>, <em class="sig-param">grad=None</em>, <em class="sig-param">z=None</em>, <em class="sig-param">n=None</em>, <em class="sig-param">lr=_Null</em>, <em class="sig-param">lamda1=_Null</em>, <em class="sig-param">beta=_Null</em>, <em class="sig-param">wd=_Null</em>, <em class="sig-param">rescale_grad=_Null</em>, <em class="sig-param">clip_gradient=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.ftrl_update" title="Permalink to this definition"></a></dt>
<dd><p>Update function for Ftrl optimizer.
Referenced from <em>Ad Click Prediction: a View from the Trenches</em>, available at
<a class="reference external" href="http://dl.acm.org/citation.cfm?id=2488200">http://dl.acm.org/citation.cfm?id=2488200</a>.</p>
<p>It updates the weights using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rescaled_grad</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">rescale_grad</span><span class="p">,</span> <span class="n">clip_gradient</span><span class="p">)</span>
<span class="n">z</span> <span class="o">+=</span> <span class="n">rescaled_grad</span> <span class="o">-</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">rescaled_grad</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">/</span> <span class="n">learning_rate</span>
<span class="n">n</span> <span class="o">+=</span> <span class="n">rescaled_grad</span><span class="o">**</span><span class="mi">2</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">(</span><span class="n">sign</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="n">lamda1</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">beta</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">/</span> <span class="n">learning_rate</span> <span class="o">+</span> <span class="n">wd</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">lamda1</span><span class="p">)</span>
</pre></div>
</div>
<p>If w, z and n are all of <code class="docutils literal notranslate"><span class="pre">row_sparse</span></code> storage type,
only the row slices whose indices appear in grad.indices are updated (for w, z and n):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
    <span class="n">rescaled_grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">*</span> <span class="n">rescale_grad</span><span class="p">,</span> <span class="n">clip_gradient</span><span class="p">)</span>
    <span class="n">z</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rescaled_grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+</span> <span class="n">rescaled_grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">[</span><span class="n">row</span><span class="p">]))</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">/</span> <span class="n">learning_rate</span>
    <span class="n">n</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rescaled_grad</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">w</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">sign</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">row</span><span class="p">])</span> <span class="o">*</span> <span class="n">lamda1</span> <span class="o">-</span> <span class="n">z</span><span class="p">[</span><span class="n">row</span><span class="p">])</span> <span class="o">/</span> <span class="p">((</span><span class="n">beta</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">[</span><span class="n">row</span><span class="p">]))</span> <span class="o">/</span> <span class="n">learning_rate</span> <span class="o">+</span> <span class="n">wd</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">row</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">lamda1</span><span class="p">)</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/optimizer_op.cc:L869</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight</p></li>
<li><p><strong>grad</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Gradient</p></li>
<li><p><strong>z</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  z</p></li>
<li><p><strong>n</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Square of grad</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>required</em>)  Learning rate</p></li>
<li><p><strong>lamda1</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0.00999999978</em>)  The L1 regularization coefficient.</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Per-Coordinate Learning Rate beta.</p></li>
<li><p><strong>wd</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.</p></li>
<li><p><strong>rescale_grad</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Rescale gradient to grad = rescale_grad*grad.</p></li>
<li><p><strong>clip_gradient</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=-1</em>)  Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient &lt;= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.gamma">
<code class="sig-name descname">gamma</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.gamma" title="Permalink to this definition"></a></dt>
<dd><p>Returns the gamma function (extension of the factorial function to the reals), computed element-wise on the input array.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> output is always dense</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.gammaln">
<code class="sig-name descname">gammaln</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.gammaln" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise log of the absolute value of the gamma function of the input.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">gammaln</span></code> output is always dense</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.log">
<code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.log" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise Natural logarithmic value of the input.</p>
<p>The natural logarithm is logarithm in base <em>e</em>, so that <code class="docutils literal notranslate"><span class="pre">log(exp(x))</span> <span class="pre">=</span> <span class="pre">x</span></code></p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">log</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L77</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.log10">
<code class="sig-name descname">log10</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.log10" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise Base-10 logarithmic value of the input.</p>
<p><code class="docutils literal notranslate"><span class="pre">10**log10(x)</span> <span class="pre">=</span> <span class="pre">x</span></code></p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">log10</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L94</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.log1p">
<code class="sig-name descname">log1p</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.log1p" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise <code class="docutils literal notranslate"><span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">x)</span></code> value of the input.</p>
<p>This function is more accurate than <code class="docutils literal notranslate"><span class="pre">log(1</span> <span class="pre">+</span> <span class="pre">x)</span></code>  for small <code class="docutils literal notranslate"><span class="pre">x</span></code> so that
<span class="math notranslate nohighlight">\(1+x\approx 1\)</span></p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">log1p</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>log1p(default) = default</p></li>
<li><p>log1p(row_sparse) = row_sparse</p></li>
<li><p>log1p(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L199</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.log2">
<code class="sig-name descname">log2</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.log2" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise Base-2 logarithmic value of the input.</p>
<p><code class="docutils literal notranslate"><span class="pre">2**log2(x)</span> <span class="pre">=</span> <span class="pre">x</span></code></p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">log2</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_logexp.cc:L106</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.log_sigmoid">
<code class="sig-name descname">log_sigmoid</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.log_sigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Computes log_sigmoid of x element-wise.</p>
<div class="math notranslate nohighlight">
\[y = log(1 / (1 + exp(-x)))\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">log_sigmoid</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L162</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.make_loss">
<code class="sig-name descname">make_loss</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.make_loss" title="Permalink to this definition"></a></dt>
<dd><p>Make your own loss function in network construction.</p>
<p>This operator accepts a customized loss function symbol as a terminal loss and
the symbol should be an operator with no backward dependency.
The output of this function is the gradient of loss with respect to the input data.</p>
<p>For example, if you are a making a cross entropy loss function. Assume <code class="docutils literal notranslate"><span class="pre">out</span></code> is the
predicted output and <code class="docutils literal notranslate"><span class="pre">label</span></code> is the true label, then the cross entropy can be defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">label</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">make_loss</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</pre></div>
</div>
<p>We will need to use <code class="docutils literal notranslate"><span class="pre">make_loss</span></code> when we are creating our own loss function or we want to
combine multiple loss functions. Also we may want to stop some variables gradients
from backpropagation. See more detail in <code class="docutils literal notranslate"><span class="pre">BlockGrad</span></code> or <code class="docutils literal notranslate"><span class="pre">stop_gradient</span></code>.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">make_loss</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>make_loss(default) = default</p></li>
<li><p>make_loss(row_sparse) = row_sparse</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L420</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.mean">
<code class="sig-name descname">mean</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">axis=_Null</em>, <em class="sig-param">keepdims=_Null</em>, <em class="sig-param">exclude=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.mean" title="Permalink to this definition"></a></dt>
<dd><p>Computes the mean of array elements over given axes.</p>
<p>Defined in /work/mxnet/src/operator/tensor/./broadcast_reduce_op.h:L84</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input</p></li>
<li><p><strong>axis</strong> (<em>Shape</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>)  <p>The axis or axes along which to perform the reduction.</p>
<blockquote>
<div><p>The default, <cite>axis=()</cite>, will compute over all elements into a
scalar array with shape <cite>(1,)</cite>.</p>
<p>If <cite>axis</cite> is int, a reduction is performed on a particular axis.</p>
<p>If <cite>axis</cite> is a tuple of ints, a reduction is performed on all the axes
specified in the tuple.</p>
<p>If <cite>exclude</cite> is true, reduction will be performed on the axes that are
NOT in axis instead.</p>
<p>Negative values means indexing from right to left.</p>
</div></blockquote>
</p></li>
<li><p><strong>keepdims</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  If this is set to <cite>True</cite>, the reduced axes are left in the result as dimension with size one.</p></li>
<li><p><strong>exclude</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Whether to perform reduction on axis that are NOT in axis instead.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.mish">
<code class="sig-name descname">mish</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.mish" title="Permalink to this definition"></a></dt>
<dd><p>Computes mish of x element-wise.</p>
<div class="math notranslate nohighlight">
\[y = x * tanh(log(1 + exp(x)))\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">mish</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L203</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.negative">
<code class="sig-name descname">negative</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.negative" title="Permalink to this definition"></a></dt>
<dd><p>Numerical negative of the argument, element-wise.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">negative</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>negative(default) = default</p></li>
<li><p>negative(row_sparse) = row_sparse</p></li>
<li><p>negative(csr) = csr</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.norm">
<code class="sig-name descname">norm</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">ord=_Null</em>, <em class="sig-param">axis=_Null</em>, <em class="sig-param">out_dtype=_Null</em>, <em class="sig-param">keepdims=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.norm" title="Permalink to this definition"></a></dt>
<dd><p>Computes the norm on an NDArray.</p>
<p>This operator computes the norm on an NDArray with the specified axis, depending
on the value of the ord parameter. By default, it computes the L2 norm on the entire
array. Currently only ord=2 supports sparse ndarrays.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
     <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]]</span>

<span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">3.1622777</span> <span class="mf">4.472136</span> <span class="p">]</span>
                          <span class="p">[</span><span class="mf">5.3851647</span> <span class="mf">6.3245554</span><span class="p">]]</span>

<span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">],</span>
                          <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]]</span>

<span class="n">rsp</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cast_storage</span><span class="p">(</span><span class="s1">&#39;row_sparse&#39;</span><span class="p">)</span>

<span class="n">norm</span><span class="p">(</span><span class="n">rsp</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.47722578</span><span class="p">]</span>

<span class="n">csr</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cast_storage</span><span class="p">(</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

<span class="n">norm</span><span class="p">(</span><span class="n">csr</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.47722578</span><span class="p">]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/broadcast_reduce_norm_value.cc:L89</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input</p></li>
<li><p><strong>ord</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default='2'</em>)  Order of the norm. Currently ord=1 and ord=2 is supported.</p></li>
<li><p><strong>axis</strong> (<em>Shape</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>)  <dl class="simple">
<dt>The axis or axes along which to perform the reduction.</dt><dd><p>The default, <cite>axis=()</cite>, will compute over all elements into a
scalar array with shape <cite>(1,)</cite>.
If <cite>axis</cite> is int, a reduction is performed on a particular axis.
If <cite>axis</cite> is a 2-tuple, it specifies the axes that hold 2-D matrices,
and the matrix norms of these matrices are computed.</p>
</dd>
</dl>
</p></li>
<li><p><strong>out_dtype</strong> (<em>{None</em><em>, </em><em>'float16'</em><em>, </em><em>'float32'</em><em>, </em><em>'float64'</em><em>, </em><em>'int32'</em><em>, </em><em>'int64'</em><em>, </em><em>'int8'}</em><em>,</em><em>optional</em><em>, </em><em>default='None'</em>)  The data type of the output.</p></li>
<li><p><strong>keepdims</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  If this is set to <cite>True</cite>, the reduced axis is left in the result as dimension with size one.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.radians">
<code class="sig-name descname">radians</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.radians" title="Permalink to this definition"></a></dt>
<dd><p>Converts each element of the input array from degrees to radians.</p>
<div class="math notranslate nohighlight">
\[radians([0, 90, 180, 270, 360]) = [0, \pi/2, \pi, 3\pi/2, 2\pi]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">radians</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>radians(default) = default</p></li>
<li><p>radians(row_sparse) = row_sparse</p></li>
<li><p>radians(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L351</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.relu">
<code class="sig-name descname">relu</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.relu" title="Permalink to this definition"></a></dt>
<dd><p>Computes rectified linear activation.</p>
<div class="math notranslate nohighlight">
\[max(features, 0)\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">relu</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>relu(default) = default</p></li>
<li><p>relu(row_sparse) = row_sparse</p></li>
<li><p>relu(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L85</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.retain">
<code class="sig-name descname">retain</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">indices=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.retain" title="Permalink to this definition"></a></dt>
<dd><p>Pick rows specified by user input index array from a row sparse matrix
and save them in the output sparse matrix.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">rsp_in</span> <span class="o">=</span> <span class="n">row_sparse_array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="n">to_retain</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">rsp_out</span> <span class="o">=</span> <span class="n">retain</span><span class="p">(</span><span class="n">rsp_in</span><span class="p">,</span> <span class="n">to_retain</span><span class="p">)</span>
<span class="n">rsp_out</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">rsp_out</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">retain</span></code> output depends on storage types of inputs</p>
<ul class="simple">
<li><p>retain(row_sparse, default) = row_sparse</p></li>
<li><p>otherwise, <code class="docutils literal notranslate"><span class="pre">retain</span></code> is not supported</p></li>
</ul>
<p>Defined in /work/mxnet/src/operator/tensor/sparse_retain.cc:L53</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array for sparse_retain operator.</p></li>
<li><p><strong>indices</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The index array of rows ids that will be retained.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.rint">
<code class="sig-name descname">rint</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.rint" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise rounded value to the nearest integer of the input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For input <code class="docutils literal notranslate"><span class="pre">n.5</span></code> <code class="docutils literal notranslate"><span class="pre">rint</span></code> returns <code class="docutils literal notranslate"><span class="pre">n</span></code> while <code class="docutils literal notranslate"><span class="pre">round</span></code> returns <code class="docutils literal notranslate"><span class="pre">n+1</span></code>.</p></li>
<li><p>For input <code class="docutils literal notranslate"><span class="pre">-n.5</span></code> both <code class="docutils literal notranslate"><span class="pre">rint</span></code> and <code class="docutils literal notranslate"><span class="pre">round</span></code> returns <code class="docutils literal notranslate"><span class="pre">-n-1</span></code>.</p></li>
</ul>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rint</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">rint</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>rint(default) = default</p></li>
<li><p>rint(row_sparse) = row_sparse</p></li>
<li><p>rint(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L858</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.round">
<code class="sig-name descname">round</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.round" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise rounded value to the nearest integer of the input.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">round</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">round</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>round(default) = default</p></li>
<li><p>round(row_sparse) = row_sparse</p></li>
<li><p>round(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L837</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.rsqrt">
<code class="sig-name descname">rsqrt</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.rsqrt" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise inverse square-root value of the input.</p>
<div class="math notranslate nohighlight">
\[rsqrt(x) = 1/\sqrt{x}\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rsqrt</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.33333334</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">rsqrt</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_pow.cc:L221</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sgd_mom_update">
<code class="sig-name descname">sgd_mom_update</code><span class="sig-paren">(</span><em class="sig-param">weight=None</em>, <em class="sig-param">grad=None</em>, <em class="sig-param">mom=None</em>, <em class="sig-param">lr=_Null</em>, <em class="sig-param">momentum=_Null</em>, <em class="sig-param">wd=_Null</em>, <em class="sig-param">rescale_grad=_Null</em>, <em class="sig-param">clip_gradient=_Null</em>, <em class="sig-param">lazy_update=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sgd_mom_update" title="Permalink to this definition"></a></dt>
<dd><p>Momentum update function for Stochastic Gradient Descent (SGD) optimizer.</p>
<p>Momentum update has better convergence rates on neural networks. Mathematically it looks
like below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}v_1 = \alpha * \nabla J(W_0)\\
v_t = \gamma v_{t-1} - \alpha * \nabla J(W_{t-1})\\
W_t = W_{t-1} + v_t\end{split}\]</div>
<p>It updates the weights using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
<span class="n">weight</span> <span class="o">+=</span> <span class="n">v</span>
</pre></div>
</div>
<p>Where the parameter <code class="docutils literal notranslate"><span class="pre">momentum</span></code> is the decay rate of momentum estimates at each epoch.</p>
<p>However, if grads storage type is <code class="docutils literal notranslate"><span class="pre">row_sparse</span></code>, <code class="docutils literal notranslate"><span class="pre">lazy_update</span></code> is True and weights storage
type is the same as momentums storage type,
only the row slices whose indices appear in grad.indices are updated (for both weight and momentum):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">gradient</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
    <span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/optimizer_op.cc:L558</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight</p></li>
<li><p><strong>grad</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Gradient</p></li>
<li><p><strong>mom</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Momentum</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>required</em>)  Learning rate</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  The decay rate of momentum estimates at each epoch.</p></li>
<li><p><strong>wd</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.</p></li>
<li><p><strong>rescale_grad</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Rescale gradient to grad = rescale_grad*grad.</p></li>
<li><p><strong>clip_gradient</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=-1</em>)  Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient &lt;= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).</p></li>
<li><p><strong>lazy_update</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  If true, lazy updates are applied if gradients stype is row_sparse and both weight and momentum have the same stype</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sgd_update">
<code class="sig-name descname">sgd_update</code><span class="sig-paren">(</span><em class="sig-param">weight=None</em>, <em class="sig-param">grad=None</em>, <em class="sig-param">lr=_Null</em>, <em class="sig-param">wd=_Null</em>, <em class="sig-param">rescale_grad=_Null</em>, <em class="sig-param">clip_gradient=_Null</em>, <em class="sig-param">lazy_update=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sgd_update" title="Permalink to this definition"></a></dt>
<dd><p>Update function for Stochastic Gradient Descent (SGD) optimizer.</p>
<p>It updates the weights using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">gradient</span> <span class="o">+</span> <span class="n">wd</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
<p>However, if gradient is of <code class="docutils literal notranslate"><span class="pre">row_sparse</span></code> storage type and <code class="docutils literal notranslate"><span class="pre">lazy_update</span></code> is True,
only the row slices whose indices appear in grad.indices are updated:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">gradient</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">gradient</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+</span> <span class="n">wd</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[</span><span class="n">row</span><span class="p">])</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/optimizer_op.cc:L517</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Weight</p></li>
<li><p><strong>grad</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Gradient</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>required</em>)  Learning rate</p></li>
<li><p><strong>wd</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Weight decay augments the objective function with a regularization term that penalizes large weights. The penalty scales with the square of the magnitude of each weight.</p></li>
<li><p><strong>rescale_grad</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  Rescale gradient to grad = rescale_grad*grad.</p></li>
<li><p><strong>clip_gradient</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=-1</em>)  Clip gradient to the range of [-clip_gradient, clip_gradient] If clip_gradient &lt;= 0, gradient clipping is turned off. grad = max(min(grad, clip_gradient), -clip_gradient).</p></li>
<li><p><strong>lazy_update</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>)  If true, lazy updates are applied if gradients stype is row_sparse.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sigmoid">
<code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Computes sigmoid of x element-wise.</p>
<div class="math notranslate nohighlight">
\[y = 1 / (1 + exp(-x))\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> output is always dense</p>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L119</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sign">
<code class="sig-name descname">sign</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sign" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise sign of the input.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sign</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">sign</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>sign(default) = default</p></li>
<li><p>sign(row_sparse) = row_sparse</p></li>
<li><p>sign(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L820</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sin">
<code class="sig-name descname">sin</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sin" title="Permalink to this definition"></a></dt>
<dd><p>Computes the element-wise sine of the input array.</p>
<p>The input should be in radians (<span class="math notranslate nohighlight">\(2\pi\)</span> rad equals 360 degrees).</p>
<div class="math notranslate nohighlight">
\[sin([0, \pi/4, \pi/2]) = [0, 0.707, 1]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">sin</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>sin(default) = default</p></li>
<li><p>sin(row_sparse) = row_sparse</p></li>
<li><p>sin(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L47</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sinh">
<code class="sig-name descname">sinh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sinh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the hyperbolic sine of the input array, computed element-wise.</p>
<div class="math notranslate nohighlight">
\[sinh(x) = 0.5\times(exp(x) - exp(-x))\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">sinh</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>sinh(default) = default</p></li>
<li><p>sinh(row_sparse) = row_sparse</p></li>
<li><p>sinh(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L371</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.slice">
<code class="sig-name descname">slice</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">begin=_Null</em>, <em class="sig-param">end=_Null</em>, <em class="sig-param">step=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.slice" title="Permalink to this definition"></a></dt>
<dd><p>Slices a region of the array.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">crop</span></code> is deprecated. Use <code class="docutils literal notranslate"><span class="pre">slice</span></code> instead.</p>
</div>
<p>This function returns a sliced array between the indices given
by <cite>begin</cite> and <cite>end</cite> with the corresponding <cite>step</cite>.
For an input array of <code class="docutils literal notranslate"><span class="pre">shape=(d_0,</span> <span class="pre">d_1,</span> <span class="pre">...,</span> <span class="pre">d_n-1)</span></code>,
slice operation with <code class="docutils literal notranslate"><span class="pre">begin=(b_0,</span> <span class="pre">b_1...b_m-1)</span></code>,
<code class="docutils literal notranslate"><span class="pre">end=(e_0,</span> <span class="pre">e_1,</span> <span class="pre">...,</span> <span class="pre">e_m-1)</span></code>, and <code class="docutils literal notranslate"><span class="pre">step=(s_0,</span> <span class="pre">s_1,</span> <span class="pre">...,</span> <span class="pre">s_m-1)</span></code>,
where m &lt;= n, results in an array with the shape
<code class="docutils literal notranslate"><span class="pre">(|e_0-b_0|/|s_0|,</span> <span class="pre">...,</span> <span class="pre">|e_m-1-b_m-1|/|s_m-1|,</span> <span class="pre">d_m,</span> <span class="pre">...,</span> <span class="pre">d_n-1)</span></code>.
The resulting arrays <em>k</em>-th dimension contains elements
from the <em>k</em>-th dimension of the input array starting
from index <code class="docutils literal notranslate"><span class="pre">b_k</span></code> (inclusive) with step <code class="docutils literal notranslate"><span class="pre">s_k</span></code>
until reaching <code class="docutils literal notranslate"><span class="pre">e_k</span></code> (exclusive).
If the <em>k</em>-th elements are <cite>None</cite> in the sequence of <cite>begin</cite>, <cite>end</cite>,
and <cite>step</cite>, the following rule will be used to set default values.
If <cite>s_k</cite> is <cite>None</cite>, set <cite>s_k=1</cite>. If <cite>s_k &gt; 0</cite>, set <cite>b_k=0</cite>, <cite>e_k=d_k</cite>;
else, set <cite>b_k=d_k-1</cite>, <cite>e_k=-1</cite>.
The storage type of <code class="docutils literal notranslate"><span class="pre">slice</span></code> output depends on storage types of inputs
* slice(csr) = csr
* otherwise, <code class="docutils literal notranslate"><span class="pre">slice</span></code> generates output with default storage</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When input data storage type is csr, it only supports
step=(), or step=(None,), or step=(1,) to generate a csr output.
For other step parameter values, it falls back to slicing
a dense tensor.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span>  <span class="mf">1.</span><span class="p">,</span>   <span class="mf">2.</span><span class="p">,</span>   <span class="mf">3.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">],</span>
     <span class="p">[</span>  <span class="mf">5.</span><span class="p">,</span>   <span class="mf">6.</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">,</span>   <span class="mf">8.</span><span class="p">],</span>
     <span class="p">[</span>  <span class="mf">9.</span><span class="p">,</span>  <span class="mf">10.</span><span class="p">,</span>  <span class="mf">11.</span><span class="p">,</span>  <span class="mf">12.</span><span class="p">]]</span>
<span class="nb">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">],</span>
                                   <span class="p">[</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">]]</span>
<span class="nb">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">step</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">],</span>
                                                          <span class="p">[</span><span class="mf">5.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">],</span>
                                                          <span class="p">[</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/matrix_op.cc:L506</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  Source input</p></li>
<li><p><strong>begin</strong> (<em>tuple of &lt;&gt;</em><em>, </em><em>required</em>)  starting indices for the slice operation, supports negative indices.</p></li>
<li><p><strong>end</strong> (<em>tuple of &lt;&gt;</em><em>, </em><em>required</em>)  ending indices for the slice operation, supports negative indices.</p></li>
<li><p><strong>step</strong> (<em>tuple of &lt;&gt;</em><em>, </em><em>optional</em><em>, </em><em>default=</em><em>[</em><em>]</em>)  step for the slice operation, supports negative values.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sqrt">
<code class="sig-name descname">sqrt</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sqrt" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise square-root value of the input.</p>
<div class="math notranslate nohighlight">
\[\textrm{sqrt}(x) = \sqrt{x}\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sqrt</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">sqrt</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>sqrt(default) = default</p></li>
<li><p>sqrt(row_sparse) = row_sparse</p></li>
<li><p>sqrt(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_pow.cc:L170</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.square">
<code class="sig-name descname">square</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.square" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise squared value of the input.</p>
<div class="math notranslate nohighlight">
\[square(x) = x^2\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">square</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">square</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>square(default) = default</p></li>
<li><p>square(row_sparse) = row_sparse</p></li>
<li><p>square(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_pow.cc:L119</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.stop_gradient">
<code class="sig-name descname">stop_gradient</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.stop_gradient" title="Permalink to this definition"></a></dt>
<dd><p>Stops gradient computation.</p>
<p>Stops the accumulated gradient of the inputs from flowing through this operator
in the backward direction. In other words, this operator prevents the contribution
of its inputs to be taken into account for computing gradients.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">v1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">v2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">b_stop_grad</span> <span class="o">=</span> <span class="n">stop_gradient</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">MakeLoss</span><span class="p">(</span><span class="n">b_stop_grad</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span>

<span class="n">executor</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">simple_bind</span><span class="p">(</span><span class="n">ctx</span><span class="o">=</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">a</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">b</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">executor</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">v1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">v2</span><span class="p">)</span>
<span class="n">executor</span><span class="o">.</span><span class="n">outputs</span>
<span class="p">[</span> <span class="mf">1.</span>  <span class="mf">5.</span><span class="p">]</span>

<span class="n">executor</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">executor</span><span class="o">.</span><span class="n">grad_arrays</span>
<span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L387</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.sum">
<code class="sig-name descname">sum</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">axis=_Null</em>, <em class="sig-param">keepdims=_Null</em>, <em class="sig-param">exclude=_Null</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.sum" title="Permalink to this definition"></a></dt>
<dd><p>Computes the sum of array elements over given axes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>sum</cite> and <cite>sum_axis</cite> are equivalent.
For ndarray of csr storage type summation along axis 0 and axis 1 is supported.
Setting keepdims or exclude to True will cause a fallback to dense operator.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span>
        <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
        <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]]</span>

<span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">[[</span>  <span class="mf">4.</span>   <span class="mf">8.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">10.</span>   <span class="mf">9.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">21.</span>   <span class="mf">6.</span><span class="p">]]</span>

<span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="p">[</span> <span class="mf">12.</span>  <span class="mf">19.</span>  <span class="mf">27.</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>

<span class="n">csr</span> <span class="o">=</span> <span class="n">cast_storage</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">)</span>

<span class="nb">sum</span><span class="p">(</span><span class="n">csr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="p">[</span> <span class="mf">8.</span>  <span class="mf">3.</span>  <span class="mf">1.</span><span class="p">]</span>

<span class="nb">sum</span><span class="p">(</span><span class="n">csr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">[</span> <span class="mf">3.</span>  <span class="mf">4.</span>  <span class="mf">5.</span><span class="p">]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/broadcast_reduce_sum_value.cc:L67</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input</p></li>
<li><p><strong>axis</strong> (<em>Shape</em><em> or </em><em>None</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>)  <p>The axis or axes along which to perform the reduction.</p>
<blockquote>
<div><p>The default, <cite>axis=()</cite>, will compute over all elements into a
scalar array with shape <cite>(1,)</cite>.</p>
<p>If <cite>axis</cite> is int, a reduction is performed on a particular axis.</p>
<p>If <cite>axis</cite> is a tuple of ints, a reduction is performed on all the axes
specified in the tuple.</p>
<p>If <cite>exclude</cite> is true, reduction will be performed on the axes that are
NOT in axis instead.</p>
<p>Negative values means indexing from right to left.</p>
</div></blockquote>
</p></li>
<li><p><strong>keepdims</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  If this is set to <cite>True</cite>, the reduced axes are left in the result as dimension with size one.</p></li>
<li><p><strong>exclude</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=0</em>)  Whether to perform reduction on axis that are NOT in axis instead.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.tan">
<code class="sig-name descname">tan</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.tan" title="Permalink to this definition"></a></dt>
<dd><p>Computes the element-wise tangent of the input array.</p>
<p>The input should be in radians (<span class="math notranslate nohighlight">\(2\pi\)</span> rad equals 360 degrees).</p>
<div class="math notranslate nohighlight">
\[tan([0, \pi/4, \pi/2]) = [0, 1, -inf]\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">tan</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>tan(default) = default</p></li>
<li><p>tan(row_sparse) = row_sparse</p></li>
<li><p>tan(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L140</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.tanh">
<code class="sig-name descname">tanh</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.tanh" title="Permalink to this definition"></a></dt>
<dd><p>Returns the hyperbolic tangent of the input array, computed element-wise.</p>
<div class="math notranslate nohighlight">
\[tanh(x) = sinh(x) / cosh(x)\]</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">tanh</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>tanh(default) = default</p></li>
<li><p>tanh(row_sparse) = row_sparse</p></li>
<li><p>tanh(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_trig.cc:L451</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.trunc">
<code class="sig-name descname">trunc</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.trunc" title="Permalink to this definition"></a></dt>
<dd><p>Return the element-wise truncated value of the input.</p>
<p>The truncated value of the scalar x is the nearest integer i which is closer to
zero than x is. In short, the fractional part of the signed number x is discarded.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trunc</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">]</span>
</pre></div>
</div>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">trunc</span></code> output depends upon the input storage type:</p>
<blockquote>
<div><ul class="simple">
<li><p>trunc(default) = default</p></li>
<li><p>trunc(row_sparse) = row_sparse</p></li>
<li><p>trunc(csr) = csr</p></li>
</ul>
</div></blockquote>
<p>Defined in /work/mxnet/src/operator/tensor/elemwise_unary_op_basic.cc:L916</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input array.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.where">
<code class="sig-name descname">where</code><span class="sig-paren">(</span><em class="sig-param">condition=None</em>, <em class="sig-param">x=None</em>, <em class="sig-param">y=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.where" title="Permalink to this definition"></a></dt>
<dd><p>Return the elements, either from x or y, depending on the condition.</p>
<p>Given three ndarrays, condition, x, and y, return an ndarray with the elements from x or y,
depending on the elements from condition are true or false. x and y must have the same shape.
If condition has the same shape as x, each element in the output array is from x if the
corresponding element in the condition is true, and from y if false.</p>
<p>If condition does not have the same shape as x, it must be a 1D array whose size is
the same as xs first dimension size. Each row of the output array is from xs row
if the corresponding element from condition is true, and from ys row if false.</p>
<p>Note that all non-zero values are interpreted as <code class="docutils literal notranslate"><span class="pre">True</span></code> in condition.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="n">cond</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>

<span class="n">where</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>

<span class="n">csr_cond</span> <span class="o">=</span> <span class="n">cast_storage</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">)</span>

<span class="n">where</span><span class="p">(</span><span class="n">csr_cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
</pre></div>
</div>
<p>Defined in /work/mxnet/src/operator/tensor/control_flow_op.cc:L57</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  condition array</p></li>
<li><p><strong>x</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  </p></li>
<li><p><strong>y</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  </p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.ndarray.sparse.zeros_like">
<code class="sig-name descname">zeros_like</code><span class="sig-paren">(</span><em class="sig-param">data=None</em>, <em class="sig-param">out=None</em>, <em class="sig-param">name=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#mxnet.ndarray.sparse.zeros_like" title="Permalink to this definition"></a></dt>
<dd><p>Return an array of zeros with the same shape, type and storage type
as the input array.</p>
<p>The storage type of <code class="docutils literal notranslate"><span class="pre">zeros_like</span></code> output depends on the storage type of the input</p>
<ul class="simple">
<li><p>zeros_like(row_sparse) = row_sparse</p></li>
<li><p>zeros_like(csr) = csr</p></li>
<li><p>zeros_like(default) = default</p></li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
     <span class="p">[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">]]</span>

<span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
                 <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The input</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em>, </em><em>optional</em>)  The output NDArray to hold the result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>out</strong>  The output of this function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray">NDArray</a> or list of NDArrays</p>
</dd>
</dl>
</dd></dl>

</div>


        <hr class="feedback-hr-top" />
<div class="feedback-container">
    <div class="feedback-question">Did this page help you?</div>
    <div class="feedback-answer-container">
        <div class="feedback-answer yes-link" data-response="yes">Yes</div>
        <div class="feedback-answer no-link" data-response="no">No</div>
    </div>
    <div class="feedback-thank-you">Thanks for your feedback!</div>
</div>
<hr class="feedback-hr-bottom" />
        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>                    

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../register/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>ndarray.register</div>
         </div>
     </a>
     <a id="button-next" href="../utils/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>ndarray.utils</div>
        </div>
     </a>
  </div>
            <footer class="site-footer h-card">
    <div class="wrapper">
        <div class="row">
            <div class="col-4">
                <h4 class="footer-category-title">Resources</h4>
                <ul class="contact-list">
                    <li><a
                            href="https://lists.apache.org/list.html?dev@mxnet.apache.org">Mailing list</a> <a class="u-email" href="mailto:dev-subscribe@mxnet.apache.org">(subscribe)</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/issues">Github Issues</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/projects">Projects</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="/community">Contribute To MXNet</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../../_static/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>

<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="../../../../_static/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright  2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>
        
  </body>
</html>