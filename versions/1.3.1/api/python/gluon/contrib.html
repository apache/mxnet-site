<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Gluon Contrib API" property="og:title">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image:secure_url">
<meta content="Gluon Contrib API" property="og:description"/>
<title>Gluon Contrib API — mxnet  documentation</title>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" rel="stylesheet"/>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../../_static/basic.css" rel="stylesheet" type="text/css">
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mxnet.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
<script src="https://code.jquery.com/jquery-1.11.1.min.js" type="text/javascript"></script>
<script src="../../../_static/underscore.js" type="text/javascript"></script>
<script src="../../../_static/searchtools_custom.js" type="text/javascript"></script>
<script src="../../../_static/doctools.js" type="text/javascript"></script>
<script src="../../../_static/selectlang.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript"> jQuery(function() { Search.loadIndex("/versions/1.3.1/searchindex.js"); Search.init();}); </script>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
      Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-96378503-1', 'auto');
      ga('send', 'pageview');

    </script>
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/jquery.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/underscore.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../../_static/doctools.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<!-- -->
<link href="../../../genindex.html" rel="index" title="Index">
<link href="../../../search.html" rel="search" title="Search"/>
<link href="gluon.html" rel="up" title="Gluon Package"/>
<link href="../io/io.html" rel="next" title="Data Loading API"/>
<link href="model_zoo.html" rel="prev" title="Gluon Model Zoo"/>
<link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"/>
</link></link></link></meta></meta></meta></head>
<body background="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-background-compressed.jpeg" role="document">
<div class="content-block"><div class="navbar navbar-fixed-top">
<div class="container" id="navContainer">
<div class="innder" id="header-inner">
<h1 id="logo-wrap">
<a href="../../../" id="logo"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo.png"/></a>
</h1>
<nav class="nav-bar" id="main-nav">
<a class="main-nav-link" href="/versions/1.3.1/install/index.html">Install</a>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Gluon <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.3.1/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="https://www.d2l.ai/">Dive into Deep Learning</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">API <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.3.1/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/scala/index.html">Scala</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-docs">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Docs <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-docs">
<li><a class="main-nav-link" href="/versions/1.3.1/faq/index.html">FAQ</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/tutorials/index.html">Tutorials</a>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.3.1/example">Examples</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/architecture/index.html">Architecture</a></li>
<li><a class="main-nav-link" href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/model_zoo/index.html">Model Zoo</a></li>
<li><a class="main-nav-link" href="https://github.com/onnx/onnx-mxnet">ONNX</a></li>
</li></ul>
</span>
<span id="dropdown-menu-position-anchor-community">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Community <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-community">
<li><a class="main-nav-link" href="http://discuss.mxnet.io">Forum</a></li>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.3.1">Github</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/community/contribute.html">Contribute</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/community/ecosystem.html">Ecosystem</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/community/powered_by.html">Powered By</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-version" style="position: relative"><a href="#" class="main-nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="true">1.3.1<span class="caret"></span></a><ul id="package-dropdown-menu" class="dropdown-menu"><li><a href="/">master</a></li><li><a href="/versions/1.7.0/">1.7.0</a></li><li><a href=/versions/1.6.0/>1.6.0</a></li><li><a href=/versions/1.5.0/>1.5.0</a></li><li><a href=/versions/1.4.1/>1.4.1</a></li><li><a href=/versions/1.3.1/>1.3.1</a></li><li><a href=/versions/1.2.1/>1.2.1</a></li><li><a href=/versions/1.1.0/>1.1.0</a></li><li><a href=/versions/1.0.0/>1.0.0</a></li><li><a href=/versions/0.12.1/>0.12.1</a></li><li><a href=/versions/0.11.0/>0.11.0</a></li></ul></span></nav>
<script> function getRootPath(){ return "../../../" } </script>
<div class="burgerIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button">☰</a>
<ul class="dropdown-menu" id="burgerMenu">
<li><a href="/versions/1.3.1/install/index.html">Install</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/tutorials/index.html">Tutorials</a></li>
<li class="dropdown-submenu dropdown">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Gluon</a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.3.1/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="http://gluon.mxnet.io">The Straight Dope (Tutorials)</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">API</a>
<ul class="dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.3.1/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.3.1/api/scala/index.html">Scala</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Docs</a>
<ul class="dropdown-menu">
<li><a href="/versions/1.3.1/faq/index.html" tabindex="-1">FAQ</a></li>
<li><a href="/versions/1.3.1/tutorials/index.html" tabindex="-1">Tutorials</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.3.1/example" tabindex="-1">Examples</a></li>
<li><a href="/versions/1.3.1/architecture/index.html" tabindex="-1">Architecture</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home" tabindex="-1">Developer Wiki</a></li>
<li><a href="/versions/1.3.1/model_zoo/index.html" tabindex="-1">Gluon Model Zoo</a></li>
<li><a href="https://github.com/onnx/onnx-mxnet" tabindex="-1">ONNX</a></li>
</ul>
</li>
<li class="dropdown-submenu dropdown">
<a aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" role="button" tabindex="-1">Community</a>
<ul class="dropdown-menu">
<li><a href="http://discuss.mxnet.io" tabindex="-1">Forum</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.3.1" tabindex="-1">Github</a></li>
<li><a href="/versions/1.3.1/community/contribute.html" tabindex="-1">Contribute</a></li>
<li><a href="/versions/1.3.1/community/ecosystem.html" tabindex="-1">Ecosystem</a></li>
<li><a href="/versions/1.3.1/community/powered_by.html" tabindex="-1">Powered By</a></li>
</ul>
</li>
<li id="dropdown-menu-position-anchor-version-mobile" class="dropdown-submenu" style="position: relative"><a href="#" tabindex="-1">1.3.1</a><ul class="dropdown-menu"><li><a tabindex="-1" href=/>master</a></li><li><a tabindex="-1" href=/versions/1.6.0/>1.6.0</a></li><li><a tabindex="-1" href=/versions/1.5.0/>1.5.0</a></li><li><a tabindex="-1" href=/versions/1.4.1/>1.4.1</a></li><li><a tabindex="-1" href=/versions/1.3.1/>1.3.1</a></li><li><a tabindex="-1" href=/versions/1.2.1/>1.2.1</a></li><li><a tabindex="-1" href=/versions/1.1.0/>1.1.0</a></li><li><a tabindex="-1" href=/versions/1.0.0/>1.0.0</a></li><li><a tabindex="-1" href=/versions/0.12.1/>0.12.1</a></li><li><a tabindex="-1" href=/versions/0.11.0/>0.11.0</a></li></ul></li></ul>
</div>
<div class="plusIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button"><span aria-hidden="true" class="glyphicon glyphicon-plus"></span></a>
<ul class="dropdown-menu dropdown-menu-right" id="plusMenu"></ul>
</div>
<div id="search-input-wrap">
<form action="../../../search.html" autocomplete="off" class="" method="get" role="search">
<div class="form-group inner-addon left-addon">
<i class="glyphicon glyphicon-search"></i>
<input class="form-control" name="q" placeholder="Search" type="text"/>
</div>
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default"/>
</input></form>
<div id="search-preview"></div>
</div>
<div id="searchIcon">
<span aria-hidden="true" class="glyphicon glyphicon-search"></span>
</div>
<!-- <div id="lang-select-wrap"> -->
<!--   <label id="lang-select-label"> -->
<!--     <\!-- <i class="fa fa-globe"></i> -\-> -->
<!--     <span></span> -->
<!--   </label> -->
<!--   <select id="lang-select"> -->
<!--     <option value="en">Eng</option> -->
<!--     <option value="zh">中文</option> -->
<!--   </select> -->
<!-- </div> -->
<!--     <a id="mobile-nav-toggle">
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
      </a> -->
</div>
</div>
</div>
<script type="text/javascript">
        $('body').css('background', 'white');
    </script>
<div class="container">
<div class="row">
<div aria-label="main navigation" class="sphinxsidebar leftsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Python Documents</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#ndarray-api">NDArray API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#symbol-api">Symbol API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#module-api">Module API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#autograd-api">Autograd API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#gluon-api">Gluon API</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="gluon.html">Gluon Package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="gluon.html#overview">Overview</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="gluon.html#contents">Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="gluon.html#parameter">Parameter</a></li>
<li class="toctree-l4"><a class="reference internal" href="gluon.html#containers">Containers</a></li>
<li class="toctree-l4"><a class="reference internal" href="gluon.html#trainer">Trainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="gluon.html#utilities">Utilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="gluon.html#api-reference">API Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nn.html">Gluon Neural Network Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="rnn.html">Gluon Recurrent Neural Network API</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html">Gluon Loss API</a></li>
<li class="toctree-l3"><a class="reference internal" href="data.html">Gluon Data API</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_zoo.html">Gluon Model Zoo</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Gluon Contrib API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#contrib">Contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="#api-reference">API Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#io-api">IO API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#image-api">Image API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#optimization-api">Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#callback-api">Callback API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#metric-api">Metric API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#profiler-api">Profiler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#run-time-compilation-api">Run-Time Compilation API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#contrib-package">Contrib Package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../r/index.html">R Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../julia/index.html">Julia Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../c++/index.html">C++ Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../scala/index.html">Scala Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perl/index.html">Perl Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/index.html">HowTo Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/index.html">System Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribute.html">Community</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="page-tracker"></div>
<div class="section" id="gluon-contrib-api">
<span id="gluon-contrib-api"></span><h1>Gluon Contrib API<a class="headerlink" href="#gluon-contrib-api" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<span id="overview"></span><h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This document lists the contrib APIs in Gluon:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#module-mxnet.gluon.contrib" title="mxnet.gluon.contrib"><code class="xref py py-obj docutils literal"><span class="pre">mxnet.gluon.contrib</span></code></a></td>
<td>Contrib neural network module.</td>
</tr>
</tbody>
</table>
<p>The <code class="docutils literal"><span class="pre">Gluon</span> <span class="pre">Contrib</span></code> API, defined in the <code class="docutils literal"><span class="pre">gluon.contrib</span></code> package, provides
many useful experimental APIs for new features.
This is a place for the community to try out the new features,
so that feature contributors can receive feedback.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This package contains experimental APIs and may change in the near future.</p>
</div>
<p>In the rest of this document, we list routines provided by the <code class="docutils literal"><span class="pre">gluon.contrib</span></code> package.</p>
</div>
<div class="section" id="contrib">
<span id="contrib"></span><h2>Contrib<a class="headerlink" href="#contrib" title="Permalink to this headline">¶</a></h2>
<div class="section" id="neural-network">
<span id="neural-network"></span><h3>Neural network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.Concurrent" title="mxnet.gluon.contrib.nn.Concurrent"><code class="xref py py-obj docutils literal"><span class="pre">Concurrent</span></code></a></td>
<td>Lays <a href="#id1"><span class="problematic" id="id2">`</span></a>Block`s concurrently.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.HybridConcurrent" title="mxnet.gluon.contrib.nn.HybridConcurrent"><code class="xref py py-obj docutils literal"><span class="pre">HybridConcurrent</span></code></a></td>
<td>Lays <a href="#id3"><span class="problematic" id="id4">`</span></a>HybridBlock`s concurrently.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.Identity" title="mxnet.gluon.contrib.nn.Identity"><code class="xref py py-obj docutils literal"><span class="pre">Identity</span></code></a></td>
<td>Block that passes through the input directly.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.SparseEmbedding" title="mxnet.gluon.contrib.nn.SparseEmbedding"><code class="xref py py-obj docutils literal"><span class="pre">SparseEmbedding</span></code></a></td>
<td>Turns non-negative integers (indexes/tokens) into dense vectors of fixed size.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.nn.SyncBatchNorm" title="mxnet.gluon.contrib.nn.SyncBatchNorm"><code class="xref py py-obj docutils literal"><span class="pre">SyncBatchNorm</span></code></a></td>
<td>Cross-GPU Synchronized Batch normalization (SyncBN)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="recurrent-neural-network">
<span id="recurrent-neural-network"></span><h3>Recurrent neural network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell" title="mxnet.gluon.contrib.rnn.VariationalDropoutCell"><code class="xref py py-obj docutils literal"><span class="pre">VariationalDropoutCell</span></code></a></td>
<td>Applies Variational Dropout on base cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DRNNCell" title="mxnet.gluon.contrib.rnn.Conv1DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DRNNCell</span></code></a></td>
<td>1D Convolutional RNN cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DRNNCell" title="mxnet.gluon.contrib.rnn.Conv2DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DRNNCell</span></code></a></td>
<td>2D Convolutional RNN cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DRNNCell" title="mxnet.gluon.contrib.rnn.Conv3DRNNCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DRNNCell</span></code></a></td>
<td>3D Convolutional RNN cells</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv1DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DLSTMCell</span></code></a></td>
<td>1D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv2DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DLSTMCell</span></code></a></td>
<td>2D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DLSTMCell" title="mxnet.gluon.contrib.rnn.Conv3DLSTMCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DLSTMCell</span></code></a></td>
<td>3D Convolutional LSTM network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv1DGRUCell" title="mxnet.gluon.contrib.rnn.Conv1DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv1DGRUCell</span></code></a></td>
<td>1D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv2DGRUCell" title="mxnet.gluon.contrib.rnn.Conv2DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv2DGRUCell</span></code></a></td>
<td>2D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.Conv3DGRUCell" title="mxnet.gluon.contrib.rnn.Conv3DGRUCell"><code class="xref py py-obj docutils literal"><span class="pre">Conv3DGRUCell</span></code></a></td>
<td>3D Convolutional Gated Rectified Unit (GRU) network cell.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.rnn.LSTMPCell" title="mxnet.gluon.contrib.rnn.LSTMPCell"><code class="xref py py-obj docutils literal"><span class="pre">LSTMPCell</span></code></a></td>
<td>Long-Short Term Memory Projected (LSTMP) network cell.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="data">
<span id="data"></span><h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.IntervalSampler" title="mxnet.gluon.contrib.data.IntervalSampler"><code class="xref py py-obj docutils literal"><span class="pre">IntervalSampler</span></code></a></td>
<td>Samples elements from [0, length) at fixed intervals.</td>
</tr>
</tbody>
</table>
<div class="section" id="text-dataset">
<span id="text-dataset"></span><h4>Text dataset<a class="headerlink" href="#text-dataset" title="Permalink to this headline">¶</a></h4>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"/>
<col width="90%"/>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.text.WikiText2" title="mxnet.gluon.contrib.data.text.WikiText2"><code class="xref py py-obj docutils literal"><span class="pre">WikiText2</span></code></a></td>
<td>WikiText-2 word-level dataset for language modeling, from Salesforce research.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mxnet.gluon.contrib.data.text.WikiText103" title="mxnet.gluon.contrib.data.text.WikiText103"><code class="xref py py-obj docutils literal"><span class="pre">WikiText103</span></code></a></td>
<td>WikiText-103 word-level dataset for language modeling, from Salesforce research.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="api-reference">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#api-reference" title="Permalink to this headline">¶</a></h2>
<script src="../../../_static/js/auto_module_index.js" type="text/javascript"></script><span class="target" id="module-mxnet.gluon.contrib"></span><p>Contrib neural network module.</p>
<span class="target" id="module-mxnet.gluon.contrib.nn"></span><p>Contrib recurrent neural network module.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.Concurrent">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">Concurrent</code><span class="sig-paren">(</span><em>axis=-1</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#Concurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.Concurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Lays <a href="#id5"><span class="problematic" id="id6">`</span></a>Block`s concurrently.</p>
<p>This block feeds its input to all children blocks, and
produce the output by concatenating all the children blocks’ outputs
on the specified axis.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Concurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give children blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>axis</strong> (<em>int</em><em>, </em><em>default -1</em>) – The axis on which to concatenate the outputs.</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.HybridConcurrent">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">HybridConcurrent</code><span class="sig-paren">(</span><em>axis=-1</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#HybridConcurrent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.HybridConcurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Lays <a href="#id7"><span class="problematic" id="id8">`</span></a>HybridBlock`s concurrently.</p>
<p>This block feeds its input to all children blocks, and
produce the output by concatenating all the children blocks’ outputs
on the specified axis.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">HybridConcurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give children blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>axis</strong> (<em>int</em><em>, </em><em>default -1</em>) – The axis on which to concatenate the outputs.</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.Identity">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">Identity</code><span class="sig-paren">(</span><em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#Identity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>Block that passes through the input directly.</p>
<p>This block can be used in conjunction with HybridConcurrent
block for residual connection.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">HybridConcurrent</span><span class="p">()</span>
<span class="c1"># use net's name_scope to give child Blocks appropriate names.</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Identity</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.SparseEmbedding">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">SparseEmbedding</code><span class="sig-paren">(</span><em>input_dim</em>, <em>output_dim</em>, <em>dtype='float32'</em>, <em>weight_initializer=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#SparseEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.SparseEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Turns non-negative integers (indexes/tokens) into dense vectors
of fixed size. eg. [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]</p>
<p>This SparseBlock is designed for distributed training with extremely large
input dimension. Both weight and gradient w.r.t. weight are <cite>RowSparseNDArray</cite>.</p>
<p>Note: if <cite>sparse_grad</cite> is set to True, the gradient w.r.t weight will be
sparse. Only a subset of optimizers support sparse gradients, including SGD, AdaGrad
and Adam. By default lazy updates is turned on, which may perform differently
from standard updates. For more details, please check the Optimization API at:
<a class="reference external" href="/api/python/optimization/optimization.html">/api/python/optimization/optimization.html</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_dim</strong> (<em>int</em>) – Size of the vocabulary, i.e. maximum integer index + 1.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Dimension of the dense embedding.</li>
<li><strong>dtype</strong> (<em>str</em><em> or </em><em>np.dtype</em><em>, </em><em>default 'float32'</em>) – Data type of output embeddings.</li>
<li><strong>weight_initializer</strong> (<a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the <cite>embeddings</cite> matrix.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: (N-1)-D tensor with shape: <cite>(x1, x2, ..., xN-1)</cite>.</li>
</ul>
</li>
<li><strong>Output</strong> – <ul>
<li><strong>out</strong>: N-D tensor with shape: <cite>(x1, x2, ..., xN-1, output_dim)</cite>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.nn.SyncBatchNorm">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.nn.</code><code class="descname">SyncBatchNorm</code><span class="sig-paren">(</span><em>in_channels=0</em>, <em>num_devices=None</em>, <em>momentum=0.9</em>, <em>epsilon=1e-05</em>, <em>center=True</em>, <em>scale=True</em>, <em>use_global_stats=False</em>, <em>beta_initializer='zeros'</em>, <em>gamma_initializer='ones'</em>, <em>running_mean_initializer='zeros'</em>, <em>running_variance_initializer='ones'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/nn/basic_layers.html#SyncBatchNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.nn.SyncBatchNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-GPU Synchronized Batch normalization (SyncBN)</p>
<p>Standard BN <a class="footnote-reference" href="#id11" id="id9">[1]</a> implementation only normalize the data within each device.
SyncBN normalizes the input within the whole mini-batch.
We follow the sync-onece implmentation described in the paper <a class="footnote-reference" href="#id12" id="id10">[2]</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<em>int</em><em>, </em><em>default 0</em>) – Number of channels (feature maps) in input data. If not specified,
initialization will be deferred to the first time <cite>forward</cite> is called
and <cite>in_channels</cite> will be inferred from the shape of input data.</li>
<li><strong>num_devices</strong> (<em>int</em><em>, </em><em>default number of visible GPUs</em>) – </li>
<li><strong>momentum</strong> (<em>float</em><em>, </em><em>default 0.9</em>) – Momentum for the moving average.</li>
<li><strong>epsilon</strong> (<em>float</em><em>, </em><em>default 1e-5</em>) – Small float added to variance to avoid dividing by zero.</li>
<li><strong>center</strong> (<em>bool</em><em>, </em><em>default True</em>) – If True, add offset of <cite>beta</cite> to normalized tensor.
If False, <cite>beta</cite> is ignored.</li>
<li><strong>scale</strong> (<em>bool</em><em>, </em><em>default True</em>) – If True, multiply by <cite>gamma</cite>. If False, <cite>gamma</cite> is not used.
When the next layer is linear (also e.g. <cite>nn.relu</cite>),
this can be disabled since the scaling
will be done by the next layer.</li>
<li><strong>use_global_stats</strong> (<em>bool</em><em>, </em><em>default False</em>) – If True, use global moving statistics instead of local batch-norm. This will force
change batch-norm into a scale shift operator.
If False, use local batch-norm.</li>
<li><strong>beta_initializer</strong> (str or <cite>Initializer</cite>, default ‘zeros’) – Initializer for the beta weight.</li>
<li><strong>gamma_initializer</strong> (str or <cite>Initializer</cite>, default ‘ones’) – Initializer for the gamma weight.</li>
<li><strong>moving_mean_initializer</strong> (str or <cite>Initializer</cite>, default ‘zeros’) – Initializer for the moving mean.</li>
<li><strong>moving_variance_initializer</strong> (str or <cite>Initializer</cite>, default ‘ones’) – Initializer for the moving variance.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><strong>data</strong>: input tensor with arbitrary shape.</li>
</ul>
</dd>
<dt>Outputs:</dt>
<dd><ul class="first last simple">
<li><strong>out</strong>: output tensor with the same shape as <cite>data</cite>.</li>
</ul>
</dd>
<dt>Reference:</dt>
<dd><table class="first docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id9">[1]</a></td><td>Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating           deep network training by reducing internal covariate shift.” <em>ICML 2015</em></td></tr>
</tbody>
</table>
<table class="last docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id10">[2]</a></td><td>Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang,           Ambrish Tyagi, and Amit Agrawal. “Context Encoding for Semantic Segmentation.” <em>CVPR 2018</em></td></tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.rnn"></span><p>Contrib recurrent neural network module.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional RNN cell.</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id15"><span class="problematic" id="id16">conv_rnn_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional RNN cell.</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id17"><span class="problematic" id="id18">conv_rnn_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DRNNCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DRNNCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DRNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional RNN cells</p>
<div class="math">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id19"><span class="problematic" id="id20">conv_rnn_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id21"><span class="problematic" id="id22">conv_lstm_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id23"><span class="problematic" id="id24">conv_lstm_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DLSTMCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DLSTMCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">“Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting”</a> paper. Xingjian et al. NIPS2015</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in c^prime_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id25"><span class="problematic" id="id26">conv_lstm_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv1DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv1DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>)</em>, <em>i2h_dilate=(1</em>, <em>)</em>, <em>h2h_dilate=(1</em>, <em>)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv1DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv1DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>1D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCW’ the shape should be (C, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCW’ and ‘NWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id27"><span class="problematic" id="id28">conv_gru_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv2DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv2DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv2DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv2DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>2D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCHW’ the shape should be (C, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCHW’ and ‘NHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id29"><span class="problematic" id="id30">conv_gru_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.Conv3DGRUCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">Conv3DGRUCell</code><span class="sig-paren">(</span><em>input_shape</em>, <em>hidden_channels</em>, <em>i2h_kernel</em>, <em>h2h_kernel</em>, <em>i2h_pad=(0</em>, <em>0</em>, <em>0)</em>, <em>i2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>h2h_dilate=(1</em>, <em>1</em>, <em>1)</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>conv_layout='NCDHW'</em>, <em>activation='tanh'</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/conv_rnn_cell.html#Conv3DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.Conv3DGRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>3D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_shape</strong> (<em>tuple of int</em>) – Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout ‘NCDHW’ the shape should be (C, D, H, W).</li>
<li><strong>hidden_channels</strong> (<em>int</em>) – Number of output channels.</li>
<li><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Input convolution kernel sizes.</li>
<li><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>) – Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</li>
<li><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>) – Pad for input convolution.</li>
<li><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Input convolution dilate.</li>
<li><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>) – Recurrent convolution dilate.</li>
<li><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the input weights matrix, used for the input convolutions.</li>
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the input convolutions.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the input convolution bias vectors.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>) – Initializer for the recurrent convolution bias vectors.</li>
<li><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>) – Layout for all convolution inputs, outputs and weights. Options are ‘NCDHW’ and ‘NDHWC’.</li>
<li><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="gluon.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>Block</em></a><em>, </em><em>default 'tanh'</em>) – Type of activation function used in n_t.
If argument type is string, it’s equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id31"><span class="problematic" id="id32">conv_gru_</span></a>‘) – Prefix for name of layers (and name of weight if params is None).</li>
<li><strong>params</strong> (<a class="reference internal" href="../symbol/rnn.html#mxnet.rnn.RNNParams" title="mxnet.rnn.RNNParams"><em>RNNParams</em></a><em>, </em><em>default None</em>) – Container for weight sharing between cells. Created if None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.VariationalDropoutCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">VariationalDropoutCell</code><span class="sig-paren">(</span><em>base_cell</em>, <em>drop_inputs=0.0</em>, <em>drop_states=0.0</em>, <em>drop_outputs=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#VariationalDropoutCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Variational Dropout on base cell.
(<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a>,</p>
<blockquote>
<div><a class="reference external" href="https://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf">https://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf</a>).</div></blockquote>
<p>Variational dropout uses the same dropout mask across time-steps. It can be applied to RNN
inputs, outputs, and states. The masks for them are not shared.</p>
<p>The dropout mask is initialized when stepping forward for the first time and will remain
the same until .reset() is called. Thus, if using the cell and stepping manually without calling
.unroll(), the .reset() should be called after each sequence.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_cell</strong> (<a class="reference internal" href="rnn.html#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>) – The cell on which to perform variational dropout.</li>
<li><strong>drop_inputs</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for inputs. Won’t apply dropout if it equals 0.</li>
<li><strong>drop_states</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for state inputs on the first state channel.
Won’t apply dropout if it equals 0.</li>
<li><strong>drop_outputs</strong> (<em>float</em><em>, </em><em>default 0.</em>) – The dropout rate for outputs. Won’t apply dropout if it equals 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.contrib.rnn.VariationalDropoutCell.unroll">
<code class="descname">unroll</code><span class="sig-paren">(</span><em>length</em>, <em>inputs</em>, <em>begin_state=None</em>, <em>layout='NTC'</em>, <em>merge_outputs=None</em>, <em>valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#VariationalDropoutCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.VariationalDropoutCell.unroll" title="Permalink to this definition">¶</a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>length</strong> (<em>int</em>) – Number of steps to unroll.</li>
<li><strong>inputs</strong> (<a class="reference internal" href="../symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>) – <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ...) if <cite>layout</cite> is ‘NTC’,
or (length, batch_size, ...) if <cite>layout</cite> is ‘TNC’.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ...).</p>
</li>
<li><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>) – Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</li>
<li><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>) – <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</li>
<li><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ...) if layout is ‘NTC’,
or (length, batch_size, ...) if layout is ‘TNC’.
If <cite>None</cite>, output whatever is faster.</li>
<li><strong>valid_length</strong> (<a class="reference internal" href="../symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>) – <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>outputs</strong> (<em>list of Symbol or Symbol</em>) – Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</li>
<li><strong>states</strong> (<em>list of Symbol</em>) – The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.rnn.LSTMPCell">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.rnn.</code><code class="descname">LSTMPCell</code><span class="sig-paren">(</span><em>hidden_size</em>, <em>projection_size</em>, <em>i2h_weight_initializer=None</em>, <em>h2h_weight_initializer=None</em>, <em>h2r_weight_initializer=None</em>, <em>i2h_bias_initializer='zeros'</em>, <em>h2h_bias_initializer='zeros'</em>, <em>input_size=0</em>, <em>prefix=None</em>, <em>params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/rnn/rnn_cell.html#LSTMPCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.rnn.LSTMPCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Long-Short Term Memory Projected (LSTMP) network cell.
(<a class="reference external" href="https://arxiv.org/abs/1402.1128">https://arxiv.org/abs/1402.1128</a>)
Each call computes the following function:
.. math:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>\<span class="n">begin</span><span class="p">{</span><span class="n">array</span><span class="p">}{</span><span class="n">ll</span><span class="p">}</span>
<span class="n">i_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_</span><span class="p">{</span><span class="n">ii</span><span class="p">}</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">ii</span><span class="p">}</span> <span class="o">+</span> <span class="n">W_</span><span class="p">{</span><span class="n">ri</span><span class="p">}</span> <span class="n">r_</span><span class="p">{(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">ri</span><span class="p">})</span> \\
<span class="n">f_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_</span><span class="p">{</span><span class="k">if</span><span class="p">}</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="k">if</span><span class="p">}</span> <span class="o">+</span> <span class="n">W_</span><span class="p">{</span><span class="n">rf</span><span class="p">}</span> <span class="n">r_</span><span class="p">{(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">rf</span><span class="p">})</span> \\
<span class="n">g_t</span> <span class="o">=</span> \<span class="n">tanh</span><span class="p">(</span><span class="n">W_</span><span class="p">{</span><span class="n">ig</span><span class="p">}</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">ig</span><span class="p">}</span> <span class="o">+</span> <span class="n">W_</span><span class="p">{</span><span class="n">rc</span><span class="p">}</span> <span class="n">r_</span><span class="p">{(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">rg</span><span class="p">}})</span> \\
<span class="n">o_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_</span><span class="p">{</span><span class="n">io</span><span class="p">}</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">io</span><span class="p">}</span> <span class="o">+</span> <span class="n">W_</span><span class="p">{</span><span class="n">ro</span><span class="p">}</span> <span class="n">r_</span><span class="p">{(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span> <span class="o">+</span> <span class="n">b_</span><span class="p">{</span><span class="n">ro</span><span class="p">})</span> \\
<span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_</span><span class="p">{(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">g_t</span> \\
<span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> \<span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span> \\
<span class="n">r_t</span> <span class="o">=</span> <span class="n">W_</span><span class="p">{</span><span class="n">hr</span><span class="p">}</span> <span class="n">h_t</span>
\<span class="n">end</span><span class="p">{</span><span class="n">array</span><span class="p">}</span>
</pre></div>
</div>
<p>where <span class="math">\(r_t\)</span> is the projected recurrent activation at time <cite>t</cite>,
math:<cite>h_t</cite> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the
cell state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input at time <cite>t</cite>, and <span class="math">\(i_t\)</span>,
<span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>, <span class="math">\(o_t\)</span> are the input, forget, cell, and
out gates, respectively.
:param hidden_size: Number of units in cell state symbol.
:type hidden_size: int
:param projection_size: Number of units in output symbol.
:type projection_size: int
:param i2h_weight_initializer: Initializer for the input weights matrix, used for the linear</p>
<blockquote>
<div>transformation of the inputs.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the recurrent weights matrix, used for the linear
transformation of the hidden state.</li>
<li><strong>h2r_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the projection weights matrix, used for the linear
transformation of the recurrent state.</li>
<li><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'lstmbias'</em>) – Initializer for the bias vector. By default, bias for the forget
gate is initialized to 1 while all other biases are initialized
to zero.</li>
<li><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../optimization/optimization.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>) – Initializer for the bias vector.</li>
<li><strong>prefix</strong> (str, default ‘<a href="#id33"><span class="problematic" id="id34">lstmp_</span></a>‘) – Prefix for name of <cite>Block`s
(and name of weight if params is `None</cite>).</li>
<li><strong>params</strong> (<a class="reference internal" href="gluon.html#mxnet.gluon.Parameter" title="mxnet.gluon.Parameter"><em>Parameter</em></a><em> or </em><em>None</em>) – Container for weight sharing between cells.
Created if <cite>None</cite>.</li>
<li><strong>Inputs</strong> – <ul>
<li><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</li>
<li><strong>states</strong>: a list of two initial recurrent state tensors, with shape
<cite>(batch_size, projection_size)</cite> and <cite>(batch_size, hidden_size)</cite> respectively.</li>
</ul>
</li>
<li><strong>Outputs</strong> – <ul>
<li><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</li>
<li><strong>next_states</strong>: a list of two output recurrent state tensors. Each has
the same shape as <cite>states</cite>.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.data"></span><p>Contrib datasets.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.IntervalSampler">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.</code><code class="descname">IntervalSampler</code><span class="sig-paren">(</span><em>length</em>, <em>interval</em>, <em>rollover=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/sampler.html#IntervalSampler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.IntervalSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements from [0, length) at fixed intervals.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>length</strong> (<em>int</em>) – Length of the sequence.</li>
<li><strong>interval</strong> (<em>int</em>) – The number of items to skip between two samples.</li>
<li><strong>rollover</strong> (<em>bool</em><em>, </em><em>default True</em>) – Whether to start again from the first skipped item after reaching the end.
If true, this sampler would start again from the first skipped item until all items
are visited.
Otherwise, iteration stops when end is reached and skipped items are ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">>>> </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">contrib</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IntervalSampler</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">>>> </span><span class="nb">list</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
<span class="go">[0, 3, 6, 9, 12, 1, 4, 7, 10, 2, 5, 8, 11]</span>
<span class="gp">>>> </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">contrib</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">IntervalSampler</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rollover</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">>>> </span><span class="nb">list</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span>
<span class="go">[0, 3, 6, 9, 12]</span>
</pre></div>
</div>
</dd></dl>
<span class="target" id="module-mxnet.gluon.contrib.data.text"></span><p>Text datasets.</p>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.text.WikiText2">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.text.</code><code class="descname">WikiText2</code><span class="sig-paren">(</span><em>root='/work/mxnet/datasets/wikitext-2'</em>, <em>segment='train'</em>, <em>vocab=None</em>, <em>seq_len=35</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/text.html#WikiText2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.text.WikiText2" title="Permalink to this definition">¶</a></dt>
<dd><p>WikiText-2 word-level dataset for language modeling, from Salesforce research.</p>
<p>From
<a class="reference external" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset</a></p>
<p>License: Creative Commons Attribution-ShareAlike</p>
<p>Each sample is a vector of length equal to the specified sequence length.
At the end of each sentence, an end-of-sentence token ‘<eos>’ is added.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>str</em><em>, </em><em>default $MXNET_HOME/datasets/wikitext-2</em>) – Path to temp folder for storing data.</li>
<li><strong>segment</strong> (<em>str</em><em>, </em><em>default 'train'</em>) – Dataset segment. Options are ‘train’, ‘validation’, ‘test’.</li>
<li><strong>vocab</strong> (<a class="reference internal" href="../contrib/text.html#mxnet.contrib.text.vocab.Vocabulary" title="mxnet.contrib.text.vocab.Vocabulary"><code class="xref py py-class docutils literal"><span class="pre">Vocabulary</span></code></a>, default None) – The vocabulary to use for indexing the text dataset.
If None, a default vocabulary is created.</li>
<li><strong>seq_len</strong> (<em>int</em><em>, </em><em>default 35</em>) – The sequence length of each sample, regardless of the sentence boundary.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="class">
<dt id="mxnet.gluon.contrib.data.text.WikiText103">
<em class="property">class </em><code class="descclassname">mxnet.gluon.contrib.data.text.</code><code class="descname">WikiText103</code><span class="sig-paren">(</span><em>root='/work/mxnet/datasets/wikitext-103'</em>, <em>segment='train'</em>, <em>vocab=None</em>, <em>seq_len=35</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/contrib/data/text.html#WikiText103"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.contrib.data.text.WikiText103" title="Permalink to this definition">¶</a></dt>
<dd><p>WikiText-103 word-level dataset for language modeling, from Salesforce research.</p>
<p>From
<a class="reference external" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset</a></p>
<p>License: Creative Commons Attribution-ShareAlike</p>
<p>Each sample is a vector of length equal to the specified sequence length.
At the end of each sentence, an end-of-sentence token ‘<eos>’ is added.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"/>
<col class="field-body"/>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root</strong> (<em>str</em><em>, </em><em>default $MXNET_HOME/datasets/wikitext-103</em>) – Path to temp folder for storing data.</li>
<li><strong>segment</strong> (<em>str</em><em>, </em><em>default 'train'</em>) – Dataset segment. Options are ‘train’, ‘validation’, ‘test’.</li>
<li><strong>vocab</strong> (<a class="reference internal" href="../contrib/text.html#mxnet.contrib.text.vocab.Vocabulary" title="mxnet.contrib.text.vocab.Vocabulary"><code class="xref py py-class docutils literal"><span class="pre">Vocabulary</span></code></a>, default None) – The vocabulary to use for indexing the text dataset.
If None, a default vocabulary is created.</li>
<li><strong>seq_len</strong> (<em>int</em><em>, </em><em>default 35</em>) – The sequence length of each sample, regardless of the sentence boundary.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<script>auto_index("api-reference");</script></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar rightsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h3><a href="../../../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">Gluon Contrib API</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#contrib">Contrib</a><ul>
<li><a class="reference internal" href="#neural-network">Neural network</a></li>
<li><a class="reference internal" href="#recurrent-neural-network">Recurrent neural network</a></li>
<li><a class="reference internal" href="#data">Data</a><ul>
<li><a class="reference internal" href="#text-dataset">Text dataset</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api-reference">API Reference</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div><div class="footer">
<div class="section-disclaimer">
<div class="container">
<div>
<img height="60" src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/apache_incubator_logo.png"/>
<p>
            Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <strong>sponsored by the <i>Apache Incubator</i></strong>. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
        </p>
<p>
            "Copyright © 2017-2018, The Apache Software Foundation
            Apache MXNet, MXNet, Apache, the Apache feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the Apache Software Foundation."
        </p>
</div>
</div>
</div>
</div> <!-- pagename != index -->
</div>
<script crossorigin="anonymous" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script src="../../../_static/js/sidebar.js" type="text/javascript"></script>
<script src="../../../_static/js/search.js" type="text/javascript"></script>
<script src="../../../_static/js/navbar.js" type="text/javascript"></script>
<script src="../../../_static/js/clipboard.min.js" type="text/javascript"></script>
<script src="../../../_static/js/copycode.js" type="text/javascript"></script>
<script src="../../../_static/js/page.js" type="text/javascript"></script>
<script src="../../../_static/js/docversion.js" type="text/javascript"></script>
<script type="text/javascript">
        $('body').ready(function () {
            $('body').css('visibility', 'visible');
        });
    </script>
</body>
</html>