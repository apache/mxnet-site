<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Gotchas using NumPy in Apache MXNet" property="og:title">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image:secure_url">
<meta content="Gotchas using NumPy in Apache MXNet" property="og:description"/>
<title>Gotchas using NumPy in Apache MXNet — mxnet  documentation</title>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" rel="stylesheet"/>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../_static/basic.css" rel="stylesheet" type="text/css">
<link href="../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../_static/mxnet.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
<script src="https://code.jquery.com/jquery-1.11.1.min.js" type="text/javascript"></script>
<script src="../../_static/underscore.js" type="text/javascript"></script>
<script src="../../_static/searchtools_custom.js" type="text/javascript"></script>
<script src="../../_static/doctools.js" type="text/javascript"></script>
<script src="../../_static/selectlang.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript"> jQuery(function() { Search.loadIndex("/versions/1.4.1/searchindex.js"); Search.init();}); </script>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
      Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-96378503-1', 'auto');
      ga('send', 'pageview');

    </script>
<!-- -->
<!-- <script type="text/javascript" src="../../_static/jquery.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/underscore.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/doctools.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<!-- -->
<link href="../../genindex.html" rel="index" title="Index">
<link href="../../search.html" rel="search" title="Search"/>
<link href="index.html" rel="up" title="Tutorials"/>
<link href="hybrid.html" rel="next" title="Hybrid - Faster training and easy deployment"/>
<link href="gluon.html" rel="prev" title="Gluon - Neural network building blocks"/>
<link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"/>
</link></link></link></meta></meta></meta></head>
<body background="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-background-compressed.jpeg" role="document">
<div class="content-block"><div class="navbar navbar-fixed-top">
<div class="container" id="navContainer">
<div class="innder" id="header-inner">
<h1 id="logo-wrap">
<a href="../../" id="logo"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo.png"/></a>
</h1>
<nav class="nav-bar" id="main-nav">
<a class="main-nav-link" href="/versions/1.4.1/install/index.html">Install</a>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Gluon <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.4.1/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="https://www.d2l.ai/">Dive into Deep Learning</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">API <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.4.1/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/scala/index.html">Scala</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-docs">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Docs <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-docs">
<li><a class="main-nav-link" href="/versions/1.4.1/faq/index.html">FAQ</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/tutorials/index.html">Tutorials</a>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.4.1/example">Examples</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/architecture/index.html">Architecture</a></li>
<li><a class="main-nav-link" href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/model_zoo/index.html">Model Zoo</a></li>
<li><a class="main-nav-link" href="https://github.com/onnx/onnx-mxnet">ONNX</a></li>
</li></ul>
</span>
<span id="dropdown-menu-position-anchor-community">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Community <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-community">
<li><a class="main-nav-link" href="http://discuss.mxnet.io">Forum</a></li>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.4.1">Github</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/community/contribute.html">Contribute</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/community/ecosystem.html">Ecosystem</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/community/powered_by.html">Powered By</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-version" style="position: relative"><a href="#" class="main-nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="true">1.4.1<span class="caret"></span></a><ul id="package-dropdown-menu" class="dropdown-menu"><li><a href="/">master</a></li><li><a href="/versions/1.7.0/">1.7.0</a></li><li><a href=/versions/1.6.0/>1.6.0</a></li><li><a href=/versions/1.5.0/>1.5.0</a></li><li><a href=/versions/1.4.1/>1.4.1</a></li><li><a href=/versions/1.3.1/>1.3.1</a></li><li><a href=/versions/1.2.1/>1.2.1</a></li><li><a href=/versions/1.1.0/>1.1.0</a></li><li><a href=/versions/1.0.0/>1.0.0</a></li><li><a href=/versions/0.12.1/>0.12.1</a></li><li><a href=/versions/0.11.0/>0.11.0</a></li></ul></span></nav>
<script> function getRootPath(){ return "../../" } </script>
<div class="burgerIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button">☰</a>
<ul class="dropdown-menu" id="burgerMenu">
<li><a href="/versions/1.4.1/install/index.html">Install</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/tutorials/index.html">Tutorials</a></li>
<li class="dropdown-submenu dropdown">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Gluon</a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.4.1/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="http://gluon.mxnet.io">The Straight Dope (Tutorials)</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">API</a>
<ul class="dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.4.1/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.4.1/api/scala/index.html">Scala</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Docs</a>
<ul class="dropdown-menu">
<li><a href="/versions/1.4.1/faq/index.html" tabindex="-1">FAQ</a></li>
<li><a href="/versions/1.4.1/tutorials/index.html" tabindex="-1">Tutorials</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.4.1/example" tabindex="-1">Examples</a></li>
<li><a href="/versions/1.4.1/architecture/index.html" tabindex="-1">Architecture</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home" tabindex="-1">Developer Wiki</a></li>
<li><a href="/versions/1.4.1/model_zoo/index.html" tabindex="-1">Gluon Model Zoo</a></li>
<li><a href="https://github.com/onnx/onnx-mxnet" tabindex="-1">ONNX</a></li>
</ul>
</li>
<li class="dropdown-submenu dropdown">
<a aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" role="button" tabindex="-1">Community</a>
<ul class="dropdown-menu">
<li><a href="http://discuss.mxnet.io" tabindex="-1">Forum</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.4.1" tabindex="-1">Github</a></li>
<li><a href="/versions/1.4.1/community/contribute.html" tabindex="-1">Contribute</a></li>
<li><a href="/versions/1.4.1/community/ecosystem.html" tabindex="-1">Ecosystem</a></li>
<li><a href="/versions/1.4.1/community/powered_by.html" tabindex="-1">Powered By</a></li>
</ul>
</li>
<li id="dropdown-menu-position-anchor-version-mobile" class="dropdown-submenu" style="position: relative"><a href="#" tabindex="-1">1.4.1</a><ul class="dropdown-menu"><li><a tabindex="-1" href=/>master</a></li><li><a tabindex="-1" href=/versions/1.6.0/>1.6.0</a></li><li><a tabindex="-1" href=/versions/1.5.0/>1.5.0</a></li><li><a tabindex="-1" href=/versions/1.4.1/>1.4.1</a></li><li><a tabindex="-1" href=/versions/1.3.1/>1.3.1</a></li><li><a tabindex="-1" href=/versions/1.2.1/>1.2.1</a></li><li><a tabindex="-1" href=/versions/1.1.0/>1.1.0</a></li><li><a tabindex="-1" href=/versions/1.0.0/>1.0.0</a></li><li><a tabindex="-1" href=/versions/0.12.1/>0.12.1</a></li><li><a tabindex="-1" href=/versions/0.11.0/>0.11.0</a></li></ul></li></ul>
</div>
<div class="plusIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button"><span aria-hidden="true" class="glyphicon glyphicon-plus"></span></a>
<ul class="dropdown-menu dropdown-menu-right" id="plusMenu"></ul>
</div>
<div id="search-input-wrap">
<form action="../../search.html" autocomplete="off" class="" method="get" role="search">
<div class="form-group inner-addon left-addon">
<i class="glyphicon glyphicon-search"></i>
<input class="form-control" name="q" placeholder="Search" type="text"/>
</div>
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default"/>
</input></form>
<div id="search-preview"></div>
</div>
<div id="searchIcon">
<span aria-hidden="true" class="glyphicon glyphicon-search"></span>
</div>
<!-- <div id="lang-select-wrap"> -->
<!--   <label id="lang-select-label"> -->
<!--     <\!-- <i class="fa fa-globe"></i> -\-> -->
<!--     <span></span> -->
<!--   </label> -->
<!--   <select id="lang-select"> -->
<!--     <option value="en">Eng</option> -->
<!--     <option value="zh">中文</option> -->
<!--   </select> -->
<!-- </div> -->
<!--     <a id="mobile-nav-toggle">
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
      </a> -->
</div>
</div>
</div>
<script type="text/javascript">
        $('body').css('background', 'white');
    </script>
<div class="container">
<div class="row">
<div aria-label="main navigation" class="sphinxsidebar leftsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">MXNet APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/index.html">MXNet Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">MXNet Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">MXNet FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gluon/index.html">About Gluon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing MXNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#nvidia-jetson-tx-family">Nvidia Jetson TX family</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#source-download">Source Download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo/index.html">MXNet Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Tutorials</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="page-tracker"></div>
<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at --><!---   http://www.apache.org/licenses/LICENSE-2.0 --><!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. --><div class="section" id="gotchas-using-numpy-in-apache-mxnet">
<span id="gotchas-using-numpy-in-apache-mxnet"></span><h1>Gotchas using NumPy in Apache MXNet<a class="headerlink" href="#gotchas-using-numpy-in-apache-mxnet" title="Permalink to this headline">¶</a></h1>
<p>The goal of this tutorial is to explain some common misconceptions about using <a class="reference external" href="http://www.numpy.org/">NumPy</a> arrays in Apache MXNet. We are going to explain why you need to minimize or completely remove usage of NumPy from your Apache MXNet code. We also going to show how to minimize NumPy performance impact, when you have to use NumPy.</p>
<div class="section" id="asynchronous-and-non-blocking-nature-of-apache-mxnet">
<span id="asynchronous-and-non-blocking-nature-of-apache-mxnet"></span><h2>Asynchronous and non-blocking nature of Apache MXNet<a class="headerlink" href="#asynchronous-and-non-blocking-nature-of-apache-mxnet" title="Permalink to this headline">¶</a></h2>
<p>Instead of using NumPy arrays Apache MXNet offers its own array implementation named <a class="reference external" href="/api/python/ndarray/ndarray.html">NDArray</a>. <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code> was intentionally designed to be similar to <code class="docutils literal"><span class="pre">NumPy</span></code>, but there are differences.</p>
<p>One key difference is in the way calculations are executed. Every <code class="docutils literal"><span class="pre">NDArray</span></code> manipulation in Apache MXNet is done in asynchronous, non-blocking way. That means, that when we write code like <code class="docutils literal"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code>, where both <code class="docutils literal"><span class="pre">a</span></code> and <code class="docutils literal"><span class="pre">b</span></code> are <code class="docutils literal"><span class="pre">NDArrays</span></code>, the function is pushed to the <a class="reference external" href="/architecture/overview.html#execution-engine">Execution Engine</a>, which starts the calculation. The function immediately returns back, and the  user thread can continue execution, despite the fact that the calculation may not have been completed yet.</p>
<p><code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code> builds the computation graph which may reorder or combine some calculations, but it honors dependency order: if there are other manipulation with <code class="docutils literal"><span class="pre">c</span></code> done later in the code, the <code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code> will start doing them once the result of <code class="docutils literal"><span class="pre">c</span></code> is available. We don’t need to write callbacks to start execution of subsequent code - the <code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code> is going to do it for us.</p>
<p>To get the result of the computation we only need to access the resulting variable, and the flow of the code will be blocked until the computation results are assigned to the resulting variable. This behavior allows to increase code performance while still supporting imperative programming mode.</p>
<p>Refer to the <a class="reference external" href="/versions/1.4.1/tutorials/basic/ndarray.html">intro tutorial to NDArray</a>, if you are new to Apache MXNet and would like to learn more how to manipulate NDArrays.</p>
</div>
<div class="section" id="converting-ndarray-to-numpy-array-blocks-calculation">
<span id="converting-ndarray-to-numpy-array-blocks-calculation"></span><h2>Converting NDArray to NumPy Array blocks calculation<a class="headerlink" href="#converting-ndarray-to-numpy-array-blocks-calculation" title="Permalink to this headline">¶</a></h2>
<p>Many people are familiar with NumPy and flexible doing tensor manipulations using it. <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code> offers  a convinient <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.asnumpy">.asnumpy() method</a> to cast <code class="docutils literal"><span class="pre">nd.array</span></code> to <code class="docutils literal"><span class="pre">np.array</span></code>. However, by doing this cast and using <code class="docutils literal"><span class="pre">np.array</span></code> for calculation, we cannot use all the goodness of <code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code>. All manipulations done on <code class="docutils literal"><span class="pre">np.array</span></code> are blocking. Moreover, the cast to <code class="docutils literal"><span class="pre">np.array</span></code> itself is a blocking operation (same as <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.asscalar">.asscalar()</a>, <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.wait_to_read">.wait_to_read()</a> and <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.waitall">.waitall()</a>).</p>
<p>That means that if we have a long computation graph and, at some point, we want to cast the result to <code class="docutils literal"><span class="pre">np.array</span></code>, it may feel like the casting takes a lot of time. But what really takes this time is <code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code>, which finishes all the async calculations we have pushed into it to get the final result, which then will be converted to <code class="docutils literal"><span class="pre">np.array</span></code>.</p>
<p>Because of the blocking nature of <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray.asnumpy">.asnumpy() method</a>, using it reduces the execution performance, especially if the calculations are done on GPU: Apache MXNet has to copy data from GPU to CPU to return <code class="docutils literal"><span class="pre">np.array</span></code>.</p>
<p>The best solution is to <strong>make manipulations directly on NDArrays by methods provided in <a class="reference external" href="/api/python/ndarray/ndarray.html">NDArray API</a></strong>.</p>
</div>
<div class="section" id="numpy-operators-vs-ndarray-operators">
<span id="numpy-operators-vs-ndarray-operators"></span><h2>NumPy operators vs. NDArray operators<a class="headerlink" href="#numpy-operators-vs-ndarray-operators" title="Permalink to this headline">¶</a></h2>
<p>Despite the fact that <a class="reference external" href="/api/python/ndarray/ndarray.html">NDArray API</a> was specifically designed to be similar to <code class="docutils literal"><span class="pre">NumPy</span></code>, sometimes it is not easy to replace existing <code class="docutils literal"><span class="pre">NumPy</span></code> computations. The main reason is that not all operators, that are available in <code class="docutils literal"><span class="pre">NumPy</span></code>, are available in <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code>. The list of currently available operators is available on <a class="reference external" href="/versions/1.4.1/api/python/ndarray/ndarray.html#the-ndarray-class">NDArray class page</a>.</p>
<p>If a required operator is missing from <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code>, there are few things you can do.</p>
<div class="section" id="combine-a-higher-level-operator-using-a-few-lower-level-operators">
<span id="combine-a-higher-level-operator-using-a-few-lower-level-operators"></span><h3>Combine a higher level operator using a few lower level operators<a class="headerlink" href="#combine-a-higher-level-operator-using-a-few-lower-level-operators" title="Permalink to this headline">¶</a></h3>
<p>There are a situation, when you can assemble a higher level operator using existing operators. An example for that is the <a class="reference external" href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.full_like.html">np.full_like()</a> operator. This operator doesn’t exist in <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code>, but can be easily replaced with a combination of existing operators.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># NumPy has full_like() operator </span>
<span class="n">np_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># NDArray doesn't have it, but we can replace it with</span>
<span class="c1"># creating an array of ones and then multiplying by fill_value</span>
<span class="n">nd_y</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">10</span>

<span class="c1"># To compare results we had to convert NDArray to NumPy</span>
<span class="c1"># But this is okay for that particular case</span>
<span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">np_y</span><span class="p">,</span> <span class="n">nd_y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span>True <!--notebook-skip-line-->
</pre></div>
</div>
</div>
<div class="section" id="find-similar-operator-with-different-name-and-or-signature">
<span id="find-similar-operator-with-different-name-and-or-signature"></span><h3>Find similar operator with different name and/or signature<a class="headerlink" href="#find-similar-operator-with-different-name-and-or-signature" title="Permalink to this headline">¶</a></h3>
<p>Some operators may have slightly different name, but are similar in terms of functionality. For example <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.ravel_multi_index">nd.ravel_multi_index()</a> is similar to <a class="reference external" href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ma.ravel.html#numpy.ma.ravel">np.ravel()</a>. In other cases some operators may have similar names, but different signatures. For example <a class="reference external" href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.split.html#numpy.split">np.split()</a> and <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.split">nd.split()</a> are similar, but the former works with indices and the latter requires the number of splits to be provided.</p>
<p>One particular example of different input requirements is <a class="reference external" href="/api/python/ndarray/ndarray.html#mxnet.ndarray.pad">nd.pad()</a>. The trick is that it can only work with 4-dimensional tensors. If your input has less dimensions, then you need to expand its number before using <code class="docutils literal"><span class="pre">nd.pad()</span></code> as it is shown in the code block below:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pad_array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
    <span class="c1"># expand dimensions to 4, because nd.pad can work only with 4 dims</span>
    <span class="n">data_expanded</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># pad all 4 dimensions with constant value of 0</span>
    <span class="n">data_padded</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">data_expanded</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s1">'constant'</span><span class="p">,</span>
                             <span class="n">pad_width</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                             <span class="n">constant_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># remove temporary dimensions </span>
    <span class="n">data_reshaped_back</span> <span class="o">=</span> <span class="n">data_padded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_reshaped_back</span>

<span class="n">pad_array</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span>[ 1.  2.  3.  0.  0.  0.  0.  0.  0.  0.] <!--notebook-skip-line-->

<NDArray 10 @cpu(0)> <!--notebook-skip-line-->
</pre></div>
</div>
</div>
<div class="section" id="search-for-an-operator-on-github">
<span id="search-for-an-operator-on-github"></span><h3>Search for an operator on <a class="reference external" href="https://github.com/apache/incubator-mxnet/labels/Operator">Github</a><a class="headerlink" href="#search-for-an-operator-on-github" title="Permalink to this headline">¶</a></h3>
<p>Apache MXNet community is responsive to requests, and everyone is welcomed to contribute new operators. Have in mind, that there is always a lag between new operators being merged into the codebase and release of a next stable version. For example, <a class="reference external" href="https://github.com/apache/incubator-mxnet/pull/11643">nd.diag()</a> operator was recently introduced to Apache MXNet, but on the moment of writing this tutorial, it is not in any stable release. You can always get all latest implementations by installing the <a class="reference external" href="/install/index.html?version=master#">master version</a> of Apache MXNet.</p>
</div>
</div>
<div class="section" id="how-to-minimize-the-impact-of-blocking-calls">
<span id="how-to-minimize-the-impact-of-blocking-calls"></span><h2>How to minimize the impact of blocking calls<a class="headerlink" href="#how-to-minimize-the-impact-of-blocking-calls" title="Permalink to this headline">¶</a></h2>
<p>There are cases, when you have to use either <code class="docutils literal"><span class="pre">.asnumpy()</span></code> or <code class="docutils literal"><span class="pre">.asscalar()</span></code> methods. As it is explained before, this will force Apache MXNet to block the execution until the result can be retrieved. One common use case is printing a metric or a value of a loss function.</p>
<p>You can minimize the impact of a blocking call by calling <code class="docutils literal"><span class="pre">.asnumpy()</span></code> or <code class="docutils literal"><span class="pre">.asscalar()</span></code> in the moment, when you think the calculation of this value is already done. In the example below, we introduce the <code class="docutils literal"><span class="pre">LossBuffer</span></code> class. It is used to cache the previous value of a loss function. By doing so, we delay printing by one iteration in hope that the <code class="docutils literal"><span class="pre">Execution</span> <span class="pre">Engine</span></code> would finish the previous iteration and blocking time would be minimized.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">nd</span><span class="p">,</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">mxnet.ndarray</span> <span class="kn">import</span> <span class="n">NDArray</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">HybridBlock</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">LossBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Simple buffer for storing loss value</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">new_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ce</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">SoftmaxCELoss</span><span class="p">()</span>
<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,)),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'int32'</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ArrayDataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'sgd'</span><span class="p">)</span>
<span class="n">loss_buffer</span> <span class="o">=</span> <span class="n">LossBuffer</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># This call saves new loss and returns previous loss</span>
        <span class="n">prev_loss</span> <span class="o">=</span> <span class="n">loss_buffer</span><span class="o">.</span><span class="n">new_loss</span><span class="p">(</span><span class="n">ce</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
        
    <span class="n">loss_buffer</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">prev_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"Loss: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">prev_loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span>Loss: 2.310760974884033 <!--notebook-skip-line-->

Loss: 2.334498643875122 <!--notebook-skip-line-->

Loss: 2.3244147300720215 <!--notebook-skip-line-->

Loss: 2.332686424255371 <!--notebook-skip-line-->

Loss: 2.321366310119629 <!--notebook-skip-line-->

Loss: 2.3236165046691895 <!--notebook-skip-line-->

Loss: 2.3178648948669434 <!--notebook-skip-line-->
</pre></div>
</div>
</div>
<div class="section" id="conclusion">
<span id="conclusion"></span><h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>For performance reasons, it is better to use native <code class="docutils literal"><span class="pre">NDArray</span> <span class="pre">API</span></code> methods and avoid using NumPy altogether. In case when you must use NumPy, you can use convenient method <code class="docutils literal"><span class="pre">.asnumpy()</span></code> on <code class="docutils literal"><span class="pre">NDArray</span></code> to get NumPy representation. By doing so, you block the whole computational process, and force data to be synced between CPU and GPU. If it is a necessary evil to do that, try to minimize the blocking time by calling <code class="docutils literal"><span class="pre">.asnumpy()</span></code> in time, when you expect the value to be already computed.</p>
<div class="btn-group" role="group">
<div class="download-btn"><a download="gotchas_numpy_in_mxnet.ipynb" href="gotchas_numpy_in_mxnet.ipynb"><span class="glyphicon glyphicon-download-alt"></span> gotchas_numpy_in_mxnet.ipynb</a></div></div></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar rightsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h3><a href="../../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">Gotchas using NumPy in Apache MXNet</a><ul>
<li><a class="reference internal" href="#asynchronous-and-non-blocking-nature-of-apache-mxnet">Asynchronous and non-blocking nature of Apache MXNet</a></li>
<li><a class="reference internal" href="#converting-ndarray-to-numpy-array-blocks-calculation">Converting NDArray to NumPy Array blocks calculation</a></li>
<li><a class="reference internal" href="#numpy-operators-vs-ndarray-operators">NumPy operators vs. NDArray operators</a><ul>
<li><a class="reference internal" href="#combine-a-higher-level-operator-using-a-few-lower-level-operators">Combine a higher level operator using a few lower level operators</a></li>
<li><a class="reference internal" href="#find-similar-operator-with-different-name-and-or-signature">Find similar operator with different name and/or signature</a></li>
<li><a class="reference internal" href="#search-for-an-operator-on-github">Search for an operator on Github</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-minimize-the-impact-of-blocking-calls">How to minimize the impact of blocking calls</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div><div class="footer">
<div class="section-disclaimer">
<div class="container">
<div>
<img height="60" src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/apache_incubator_logo.png"/>
<p>
            Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <strong>sponsored by the <i>Apache Incubator</i></strong>. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
        </p>
<p>
            "Copyright © 2017-2018, The Apache Software Foundation
            Apache MXNet, MXNet, Apache, the Apache feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the Apache Software Foundation."
        </p>
</div>
</div>
</div>
</div> <!-- pagename != index -->
</div>
<script crossorigin="anonymous" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script src="../../_static/js/sidebar.js" type="text/javascript"></script>
<script src="../../_static/js/search.js" type="text/javascript"></script>
<script src="../../_static/js/navbar.js" type="text/javascript"></script>
<script src="../../_static/js/clipboard.min.js" type="text/javascript"></script>
<script src="../../_static/js/copycode.js" type="text/javascript"></script>
<script src="../../_static/js/page.js" type="text/javascript"></script>
<script src="../../_static/js/docversion.js" type="text/javascript"></script>
<script type="text/javascript">
        $('body').ready(function () {
            $('body').css('visibility', 'visible');
        });
    </script>
</body>
</html>