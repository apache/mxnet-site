<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="How to write a custom layer in Apache MxNet Gluon API" property="og:title">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image">
<meta content="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/og-logo.png" property="og:image:secure_url">
<meta content="How to write a custom layer in Apache MxNet Gluon API" property="og:description"/>
<title>How to write a custom layer in Apache MxNet Gluon API — mxnet  documentation</title>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" rel="stylesheet"/>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../_static/basic.css" rel="stylesheet" type="text/css">
<link href="../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../_static/mxnet.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
<script src="https://code.jquery.com/jquery-1.11.1.min.js" type="text/javascript"></script>
<script src="../../_static/underscore.js" type="text/javascript"></script>
<script src="../../_static/searchtools_custom.js" type="text/javascript"></script>
<script src="../../_static/doctools.js" type="text/javascript"></script>
<script src="../../_static/selectlang.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/javascript"> jQuery(function() { Search.loadIndex("/versions/1.5.0/searchindex.js"); Search.init();}); </script>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new
      Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-96378503-1', 'auto');
      ga('send', 'pageview');

    </script>
<!-- -->
<!-- <script type="text/javascript" src="../../_static/jquery.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/underscore.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="../../_static/doctools.js"></script> -->
<!-- -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<!-- -->
<link href="../../genindex.html" rel="index" title="Index">
<link href="../../search.html" rel="search" title="Search"/>
<link href="index.html" rel="up" title="Tutorials"/>
<link href="customop.html" rel="next" title="Creating custom operators with numpy"/>
<link href="autograd.html" rel="prev" title="Automatic differentiation"/>
<link href="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-icon.png" rel="icon" type="image/png"/>
</link></link></link></meta></meta></meta></head>
<body background="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet-background-compressed.jpeg" role="document">
<div class="content-block"><div class="navbar navbar-fixed-top">
<div class="container" id="navContainer">
<div class="innder" id="header-inner">
<h1 id="logo-wrap">
<a href="../../" id="logo"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo.png"/></a>
</h1>
<nav class="nav-bar" id="main-nav">
<a class="main-nav-link" href="/versions/1.5.0/install/index.html">Install</a>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Gluon <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="https://www.d2l.ai/">Dive into Deep Learning</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">API <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/scala/index.html">Scala</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-docs">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Docs <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-docs">
<li><a class="main-nav-link" href="/versions/1.5.0/faq/index.html">FAQ</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/index.html">Tutorials</a>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.5.0/example">Examples</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/architecture/index.html">Architecture</a></li>
<li><a class="main-nav-link" href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/model_zoo/index.html">Model Zoo</a></li>
<li><a class="main-nav-link" href="https://github.com/onnx/onnx-mxnet">ONNX</a></li>
</li></ul>
</span>
<span id="dropdown-menu-position-anchor-community">
<a aria-expanded="true" aria-haspopup="true" class="main-nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button">Community <span class="caret"></span></a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu-community">
<li><a class="main-nav-link" href="http://discuss.mxnet.io">Forum</a></li>
<li><a class="main-nav-link" href="https://github.com/apache/incubator-mxnet/tree/1.5.0">Github</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/contribute.html">Contribute</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/ecosystem.html">Ecosystem</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/community/powered_by.html">Powered By</a></li>
</ul>
</span>
<span id="dropdown-menu-position-anchor-version" style="position: relative"><a href="#" class="main-nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="true">1.5.0<span class="caret"></span></a><ul id="package-dropdown-menu" class="dropdown-menu"><li><a href="/">master</a></li><li><a href="/versions/1.7.0/">1.7.0</a></li><li><a href=/versions/1.6.0/>1.6.0</a></li><li><a href=/versions/1.5.0/>1.5.0</a></li><li><a href=/versions/1.4.1/>1.4.1</a></li><li><a href=/versions/1.3.1/>1.3.1</a></li><li><a href=/versions/1.2.1/>1.2.1</a></li><li><a href=/versions/1.1.0/>1.1.0</a></li><li><a href=/versions/1.0.0/>1.0.0</a></li><li><a href=/versions/0.12.1/>0.12.1</a></li><li><a href=/versions/0.11.0/>0.11.0</a></li></ul></span></nav>
<script> function getRootPath(){ return "../../" } </script>
<div class="burgerIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button">☰</a>
<ul class="dropdown-menu" id="burgerMenu">
<li><a href="/versions/1.5.0/install/index.html">Install</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/index.html">Tutorials</a></li>
<li class="dropdown-submenu dropdown">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Gluon</a>
<ul class="dropdown-menu navbar-menu" id="package-dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/tutorials/gluon/gluon.html">About</a></li>
<li><a class="main-nav-link" href="http://gluon.mxnet.io">The Straight Dope (Tutorials)</a></li>
<li><a class="main-nav-link" href="https://gluon-cv.mxnet.io">GluonCV Toolkit</a></li>
<li><a class="main-nav-link" href="https://gluon-nlp.mxnet.io/">GluonNLP Toolkit</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">API</a>
<ul class="dropdown-menu">
<li><a class="main-nav-link" href="/versions/1.5.0/api/python/index.html">Python</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/c++/index.html">C++</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/clojure/index.html">Clojure</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/java/index.html">Java</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/julia/index.html">Julia</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/perl/index.html">Perl</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/r/index.html">R</a></li>
<li><a class="main-nav-link" href="/versions/1.5.0/api/scala/index.html">Scala</a></li>
</ul>
</li>
<li class="dropdown-submenu">
<a aria-expanded="true" aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" tabindex="-1">Docs</a>
<ul class="dropdown-menu">
<li><a href="/versions/1.5.0/faq/index.html" tabindex="-1">FAQ</a></li>
<li><a href="/versions/1.5.0/tutorials/index.html" tabindex="-1">Tutorials</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.5.0/example" tabindex="-1">Examples</a></li>
<li><a href="/versions/1.5.0/architecture/index.html" tabindex="-1">Architecture</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home" tabindex="-1">Developer Wiki</a></li>
<li><a href="/versions/1.5.0/model_zoo/index.html" tabindex="-1">Gluon Model Zoo</a></li>
<li><a href="https://github.com/onnx/onnx-mxnet" tabindex="-1">ONNX</a></li>
</ul>
</li>
<li class="dropdown-submenu dropdown">
<a aria-haspopup="true" class="dropdown-toggle burger-link" data-toggle="dropdown" href="#" role="button" tabindex="-1">Community</a>
<ul class="dropdown-menu">
<li><a href="http://discuss.mxnet.io" tabindex="-1">Forum</a></li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/1.5.0" tabindex="-1">Github</a></li>
<li><a href="/versions/1.5.0/community/contribute.html" tabindex="-1">Contribute</a></li>
<li><a href="/versions/1.5.0/community/ecosystem.html" tabindex="-1">Ecosystem</a></li>
<li><a href="/versions/1.5.0/community/powered_by.html" tabindex="-1">Powered By</a></li>
</ul>
</li>
<li id="dropdown-menu-position-anchor-version-mobile" class="dropdown-submenu" style="position: relative"><a href="#" tabindex="-1">1.5.0</a><ul class="dropdown-menu"><li><a tabindex="-1" href=/>master</a></li><li><a tabindex="-1" href=/versions/1.6.0/>1.6.0</a></li><li><a tabindex="-1" href=/versions/1.5.0/>1.5.0</a></li><li><a tabindex="-1" href=/versions/1.4.1/>1.4.1</a></li><li><a tabindex="-1" href=/versions/1.3.1/>1.3.1</a></li><li><a tabindex="-1" href=/versions/1.2.1/>1.2.1</a></li><li><a tabindex="-1" href=/versions/1.1.0/>1.1.0</a></li><li><a tabindex="-1" href=/versions/1.0.0/>1.0.0</a></li><li><a tabindex="-1" href=/versions/0.12.1/>0.12.1</a></li><li><a tabindex="-1" href=/versions/0.11.0/>0.11.0</a></li></ul></li></ul>
</div>
<div class="plusIcon dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#" role="button"><span aria-hidden="true" class="glyphicon glyphicon-plus"></span></a>
<ul class="dropdown-menu dropdown-menu-right" id="plusMenu"></ul>
</div>
<div id="search-input-wrap">
<form action="../../search.html" autocomplete="off" class="" method="get" role="search">
<div class="form-group inner-addon left-addon">
<i class="glyphicon glyphicon-search"></i>
<input class="form-control" name="q" placeholder="Search" type="text"/>
</div>
<input name="check_keywords" type="hidden" value="yes">
<input name="area" type="hidden" value="default"/>
</input></form>
<div id="search-preview"></div>
</div>
<div id="searchIcon">
<span aria-hidden="true" class="glyphicon glyphicon-search"></span>
</div>
<!-- <div id="lang-select-wrap"> -->
<!--   <label id="lang-select-label"> -->
<!--     <\!-- <i class="fa fa-globe"></i> -\-> -->
<!--     <span></span> -->
<!--   </label> -->
<!--   <select id="lang-select"> -->
<!--     <option value="en">Eng</option> -->
<!--     <option value="zh">中文</option> -->
<!--   </select> -->
<!-- </div> -->
<!--     <a id="mobile-nav-toggle">
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
        <span class="mobile-nav-toggle-bar"></span>
      </a> -->
</div>
</div>
</div>
<script type="text/javascript">
        $('body').css('background', 'white');
    </script>
<div class="container">
<div class="row">
<div aria-label="main navigation" class="sphinxsidebar leftsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">MXNet APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/index.html">MXNet Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">MXNet Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">MXNet FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gluon/index.html">About Gluon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing MXNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#nvidia-jetson-tx-family">Nvidia Jetson TX family</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html#source-download">Source Download</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo/index.html">MXNet Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Tutorials</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="page-tracker"></div>
<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at --><!---   http://www.apache.org/licenses/LICENSE-2.0 --><!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. --><div class="section" id="how-to-write-a-custom-layer-in-apache-mxnet-gluon-api">
<span id="how-to-write-a-custom-layer-in-apache-mxnet-gluon-api"></span><h1>How to write a custom layer in Apache MxNet Gluon API<a class="headerlink" href="#how-to-write-a-custom-layer-in-apache-mxnet-gluon-api" title="Permalink to this headline">¶</a></h1>
<p>While Gluon API for Apache MxNet comes with <a class="reference external" href="/api/python/gluon/nn.html">a decent number of pre-defined layers</a>, at some point one may find that a new layer is needed. Adding a new layer in Gluon API is straightforward, yet there are a few things that one needs to keep in mind.</p>
<p>In this article, I will cover how to create a new layer from scratch, how to use it, what are possible pitfalls and how to avoid them.</p>
<div class="section" id="the-simplest-custom-layer">
<span id="the-simplest-custom-layer"></span><h2>The simplest custom layer<a class="headerlink" href="#the-simplest-custom-layer" title="Permalink to this headline">¶</a></h2>
<p>To create a new layer in Gluon API, one must create a class that inherits from <a class="reference external" href="https://github.com/apache/incubator-mxnet/blob/1.5.0/python/mxnet/gluon/block.py#L123">Block</a> class. This class provides the most basic functionality, and all pre-defined layers inherit from it directly or via other subclasses. Because each layer in Apache MxNet inherits from <code class="docutils literal"><span class="pre">Block</span></code>, words “layer” and “block” are used interchangeable inside of the Apache MxNet community.</p>
<p>The only instance method needed to be implemented is <a class="reference external" href="https://github.com/apache/incubator-mxnet/blob/1.5.0/python/mxnet/gluon/block.py#L415">forward(self, x)</a>, which defines what exactly your layer is going to do during forward propagation. Notice, that it doesn’t require to provide what the block should do during back propogation. Back propogation pass for blocks is done by Apache MxNet for you.</p>
<p>In the example below, we define a new layer and implement <code class="docutils literal"><span class="pre">forward()</span></code> method to normalize input data by fitting it into a range of [0, 1].</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Do some initial imports used throughout this tutorial </span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span><span class="p">,</span> <span class="n">gluon</span><span class="p">,</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon.nn</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="n">mx</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                      <span class="c1"># Set seed for reproducable results</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NormalizationLayer</span><span class="p">(</span><span class="n">gluon</span><span class="o">.</span><span class="n">Block</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizationLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">nd</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">nd</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>The rest of methods of the <code class="docutils literal"><span class="pre">Block</span></code> class are already implemented, and majority of them are used to work with parameters of a block. There is one very special method named <a class="reference external" href="https://github.com/apache/incubator-mxnet/blob/1.5.0/python/mxnet/gluon/block.py#L384">hybridize()</a>, though, which I am going to cover before moving to a more complex example of a custom layer.</p>
</div>
<div class="section" id="hybridization-and-the-difference-between-block-and-hybridblock">
<span id="hybridization-and-the-difference-between-block-and-hybridblock"></span><h2>Hybridization and the difference between Block and HybridBlock<a class="headerlink" href="#hybridization-and-the-difference-between-block-and-hybridblock" title="Permalink to this headline">¶</a></h2>
<p>Looking into implementation of <a class="reference external" href="/api/python/gluon/nn.html">existing layers</a>, one may find that more often a block inherits from a <a class="reference external" href="https://github.com/apache/incubator-mxnet/blob/1.5.0/python/mxnet/gluon/block.py#L428">HybridBlock</a>, instead of directly inheriting from <code class="docutils literal"><span class="pre">Block</span></code>.</p>
<p>The reason for that is that <code class="docutils literal"><span class="pre">HybridBlock</span></code> allows to write custom layers that can be used in imperative programming as well as in symbolic programming. It is convinient to support both ways, because the imperative programming eases the debugging of the code and the symbolic one provides faster execution speed. You can learn more about the difference between symbolic vs. imperative programming from <a class="reference external" href="/architecture/program_model.html">this article</a>.</p>
<p>Hybridization is a process that Apache MxNet uses to create a symbolic graph of a forward computation. This allows to increase computation performance by optimizing the computational symbolic graph. Once the symbolic graph is created, Apache MxNet caches and reuses it for subsequent computations.</p>
<p>To simplify support of both imperative and symbolic programming, Apache MxNet introduce the <code class="docutils literal"><span class="pre">HybridBlock</span></code> class. Compare to the <code class="docutils literal"><span class="pre">Block</span></code> class, <code class="docutils literal"><span class="pre">HybridBlock</span></code> already has its <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.HybridBlock.forward">forward()</a> method implemented, but it defines a <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.HybridBlock.hybrid_forward">hybrid_forward()</a> method that needs to be implemented.</p>
<p>The main difference between <code class="docutils literal"><span class="pre">forward()</span></code> and <code class="docutils literal"><span class="pre">hybrid_forward()</span></code> is an <code class="docutils literal"><span class="pre">F</span></code> argument. This argument sometimes is refered as a <code class="docutils literal"><span class="pre">backend</span></code> in the Apache MxNet community. Depending on if hybridization has been done or not, <code class="docutils literal"><span class="pre">F</span></code> can refer either to <a class="reference external" href="/api/python/ndarray/ndarray.html">mxnet.ndarray API</a> or <a class="reference external" href="/api/python/symbol/symbol.html">mxnet.symbol API</a>. The former is used for imperative programming, and the latter for symbolic programming.</p>
<p>To support hybridization, it is important to use only methods avaible directly from <code class="docutils literal"><span class="pre">F</span></code> parameter. Usually, there are equivalent methods in both APIs, but sometimes there are mismatches or small variations. For example, by default, subtraction and division of NDArrays support broadcasting, while in Symbol API broadcasting is supported in a separate operators.</p>
<p>Knowing this, we can can rewrite our example layer, using HybridBlock:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NormalizationHybridLayer</span><span class="p">(</span><span class="n">gluon</span><span class="o">.</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizationHybridLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_div</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">broadcast_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">broadcast_sub</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
<p>Thanks to inheriting from HybridBlock, one can easily do forward pass on a given ndarray, either on CPU or GPU:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">NormalizationHybridLayer</span><span class="p">()</span>
<span class="n">layer</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">ctx</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">0.</span>  <span class="mf">0.5</span> <span class="mf">1.</span> <span class="p">]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">3</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>
</pre></div>
</div>
<p>As a rule of thumb, one should always implement custom layers by inheriting from <code class="docutils literal"><span class="pre">HybridBlock</span></code>. This allows to have more flexibility, and doesn’t affect execution speed once hybridization is done.</p>
<p>Unfortunately, at the moment of writing this tutorial, NLP related layers such as <a class="reference external" href="/api/python/gluon/rnn.html#mxnet.gluon.rnn.RNN">RNN</a>, <a class="reference external" href="/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRU">GRU</a> and <a class="reference external" href="/api/python/gluon/rnn.html#mxnet.gluon.rnn.LSTM">LSTM</a> are directly inhereting from the <code class="docutils literal"><span class="pre">Block</span></code> class via common <code class="docutils literal"><span class="pre">_RNNLayer</span></code> class. That means that networks with such layers cannot be hybridized. But this might change in the future, so stay tuned.</p>
<p>It is important to notice that hybridization has nothing to do with computation on GPU. One can train both hybridized and non-hybridized networks on both CPU and GPU, though hybridized networks would work faster. Though, it is hard to say in advance how much faster it is going to be.</p>
</div>
<div class="section" id="adding-a-custom-layer-to-a-network">
<span id="adding-a-custom-layer-to-a-network"></span><h2>Adding a custom layer to a network<a class="headerlink" href="#adding-a-custom-layer-to-a-network" title="Permalink to this headline">¶</a></h2>
<p>While it is possible, custom layers are rarely used separately. Most often they are used with predefined layers to create a neural network. Output of one layer is used as an input of another layer.</p>
<p>Depending on which class you used as a base one, you can use either <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.nn.Sequential">Sequential</a> or <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.nn.HybridSequential">HybridSequential</a> container to form a sequential neural network. By adding layers one by one, one adds dependencies of one layer’s input from another layer’s output. It is worth noting, that both <code class="docutils literal"><span class="pre">Sequential</span></code> and <code class="docutils literal"><span class="pre">HybridSequential</span></code> containers inherit from <code class="docutils literal"><span class="pre">Block</span></code> and <code class="docutils literal"><span class="pre">HybridBlock</span></code> respectively.</p>
<p>Below is an example of how to create a simple neural network with a custom layer. In this example, <code class="docutils literal"><span class="pre">NormalizationHybridLayer</span></code> gets as an input the output from <code class="docutils literal"><span class="pre">Dense(5)</span></code> layer and pass its output as an input to <code class="docutils literal"><span class="pre">Dense(1)</span></code> layer.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">HybridSequential</span><span class="p">()</span>                         <span class="c1"># Define a Neural Network as a sequence of hybrid blocks</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>                                    <span class="c1"># Used to disambiguate saving and loading net parameters</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>                                     <span class="c1"># Add Dense layer with 5 neurons</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">NormalizationHybridLayer</span><span class="p">())</span>                   <span class="c1"># Add our custom layer</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>                                     <span class="c1"># Add Dense layer with 1 neurons</span>


<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">magnitude</span><span class="o">=</span><span class="mf">2.24</span><span class="p">))</span>            <span class="c1"># Initialize parameters of all layers</span>
<span class="n">net</span><span class="o">.</span><span class="n">hybridize</span><span class="p">()</span>                                           <span class="c1"># Create, optimize and cache computational graph</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># Create 5 random examples with 2 feature each in range [-10, 10]</span>
<span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="o">-</span><span class="mf">0.13601446</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.26103732</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.05046433</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">1.2375476</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.15506986</span><span class="p">]]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">5</span><span class="n">x1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>
</pre></div>
</div>
</div>
<div class="section" id="parameters-of-a-custom-layer">
<span id="parameters-of-a-custom-layer"></span><h2>Parameters of a custom layer<a class="headerlink" href="#parameters-of-a-custom-layer" title="Permalink to this headline">¶</a></h2>
<p>Usually, a layer has a set of associated parameters, sometimes also referred as weights. This is an internal state of a layer. Most often, these parameters are the ones, that we want to learn during backpropogation step, but sometimes these parameters might be just constants we want to use during forward pass.</p>
<p>All parameters of a block are stored and accessed via <a class="reference external" href="https://github.com/apache/incubator-mxnet/blob/1.5.0/python/mxnet/gluon/parameter.py#L508">ParameterDict</a> class. This class helps with initialization, updating, saving and loading of the parameters. Each layer can have multiple set of parameters, and all of them can be stored in a single instance of the <code class="docutils literal"><span class="pre">ParameterDict</span></code> class. On a block level, the instance of the <code class="docutils literal"><span class="pre">ParameterDict</span></code> class is accessible via <code class="docutils literal"><span class="pre">self.params</span></code> field, and outside of a block one can access all parameters of the network via <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.Block.collect_params">collect_params()</a> method called on a <code class="docutils literal"><span class="pre">container</span></code>. <code class="docutils literal"><span class="pre">ParameterDict</span></code> uses <a class="reference external" href="/api/python/gluon/gluon.html#mxnet.gluon.Parameter">Parameter</a> class to represent parameters inside of Apache MxNet neural network. If parameter doesn’t exist, trying to get a parameter via <code class="docutils literal"><span class="pre">self.params</span></code> will create it automatically.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NormalizationHybridLayer</span><span class="p">(</span><span class="n">gluon</span><span class="o">.</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">scales</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizationHybridLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'weights'</span><span class="p">,</span>
                                           <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                           <span class="n">allow_deferred_init</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scales</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'scales'</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="n">scales</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                      <span class="n">init</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">scales</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span> <span class="c1"># Convert to regular list to make this object serializable</span>
                                      <span class="n">differentiable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">scales</span><span class="p">):</span>
        <span class="n">normalized_data</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_div</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">broadcast_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">broadcast_sub</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
        <span class="n">weighted_data</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">FullyConnected</span><span class="p">(</span><span class="n">normalized_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">num_hidden</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">no_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">scaled_data</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_mul</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="n">weighted_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scaled_data</span>
</pre></div>
</div>
<p>In the example above 2 set of parameters are defined:</p>
<ol class="simple">
<li>Parameter <code class="docutils literal"><span class="pre">weights</span></code> is trainable. Its shape is unknown during construction phase and will be infered on the first run of forward propogation;</li>
<li>Parameter <code class="docutils literal"><span class="pre">scale</span></code> is a constant that doesn’t change. Its shape is defined during construction.</li>
</ol>
<p>Notice a few aspects of this code:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">name_scope()</span></code> method is used to add a prefix to parameter names during saving and loading</li>
<li>Shape is not provided when creating <code class="docutils literal"><span class="pre">weights</span></code>. Instead it is going to be infered from the shape of the input</li>
<li><code class="docutils literal"><span class="pre">Scales</span></code> parameter is initialized and marked as <code class="docutils literal"><span class="pre">differentiable=False</span></code>.</li>
<li><code class="docutils literal"><span class="pre">F</span></code> backend is used for all calculations</li>
<li>The calculation of dot product is done using <code class="docutils literal"><span class="pre">F.FullyConnected()</span></code> method instead of <code class="docutils literal"><span class="pre">F.dot()</span></code> method. The one was chosen over another because the former supports automatic infering shapes of inputs while the latter doesn’t. This is extremely important to know, if one doesn’t want to hard code all the shapes. The best way to learn what operators supports automatic inference of input shapes at the moment is browsing C++ implementation of operators to see if one uses a method <code class="docutils literal"><span class="pre">SHAPE_ASSIGN_CHECK(*in_shape,</span> <span class="pre">fullc::kWeight,</span> <span class="pre">Shape2(param.num_hidden,</span> <span class="pre">num_input));</span></code></li>
<li><code class="docutils literal"><span class="pre">hybrid_forward()</span></code> method signature has changed. It accepts two new arguments: <code class="docutils literal"><span class="pre">weights</span></code> and <code class="docutils literal"><span class="pre">scales</span></code>.</li>
</ul>
<p>The last peculiarity is due to support of imperative and symbolic programming by <code class="docutils literal"><span class="pre">HybridBlock</span></code>. During training phase, parameters are passed to the layer by Apache MxNet framework as additional arguments to the method, because they might need to be converted to a <code class="docutils literal"><span class="pre">Symbol</span></code> depending on if the layer was hybridized. One shouldn’t use <code class="docutils literal"><span class="pre">self.weights</span></code> and <code class="docutils literal"><span class="pre">self.scales</span></code> or <code class="docutils literal"><span class="pre">self.params.get</span></code> in <code class="docutils literal"><span class="pre">hybrid_forward</span></code> except to get shapes of parameters.</p>
<p>Running forward pass on this network is very similar to the previous example, so instead of just doing one forward pass, let’s run whole training for a few epochs to show that <code class="docutils literal"><span class="pre">scales</span></code> parameter doesn’t change during the training while <code class="docutils literal"><span class="pre">weights</span></code> parameter is changing.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_params</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Helper function to print out the state of parameters of NormalizationHybridLayer</span>
<span class="sd">    """</span>
    <span class="k">print</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">hybridlayer_params</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s1">'normalizationhybridlayer'</span> <span class="ow">in</span> <span class="n">k</span> <span class="p">}</span>
    
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">hybridlayer_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'{} = {}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">data</span><span class="p">()))</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">HybridSequential</span><span class="p">()</span>                             <span class="c1"># Define a Neural Network as a sequence of hybrid blocks</span>
<span class="k">with</span> <span class="n">net</span><span class="o">.</span><span class="n">name_scope</span><span class="p">():</span>                                        <span class="c1"># Used to disambiguate saving and loading net parameters</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>                                         <span class="c1"># Add Dense layer with 5 neurons</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">NormalizationHybridLayer</span><span class="p">(</span><span class="n">hidden_units</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                                     <span class="n">scales</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])))</span> <span class="c1"># Add our custom layer</span>
    <span class="n">net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>                                         <span class="c1"># Add Dense layer with 1 neurons</span>


<span class="n">net</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">Xavier</span><span class="p">(</span><span class="n">magnitude</span><span class="o">=</span><span class="mf">2.24</span><span class="p">))</span>                <span class="c1"># Initialize parameters of all layers</span>
<span class="n">net</span><span class="o">.</span><span class="n">hybridize</span><span class="p">()</span>                                               <span class="c1"># Create, optimize and cache computational graph</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>     <span class="c1"># Create 5 random examples with 2 feature each in range [-10, 10]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span>                                <span class="c1"># Mean squared error between output and label</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span>                 <span class="c1"># Init trainer with Stochastic Gradient Descent (sgd) optimization method and parameters for it</span>
                        <span class="s1">'sgd'</span><span class="p">,</span> 
                        <span class="p">{</span><span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">'momentum'</span><span class="p">:</span> <span class="mf">0.9</span> <span class="p">})</span>
                        
<span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>                                       <span class="c1"># Autograd records computations done on NDArrays inside "with" block </span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                                       <span class="c1"># Run forward propogation</span>
    
    <span class="n">print_params</span><span class="p">(</span><span class="s2">"=========== Parameters after forward pass ===========</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">net</span><span class="p">)</span>    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>                            <span class="c1"># Calculate MSE</span>
    
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                                               <span class="c1"># Backward computes gradients and stores them as a separate array within each NDArray in .grad field</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>                                  <span class="c1"># Trainer updates parameters of every block, using .grad field using oprimization method (sgd in this example)</span>
                                                              <span class="c1"># We provide batch size that is used as a divider in cost function formula</span>
<span class="n">print_params</span><span class="p">(</span><span class="s2">"=========== Parameters after backward pass ===========</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">===========</span> <span class="n">Parameters</span> <span class="n">after</span> <span class="n">forward</span> <span class="k">pass</span> <span class="o">===========</span>

<span class="n">hybridsequential94_normalizationhybridlayer0_weights</span> <span class="o">=</span> 
<span class="p">[[</span><span class="o">-</span><span class="mf">0.3983642</span>  <span class="o">-</span><span class="mf">0.505708</span>   <span class="o">-</span><span class="mf">0.02425683</span> <span class="o">-</span><span class="mf">0.3133553</span>  <span class="o">-</span><span class="mf">0.35161012</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.6467543</span>   <span class="mf">0.3918715</span>  <span class="o">-</span><span class="mf">0.6154656</span>  <span class="o">-</span><span class="mf">0.20702496</span> <span class="o">-</span><span class="mf">0.4243446</span> <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.6077331</span>   <span class="mf">0.03922009</span>  <span class="mf">0.13425875</span>  <span class="mf">0.5729856</span>  <span class="o">-</span><span class="mf">0.14446527</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.3572498</span>   <span class="mf">0.18545026</span> <span class="o">-</span><span class="mf">0.09098256</span>  <span class="mf">0.5106366</span>  <span class="o">-</span><span class="mf">0.35151464</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.39846328</span>  <span class="mf">0.22245121</span>  <span class="mf">0.13075739</span>  <span class="mf">0.33387476</span> <span class="o">-</span><span class="mf">0.10088372</span><span class="p">]]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">5</span><span class="n">x5</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>

<span class="n">hybridsequential94_normalizationhybridlayer0_scales</span> <span class="o">=</span> 
<span class="p">[</span><span class="mf">2.</span><span class="p">]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>

<span class="o">===========</span> <span class="n">Parameters</span> <span class="n">after</span> <span class="n">backward</span> <span class="k">pass</span> <span class="o">===========</span>

<span class="n">hybridsequential94_normalizationhybridlayer0_weights</span> <span class="o">=</span> 
<span class="p">[[</span><span class="o">-</span><span class="mf">0.29839832</span> <span class="o">-</span><span class="mf">0.47213346</span>  <span class="mf">0.08348035</span> <span class="o">-</span><span class="mf">0.2324698</span>  <span class="o">-</span><span class="mf">0.27368504</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.76268613</span>  <span class="mf">0.43080837</span> <span class="o">-</span><span class="mf">0.49052125</span> <span class="o">-</span><span class="mf">0.11322092</span> <span class="o">-</span><span class="mf">0.3339738</span> <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.48665082</span> <span class="o">-</span><span class="mf">0.00144657</span>  <span class="mf">0.00376363</span>  <span class="mf">0.47501418</span> <span class="o">-</span><span class="mf">0.23885089</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.22626656</span>  <span class="mf">0.22944227</span>  <span class="mf">0.05018325</span>  <span class="mf">0.6166192</span>  <span class="o">-</span><span class="mf">0.24941102</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.44946212</span>  <span class="mf">0.20532274</span>  <span class="mf">0.07579394</span>  <span class="mf">0.29261002</span> <span class="o">-</span><span class="mf">0.14063817</span><span class="p">]]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">5</span><span class="n">x5</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>

<span class="n">hybridsequential94_normalizationhybridlayer0_scales</span> <span class="o">=</span> 
<span class="p">[</span><span class="mf">2.</span><span class="p">]</span>
<span class="o"><</span><span class="n">NDArray</span> <span class="mi">1</span> <span class="nd">@cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">></span>
</pre></div>
</div>
<p>As it is seen from the output above, <code class="docutils literal"><span class="pre">weights</span></code> parameter has been changed by the training and <code class="docutils literal"><span class="pre">scales</span></code> not.</p>
</div>
<div class="section" id="conclusion">
<span id="conclusion"></span><h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>One important quality of a Deep learning framework is extensibility. Empowered by flexible abstractions, like <code class="docutils literal"><span class="pre">Block</span></code> and <code class="docutils literal"><span class="pre">HybridBlock</span></code>, one can easily extend Apache MxNet functionality to match its needs.</p>
<div class="btn-group" role="group">
<div class="download-btn"><a download="custom_layer.ipynb" href="custom_layer.ipynb"><span class="glyphicon glyphicon-download-alt"></span> custom_layer.ipynb</a></div></div></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar rightsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h3><a href="../../index.html">Table Of Contents</a></h3>
<ul>
<li><a class="reference internal" href="#">How to write a custom layer in Apache MxNet Gluon API</a><ul>
<li><a class="reference internal" href="#the-simplest-custom-layer">The simplest custom layer</a></li>
<li><a class="reference internal" href="#hybridization-and-the-difference-between-block-and-hybridblock">Hybridization and the difference between Block and HybridBlock</a></li>
<li><a class="reference internal" href="#adding-a-custom-layer-to-a-network">Adding a custom layer to a network</a></li>
<li><a class="reference internal" href="#parameters-of-a-custom-layer">Parameters of a custom layer</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div><div class="footer">
<div class="section-disclaimer">
<div class="container">
<div>
<img height="60" src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/apache_incubator_logo.png"/>
<p>
            Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <strong>sponsored by the <i>Apache Incubator</i></strong>. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
        </p>
<p>
            "Copyright © 2017-2018, The Apache Software Foundation
            Apache MXNet, MXNet, Apache, the Apache feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the Apache Software Foundation."
        </p>
</div>
</div>
</div>
</div> <!-- pagename != index -->
</div>
<script crossorigin="anonymous" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<script src="../../_static/js/sidebar.js" type="text/javascript"></script>
<script src="../../_static/js/search.js" type="text/javascript"></script>
<script src="../../_static/js/navbar.js" type="text/javascript"></script>
<script src="../../_static/js/clipboard.min.js" type="text/javascript"></script>
<script src="../../_static/js/copycode.js" type="text/javascript"></script>
<script src="../../_static/js/page.js" type="text/javascript"></script>
<script src="../../_static/js/docversion.js" type="text/javascript"></script>
<script type="text/javascript">
        $('body').ready(function () {
            $('body').css('visibility', 'visible');
        });
    </script>
</body>
</html>