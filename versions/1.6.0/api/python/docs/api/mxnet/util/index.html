<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <style>
        .dropdown {
        position: relative;
        display: inline-block;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #f9f9f9;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
        padding: 12px 16px;
        z-index: 1;
        text-align: left;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown-option:hover {
        color: #FF4500 !important;
    }

    .dropdown-option-active {
        color: #FF4500;
        font-weight: lighter;
    }

    .dropdown-option {
        color: #000000;
        font-weight: lighter;
    }

    .dropdown-header {
        color: #FFFFFF;
        display: inline-flex;
    }

    .dropdown-caret {
        width: 18px;
    }

    .dropdown-caret-path {
        fill: #FFFFFF;
    }
    </style>
    
    <title>mxnet.util &#8212; Apache MXNet  documentation</title>

    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mxnet.css" />
    <link rel="stylesheet" href="../../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/google_analytics.js"></script>
    <script src="../../../_static/autodoc.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../_static/mxnet-icon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="mxnet.visualization" href="../visualization/index.html" />
    <link rel="prev" title="mxnet.torch" href="../torch/index.html" /> 
  </head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
      <a class="site-title" rel="author" href="/versions/1.6.0/"><img
            src="../../../_static/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/versions/1.6.0/get_started">Get Started</a>
        <a class="page-link" href="/versions/1.6.0/blog">Blog</a>
        <a class="page-link" href="/versions/1.6.0/features">Features</a>
        <a class="page-link" href="/versions/1.6.0/ecosystem">Ecosystem</a>
        <a class="page-link page-current" href="/versions/1.6.0/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
        <div class="dropdown">
          <span class="dropdown-header">1.6.0
            <svg class="dropdown-caret" viewBox="0 0 32 32" class="icon icon-caret-bottom" aria-hidden="true"><path class="dropdown-caret-path" d="M24 11.305l-7.997 11.39L8 11.305z"></path></svg>
          </span>
          <div class="dropdown-content">
            <a class="dropdown-option" href="/">master</a><br>
            <a class="dropdown-option" href="/versions/1.7.0/">1.7.0</a><br>
            <a class="dropdown-option-active" href="/versions/1.6.0/">1.6.0</a><br>
            <a class="dropdown-option" href="/versions/1.5.0/">1.5.0</a><br>
            <a class="dropdown-option" href="/versions/1.4.1/">1.4.1</a><br>
            <a class="dropdown-option" href="/versions/1.3.1/">1.3.1</a><br>
            <a class="dropdown-option" href="/versions/1.2.1/">1.2.1</a><br>
            <a class="dropdown-option" href="/versions/1.1.0/">1.1.0</a><br>
            <a class="dropdown-option" href="/versions/1.0.0/">1.0.0</a><br>
            <a class="dropdown-option" href="/versions/0.12.1/">0.12.1</a><br>
            <a class="dropdown-option" href="/versions/0.11.0/">0.11.0</a>
          </div>
        </div>
      </div>
    </nav>
  </div>
</header>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../index.html">Python API</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="../index.html">mxnet</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">mxnet.util</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../../../_sources/api/mxnet/util/index.rst" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/1-ndarray.html">Manipulate data with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/2-nn.html">Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/3-autograd.html">Automatic differentiation with <code class="docutils literal notranslate"><span class="pre">autograd</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/4-train.html">Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/5-predict.html">Predict with a pre-trained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/6-use_gpus.html">Use GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/image-augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/pretrained_models.html">Using pre-trained models in MXNet</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/train.html">Train a Linear Regression Model with Sparse Symbols</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/super_resolution.html">Importing an ONNX model into MXNet</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_quantization.html">Quantize with MKL-DNN backend</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/index.html">TensorRT</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/tensorrt.html">Optimized GPU Inference</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/scala.html">Deploy into a Java or Scala Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/wine_detector.html">Real-time Object Detection with MXNet On The Raspberry Pi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/custom_layer.html">Custom Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../metric/index.html">mxnet.metric</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/symbol.html">symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">mxnet</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../attribute/index.html">mxnet.attribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../base/index.html">mxnet.base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../context/index.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../executor_manager/index.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libinfo/index.html">mxnet.libinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../log/index.html">mxnet.log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model/index.html">mxnet.model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../name/index.html">mxnet.name</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebook/index.html">mxnet.notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operator/index.html">mxnet.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../random/index.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../registry/index.html">mxnet.registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../torch/index.html">mxnet.torch</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mxnet.util</a></li>
<li class="toctree-l3"><a class="reference internal" href="../visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../../../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/1-ndarray.html">Manipulate data with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/2-nn.html">Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/3-autograd.html">Automatic differentiation with <code class="docutils literal notranslate"><span class="pre">autograd</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/4-train.html">Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/5-predict.html">Predict with a pre-trained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/6-use_gpus.html">Use GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/image-augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/pretrained_models.html">Using pre-trained models in MXNet</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/train.html">Train a Linear Regression Model with Sparse Symbols</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/super_resolution.html">Importing an ONNX model into MXNet</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_quantization.html">Quantize with MKL-DNN backend</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/index.html">TensorRT</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/tensorrt.html">Optimized GPU Inference</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/scala.html">Deploy into a Java or Scala Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/wine_detector.html">Real-time Object Detection with MXNet On The Raspberry Pi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/custom_layer.html">Custom Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../metric/index.html">mxnet.metric</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/symbol.html">symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">mxnet</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../attribute/index.html">mxnet.attribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../base/index.html">mxnet.base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../context/index.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../executor_manager/index.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libinfo/index.html">mxnet.libinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../log/index.html">mxnet.log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model/index.html">mxnet.model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../name/index.html">mxnet.name</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebook/index.html">mxnet.notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operator/index.html">mxnet.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../random/index.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../registry/index.html">mxnet.registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../torch/index.html">mxnet.torch</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mxnet.util</a></li>
<li class="toctree-l3"><a class="reference internal" href="../visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="module-mxnet.util">
<span id="mxnet-util"></span><h1>mxnet.util<a class="headerlink" href="#module-mxnet.util" title="Permalink to this headline"></a></h1>
<p>general utility functions</p>
<p><strong>Functions</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.get_cuda_compute_capability" title="mxnet.util.get_cuda_compute_capability"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_cuda_compute_capability</span></code></a>(ctx)</p></td>
<td><p>Returns the cuda compute capability of the input <cite>ctx</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.is_np_array" title="mxnet.util.is_np_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_np_array</span></code></a>()</p></td>
<td><p>Checks whether the NumPy-array semantics is currently turned on.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.is_np_shape" title="mxnet.util.is_np_shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_np_shape</span></code></a>()</p></td>
<td><p>Checks whether the NumPy shape semantics is currently turned on.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.makedirs" title="mxnet.util.makedirs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">makedirs</span></code></a>(d)</p></td>
<td><p>Create directories recursively if they dont exist.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.np_array" title="mxnet.util.np_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">np_array</span></code></a>([active])</p></td>
<td><p>Returns an activated/deactivated NumPy-array scope to be used in with statement and captures code that needs the NumPy-array semantics.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.np_shape" title="mxnet.util.np_shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">np_shape</span></code></a>([active])</p></td>
<td><p>Returns an activated/deactivated NumPy shape scope to be used in with statement and captures code that needs the NumPy shape semantics, i.e.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.np_ufunc_legal_option" title="mxnet.util.np_ufunc_legal_option"><code class="xref py py-obj docutils literal notranslate"><span class="pre">np_ufunc_legal_option</span></code></a>(key,value)</p></td>
<td><p>Checking if ufunc arguments are legal inputs</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.reset_np" title="mxnet.util.reset_np"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_np</span></code></a>()</p></td>
<td><p>Deactivate NumPy shape and array semantics at the same time.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.set_module" title="mxnet.util.set_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_module</span></code></a>(module)</p></td>
<td><p>Decorator for overriding __module__ on a function or class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.set_np" title="mxnet.util.set_np"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_np</span></code></a>([shape,array])</p></td>
<td><p>Setting NumPy shape and array semantics at the same time.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.set_np_shape" title="mxnet.util.set_np_shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_np_shape</span></code></a>(active)</p></td>
<td><p>Turns on/off NumPy shape semantics, in which <cite>()</cite> represents the shape of scalar tensors, and tuples with <cite>0</cite> elements, for example, <cite>(0,)</cite>, <cite>(1, 0, 2)</cite>, represent the shapes of zero-size tensors.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.use_np" title="mxnet.util.use_np"><code class="xref py py-obj docutils literal notranslate"><span class="pre">use_np</span></code></a>(func)</p></td>
<td><p>A convenience decorator for wrapping user provided functions and classes in the scope of both NumPy-shape and NumPy-array semantics, which means that (1) empty tuples <cite>()</cite> and tuples with zeros, such as <cite>(0, 1)</cite>, <cite>(1, 0, 2)</cite>, will be treated as scalar tensors shapes and zero-size tensors shapes in shape inference functions of operators, instead of as unknown in legacy mode; (2) ndarrays of type <cite>mxnet.numpy.ndarray</cite> should be created instead of <cite>mx.nd.NDArray</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.use_np_array" title="mxnet.util.use_np_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">use_np_array</span></code></a>(func)</p></td>
<td><p>A decorator wrapping Gluon <cite>Block`s and all its methods, properties, and static functions with the semantics of NumPy-array, which means that where ndarrays are created, `mxnet.numpy.ndarray`s should be created, instead of legacy ndarrays of type `mx.nd.NDArray</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.use_np_shape" title="mxnet.util.use_np_shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">use_np_shape</span></code></a>(func)</p></td>
<td><p>A decorator wrapping a function or class with activated NumPy-shape semantics.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.wrap_np_binary_func" title="mxnet.util.wrap_np_binary_func"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wrap_np_binary_func</span></code></a>(func)</p></td>
<td><p>A convenience decorator for wrapping numpy-compatible binary ufuncs to provide uniform error handling.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.util.wrap_np_unary_func" title="mxnet.util.wrap_np_unary_func"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wrap_np_unary_func</span></code></a>(func)</p></td>
<td><p>A convenience decorator for wrapping numpy-compatible unary ufuncs to provide uniform error handling.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.util.wraps_safely" title="mxnet.util.wraps_safely"><code class="xref py py-obj docutils literal notranslate"><span class="pre">wraps_safely</span></code></a>(wrapped[,assigned])</p></td>
<td><p>This function is safe version of <cite>functools.wraps</cite> in Python2 which skips wrapping functions for the attributes that do not exist.</p></td>
</tr>
</tbody>
</table>
<dl class="function">
<dt id="mxnet.util.get_cuda_compute_capability">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">get_cuda_compute_capability</code><span class="sig-paren">(</span><em class="sig-param">ctx</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#get_cuda_compute_capability"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.get_cuda_compute_capability" title="Permalink to this definition"></a></dt>
<dd><p>Returns the cuda compute capability of the input <cite>ctx</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ctx</strong> (<a class="reference internal" href="../context/index.html#mxnet.context.Context" title="mxnet.context.Context"><em>Context</em></a>)  GPU context whose corresponding cuda compute capability is to be retrieved.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>cuda_compute_capability</strong>  CUDA compute capability. For example, it returns 70 for CUDA arch equal to <cite>sm_70</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">References</p>
<p><a class="reference external" href="https://gist.github.com/f0k/63a664160d016a491b2cbea15913d549#file-cuda_check-py">https://gist.github.com/f0k/63a664160d016a491b2cbea15913d549#file-cuda_check-py</a></p>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.is_np_array">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">is_np_array</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#is_np_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.is_np_array" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether the NumPy-array semantics is currently turned on.
This is currently used in Gluon for checking whether an array of type <cite>mxnet.numpy.ndarray</cite>
or <cite>mx.nd.NDArray</cite> should be created. For example, at the time when a parameter
is created in a <cite>Block</cite>, an <cite>mxnet.numpy.ndarray</cite> is created if this returns true; else
an <cite>mx.nd.NDArray</cite> is created.</p>
<p>Normally, users are not recommended to use this API directly unless you known exactly
what is going on under the hood.</p>
<p>Please note that this is designed as an infrastructure for the incoming
MXNet-NumPy operators. Legacy operators registered in the modules
<cite>mx.nd</cite> and <cite>mx.sym</cite> are not guaranteed to behave like their counterparts
in NumPy within this semantics.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>A bool value indicating whether the NumPy-array semantics is currently on.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.is_np_shape">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">is_np_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#is_np_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.is_np_shape" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether the NumPy shape semantics is currently turned on.
In NumPy shape semantics, <cite>()</cite> represents the shape of scalar tensors,
and tuples with <cite>0</cite> elements, for example, <cite>(0,)</cite>, <cite>(1, 0, 2)</cite>, represent
the shapes of zero-size tensors. This is turned off by default for keeping
backward compatibility.</p>
<p>In the NumPy shape semantics, <cite>-1</cite> indicates an unknown size. For example,
<cite>(-1, 2, 2)</cite> means that the size of the first dimension is unknown. Its size
may be inferred during shape inference.</p>
<p>Please note that this is designed as an infrastructure for the incoming
MXNet-NumPy operators. Legacy operators registered in the modules
<cite>mx.nd</cite> and <cite>mx.sym</cite> are not guaranteed to behave like their counterparts
in NumPy within this semantics.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>A bool value indicating whether the NumPy shape semantics is currently on.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mxnet</span> <span class="k">as</span> <span class="nn">mx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prev_state</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">set_np_shape</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">prev_state</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">is_np_shape</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.makedirs">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">makedirs</code><span class="sig-paren">(</span><em class="sig-param">d</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#makedirs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.makedirs" title="Permalink to this definition"></a></dt>
<dd><p>Create directories recursively if they dont exist. os.makedirs(exist_ok=True) is not
available in Python2</p>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.np_array">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">np_array</code><span class="sig-paren">(</span><em class="sig-param">active=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#np_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.np_array" title="Permalink to this definition"></a></dt>
<dd><p>Returns an activated/deactivated NumPy-array scope to be used in with statement
and captures code that needs the NumPy-array semantics.</p>
<p>Currently, this is used in Gluon to enforce array creation in <cite>Block`s as type
`mxnet.numpy.ndarray</cite>, instead of <cite>mx.nd.NDArray</cite>.</p>
<p>It is recommended to use the decorator <cite>use_np_array</cite> to decorate the classes
that need this semantics, instead of using this function in a <cite>with</cite> statement
unless you know exactly what has been scoped by this semantics.</p>
<p>Please note that this is designed as an infrastructure for the incoming
MXNet-NumPy operators. Legacy operators registered in the modules
<cite>mx.nd</cite> and <cite>mx.sym</cite> are not guaranteed to behave like their counterparts
in NumPy even within this scope.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>active</strong> (<em>bool</em>)  Indicates whether to activate NumPy-array semantics.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A scope object for wrapping the code w/ or w/o NumPy-shape semantics.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>_NumpyShapeScope</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.np_shape">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">np_shape</code><span class="sig-paren">(</span><em class="sig-param">active=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#np_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.np_shape" title="Permalink to this definition"></a></dt>
<dd><p>Returns an activated/deactivated NumPy shape scope to be used in with statement
and captures code that needs the NumPy shape semantics, i.e. support of scalar and
zero-size tensors.</p>
<p>Please note that this is designed as an infrastructure for the incoming
MXNet-NumPy operators. Legacy operators registered in the modules
<cite>mx.nd</cite> and <cite>mx.sym</cite> are not guaranteed to behave like their counterparts
in NumPy even within this scope.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>active</strong> (<em>bool</em>)  Indicates whether to activate NumPy-shape semantics.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul>
<li><p><em>_NumpyShapeScope</em>  A scope object for wrapping the code w/ or w/o NumPy-shape semantics.</p></li>
<li><p><em>Example::</em> </p>
<dl>
<dt>with mx.np_shape(active=True):</dt><dd><p># A scalar tensors shape is <cite>()</cite>, whose <cite>ndim</cite> is <cite>0</cite>.
scalar = mx.nd.ones(shape=())
assert scalar.shape == ()</p>
<p># If NumPy shape semantics is enabled, 0 in a shape means that
# dimension contains zero elements.
data = mx.sym.var(data, shape=(0, 2, 3))
ret = mx.sym.sin(data)
arg_shapes, out_shapes, _ = ret.infer_shape()
assert arg_shapes[0] == (0, 2, 3)
assert out_shapes[0] == (0, 2, 3)</p>
<p># -1 means unknown shape dimension size in the new NumPy shape definition
data = mx.sym.var(data, shape=(-1, 2, 3))
ret = mx.sym.sin(data)
arg_shapes, out_shapes, _ = ret.infer_shape_partial()
assert arg_shapes[0] == (-1, 2, 3)
assert out_shapes[0] == (-1, 2, 3)</p>
<p># When a shape is completely unknown when NumPy shape semantics is on, it is
# represented as <cite>None</cite> in Python.
data = mx.sym.var(data)
ret = mx.sym.sin(data)
arg_shapes, out_shapes, _ = ret.infer_shape_partial()
assert arg_shapes[0] is None
assert out_shapes[0] is None</p>
</dd>
<dt>with mx.np_shape(active=False):</dt><dd><p># 0 means unknown shape dimension size in the legacy shape definition.
data = mx.sym.var(data, shape=(0, 2, 3))
ret = mx.sym.sin(data)
arg_shapes, out_shapes, _ = ret.infer_shape_partial()
assert arg_shapes[0] == (0, 2, 3)
assert out_shapes[0] == (0, 2, 3)</p>
<p># When a shape is completely unknown in the legacy mode (default), its ndim is
# equal to 0 and it is represented as <cite>()</cite> in Python.
data = mx.sym.var(data)
ret = mx.sym.sin(data)
arg_shapes, out_shapes, _ = ret.infer_shape_partial()
assert arg_shapes[0] == ()
assert out_shapes[0] == ()</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.np_ufunc_legal_option">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">np_ufunc_legal_option</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#np_ufunc_legal_option"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.np_ufunc_legal_option" title="Permalink to this definition"></a></dt>
<dd><p>Checking if ufunc arguments are legal inputs</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> (<em>string</em>)  the key of the ufunc argument.</p></li>
<li><p><strong>value</strong> (<em>string</em>)  the value of the ufunc argument.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>legal</strong>  Whether or not the argument is a legal one. True when the key is one of the ufunc
arguments and value is an allowed value. False when the key is not one of the ufunc
arugments or the value is not an allowed value even when the key is a legal one.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>boolean</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.reset_np">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">reset_np</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#reset_np"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.reset_np" title="Permalink to this definition"></a></dt>
<dd><p>Deactivate NumPy shape and array semantics at the same time.</p>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.set_module">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">set_module</code><span class="sig-paren">(</span><em class="sig-param">module</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#set_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.set_module" title="Permalink to this definition"></a></dt>
<dd><p>Decorator for overriding __module__ on a function or class.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@set_module</span><span class="p">(</span><span class="s1">&#39;mxnet.numpy&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="k">assert</span> <span class="n">example</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">==</span> <span class="s1">&#39;numpy&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.set_np">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">set_np</code><span class="sig-paren">(</span><em class="sig-param">shape=True</em>, <em class="sig-param">array=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#set_np"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.set_np" title="Permalink to this definition"></a></dt>
<dd><p>Setting NumPy shape and array semantics at the same time.
It is required to keep NumPy shape semantics active while activating NumPy array semantics.
Deactivating NumPy shape semantics while NumPy array semantics is still active is not allowed.
It is highly recommended to set these two flags to <cite>True</cite> at the same time to fully enable
NumPy-like behaviors. Please refer to the Examples section for a better understanding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<em>bool</em>)  A boolean value indicating whether the NumPy-shape semantics should be turned on or off.
When this flag is set to <cite>True</cite>, zero-size and zero-dim shapes are all valid shapes in
shape inference process, instead of treated as unknown shapes in legacy mode.</p></li>
<li><p><strong>array</strong> (<em>bool</em>)  A boolean value indicating whether the NumPy-array semantics should be turned on or off.
When this flag is set to <cite>True</cite>, it enables Gluon code flow to use or generate <cite>mxnet.numpy.ndarray`s
instead of `mxnet.ndarray.NDArray</cite>. For example, a <cite>Block</cite> would create parameters of type
<cite>mxnet.numpy.ndarray</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mxnet</span> <span class="k">as</span> <span class="nn">mx</span>
</pre></div>
</div>
<p>Creating zero-dim ndarray in legacy mode would fail at shape inference.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">())</span>
<span class="go">mxnet.base.MXNetError: Operator _ones inferring shapes failed.</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">mxnet.base.MXNetError: Operator _ones inferring shapes failed.</span>
</pre></div>
</div>
<p>In legacy mode, Gluon layers would create parameters and outputs of type <cite>mx.nd.NDArray</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">[[0.01983214 0.07832371]</span>
<span class="go"> [0.01983214 0.07832371]</span>
<span class="go"> [0.01983214 0.07832371]]</span>
<span class="go">&lt;NDArray 3x2 @cpu(0)&gt;</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dense</span><span class="o">.</span><span class="n">collect_params</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
<span class="go">[</span>
<span class="go">[[0.0068339  0.01299825]</span>
<span class="go"> [0.0301265  0.04819721]]</span>
<span class="go">&lt;NDArray 2x2 @cpu(0)&gt;,</span>
<span class="go">[0. 0.]</span>
<span class="go">&lt;NDArray 2 @cpu(0)&gt;]</span>
</pre></div>
</div>
<p>When the <cite>shape</cite> flag is <cite>True</cite>, both shape inferences are successful.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>  <span class="c1"># this is required to activate NumPy-like behaviors</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">())</span>
<span class="go">array(1.)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">array([], shape=(2, 0, 3))</span>
</pre></div>
</div>
<p>When the <cite>array</cite> flag is <cite>True</cite>, Gluon layers would create parameters and outputs of type <cite>mx.np.ndarray</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">array([[0.01983214, 0.07832371],</span>
<span class="go">       [0.01983214, 0.07832371],</span>
<span class="go">       [0.01983214, 0.07832371]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dense</span><span class="o">.</span><span class="n">collect_params</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
<span class="go">[array([[0.0068339 , 0.01299825],</span>
<span class="go">       [0.0301265 , 0.04819721]]), array([0., 0.])]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.set_np_shape">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">set_np_shape</code><span class="sig-paren">(</span><em class="sig-param">active</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#set_np_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.set_np_shape" title="Permalink to this definition"></a></dt>
<dd><p>Turns on/off NumPy shape semantics, in which <cite>()</cite> represents the shape of scalar tensors,
and tuples with <cite>0</cite> elements, for example, <cite>(0,)</cite>, <cite>(1, 0, 2)</cite>, represent the shapes
of zero-size tensors. This is turned off by default for keeping backward compatibility.</p>
<p>Please note that this is designed as an infrastructure for the incoming
MXNet-NumPy operators. Legacy operators registered in the modules
<cite>mx.nd</cite> and <cite>mx.sym</cite> are not guaranteed to behave like their counterparts
in NumPy within this semantics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>active</strong> (<em>bool</em>)  Indicates whether to turn on/off NumPy shape semantics.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A bool value indicating the previous state of NumPy shape semantics.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mxnet</span> <span class="k">as</span> <span class="nn">mx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prev_state</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">set_np_shape</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">prev_state</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">is_np_shape</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.use_np">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">use_np</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#use_np"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.use_np" title="Permalink to this definition"></a></dt>
<dd><p>A convenience decorator for wrapping user provided functions and classes in the scope of
both NumPy-shape and NumPy-array semantics, which means that (1) empty tuples <cite>()</cite> and tuples
with zeros, such as <cite>(0, 1)</cite>, <cite>(1, 0, 2)</cite>, will be treated as scalar tensors shapes and
zero-size tensors shapes in shape inference functions of operators, instead of as unknown
in legacy mode; (2) ndarrays of type <cite>mxnet.numpy.ndarray</cite> should be created instead of
<cite>mx.nd.NDArray</cite>.</p>
<dl>
<dt>Example::</dt><dd><p>import mxnet as mx
from mxnet import gluon, np</p>
<dl class="simple">
<dt>class TestHybridBlock1(gluon.HybridBlock):</dt><dd><dl class="simple">
<dt>def __init__(self):</dt><dd><p>super(TestHybridBlock1, self).__init__()
self.w = self.params.get(w, shape=(2, 2))</p>
</dd>
<dt>def hybrid_forward(self, F, x, w):</dt><dd><p>return F.dot(x, w) + F.ones((1,))</p>
</dd>
</dl>
</dd>
</dl>
<p>x = mx.nd.ones((2, 2))
net1 = TestHybridBlock1()
net1.initialize()
out = net1.forward(x)
for _, v in net1.collect_params().items():</p>
<blockquote>
<div><p>assert type(v.data()) is mx.nd.NDArray</p>
</div></blockquote>
<p>assert type(out) is mx.nd.NDArray</p>
<p>&#64;np.use_np
class TestHybridBlock2(gluon.HybridBlock):</p>
<blockquote>
<div><dl class="simple">
<dt>def __init__(self):</dt><dd><p>super(TestHybridBlock2, self).__init__()
self.w = self.params.get(w, shape=(2, 2))</p>
</dd>
<dt>def hybrid_forward(self, F, x, w):</dt><dd><p>return F.np.dot(x, w) + F.np.ones(())</p>
</dd>
</dl>
</div></blockquote>
<p>x = np.ones((2, 2))
net2 = TestHybridBlock2()
net2.initialize()
out = net2.forward(x)
for _, v in net2.collect_params().items():</p>
<blockquote>
<div><p>print(type(v.data()))
assert type(v.data()) is np.ndarray</p>
</div></blockquote>
<p>assert type(out) is np.ndarray</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>a user-provided callable function</em><em> or </em><em>class to be scoped by the</em>)  </p></li>
<li><p><strong>and NumPy-array semantics.</strong> (<em>NumPy-shape</em>)  </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function or class wrapped in the Numpy-shape and NumPy-array scope.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../autograd/index.html#mxnet.autograd.Function" title="mxnet.autograd.Function">Function</a> or class</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.use_np_array">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">use_np_array</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#use_np_array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.use_np_array" title="Permalink to this definition"></a></dt>
<dd><p>A decorator wrapping Gluon <cite>Block`s and all its methods, properties, and static functions
with the semantics of NumPy-array, which means that where ndarrays are created,
`mxnet.numpy.ndarray`s should be created, instead of legacy ndarrays of type `mx.nd.NDArray</cite>.
For example, at the time when a parameter is created in a <cite>Block</cite>, an <cite>mxnet.numpy.ndarray</cite>
is created if its decorated with this decorator.</p>
<dl>
<dt>Example::</dt><dd><p>import mxnet as mx
from mxnet import gluon, np</p>
<dl class="simple">
<dt>class TestHybridBlock1(gluon.HybridBlock):</dt><dd><dl class="simple">
<dt>def __init__(self):</dt><dd><p>super(TestHybridBlock1, self).__init__()
self.w = self.params.get(w, shape=(2, 2))</p>
</dd>
<dt>def hybrid_forward(self, F, x, w):</dt><dd><p>return F.dot(x, w)</p>
</dd>
</dl>
</dd>
</dl>
<p>x = mx.nd.ones((2, 2))
net1 = TestHybridBlock1()
net1.initialize()
out = net1.forward(x)
for _, v in net1.collect_params().items():</p>
<blockquote>
<div><p>assert type(v.data()) is mx.nd.NDArray</p>
</div></blockquote>
<p>assert type(out) is mx.nd.NDArray</p>
<p>&#64;np.use_np_array
class TestHybridBlock2(gluon.HybridBlock):</p>
<blockquote>
<div><dl class="simple">
<dt>def __init__(self):</dt><dd><p>super(TestHybridBlock2, self).__init__()
self.w = self.params.get(w, shape=(2, 2))</p>
</dd>
<dt>def hybrid_forward(self, F, x, w):</dt><dd><p>return F.np.dot(x, w)</p>
</dd>
</dl>
</div></blockquote>
<p>x = np.ones((2, 2))
net2 = TestHybridBlock2()
net2.initialize()
out = net2.forward(x)
for _, v in net2.collect_params().items():</p>
<blockquote>
<div><p>print(type(v.data()))
assert type(v.data()) is np.ndarray</p>
</div></blockquote>
<p>assert type(out) is np.ndarray</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>a user-provided callable function</em><em> or </em><em>class to be scoped by the NumPy-array semantics.</em>)  </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function or class wrapped in the NumPy-array scope.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../autograd/index.html#mxnet.autograd.Function" title="mxnet.autograd.Function">Function</a> or class</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.use_np_shape">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">use_np_shape</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#use_np_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.use_np_shape" title="Permalink to this definition"></a></dt>
<dd><p>A decorator wrapping a function or class with activated NumPy-shape semantics.
When <cite>func</cite> is a function, this ensures that the execution of the function is scoped with NumPy
shape semantics, such as the support for zero-dim and zero size tensors. When
<cite>func</cite> is a class, it ensures that all the methods, static functions, and properties
of the class are executed with the NumPy shape semantics.</p>
<dl>
<dt>Example::</dt><dd><p>import mxnet as mx
&#64;mx.use_np_shape
def scalar_one():</p>
<blockquote>
<div><p>return mx.nd.ones(())</p>
</div></blockquote>
<p>print(scalar_one())</p>
<p>&#64;np.use_np_shape
class ScalarTensor(object):</p>
<blockquote>
<div><dl>
<dt>def __init__(self, val=None):</dt><dd><dl class="simple">
<dt>if val is None:</dt><dd><p>val = ScalarTensor.random().value</p>
</dd>
</dl>
<p>self._scalar = mx.nd.ones(()) * val</p>
</dd>
<dt>def __repr__(self):</dt><dd><p>print(Is __repr__ in np_shape semantics? {}!.format(str(np.is_np_shape())))
return str(self._scalar.asnumpy())</p>
</dd>
</dl>
<p>&#64;staticmethod
def random():</p>
<blockquote>
<div><p>val = mx.nd.random.uniform().asnumpy().item()
return ScalarTensor(val)</p>
</div></blockquote>
<p>&#64;property
def value(self):</p>
<blockquote>
<div><p>print(Is value property in np_shape semantics? {}!.format(str(np.is_np_shape())))
return self._scalar.asnumpy().item()</p>
</div></blockquote>
</div></blockquote>
<p>print(Is global scope of np_shape activated? {}!.format(str(np.is_np_shape())))
scalar_tensor = ScalarTensor()
print(scalar_tensor)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>a user-provided callable function</em><em> or </em><em>class to be scoped by the NumPy-shape semantics.</em>)  </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function or class wrapped in the NumPy-shape scope.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../autograd/index.html#mxnet.autograd.Function" title="mxnet.autograd.Function">Function</a> or class</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.wrap_np_binary_func">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">wrap_np_binary_func</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#wrap_np_binary_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.wrap_np_binary_func" title="Permalink to this definition"></a></dt>
<dd><p>A convenience decorator for wrapping numpy-compatible binary ufuncs to provide uniform
error handling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>a numpy-compatible binary function to be wrapped for better error handling.</em>)  </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function wrapped with proper error handling.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../autograd/index.html#mxnet.autograd.Function" title="mxnet.autograd.Function">Function</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.wrap_np_unary_func">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">wrap_np_unary_func</code><span class="sig-paren">(</span><em class="sig-param">func</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#wrap_np_unary_func"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.wrap_np_unary_func" title="Permalink to this definition"></a></dt>
<dd><p>A convenience decorator for wrapping numpy-compatible unary ufuncs to provide uniform
error handling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>a numpy-compatible unary function to be wrapped for better error handling.</em>)  </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A function wrapped with proper error handling.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../../autograd/index.html#mxnet.autograd.Function" title="mxnet.autograd.Function">Function</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mxnet.util.wraps_safely">
<code class="sig-prename descclassname">mxnet.util.</code><code class="sig-name descname">wraps_safely</code><span class="sig-paren">(</span><em class="sig-param">wrapped</em>, <em class="sig-param">assigned=('__module__'</em>, <em class="sig-param">'__name__'</em>, <em class="sig-param">'__qualname__'</em>, <em class="sig-param">'__doc__'</em>, <em class="sig-param">'__annotations__')</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/util.html#wraps_safely"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.util.wraps_safely" title="Permalink to this definition"></a></dt>
<dd><p>This function is safe version of <cite>functools.wraps</cite> in Python2 which skips wrapping functions
for the attributes that do not exist.</p>
</dd></dl>

</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>                    

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../torch/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>mxnet.torch</div>
         </div>
     </a>
     <a id="button-next" href="../visualization/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>mxnet.visualization</div>
        </div>
     </a>
  </div>
            <footer class="site-footer h-card">
    <div class="wrapper">
        <div class="row">
            <div class="col-4">
                <h4 class="footer-category-title">Resources</h4>
                <ul class="contact-list">
                    <li><a class="u-email" href="mailto:dev@mxnet.apache.org">Dev list</a></li>
                    <li><a class="u-email" href="mailto:user@mxnet.apache.org">User mailing list</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="https://issues.apache.org/jira/projects/MXNET/issues">Jira Tracker</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/labels/Roadmap">Github Roadmap</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>
                    <li><a href="/versions/1.6.0/community/contribute">Contribute To MXNet</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>

<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="../../../_static/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright  2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>
        
  </body>
</html>