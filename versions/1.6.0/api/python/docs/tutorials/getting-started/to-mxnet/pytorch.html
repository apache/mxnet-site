<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <style>
        .dropdown {
        position: relative;
        display: inline-block;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #f9f9f9;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
        padding: 12px 16px;
        z-index: 1;
        text-align: left;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown-option:hover {
        color: #FF4500 !important;
    }

    .dropdown-option-active {
        color: #FF4500;
        font-weight: lighter;
    }

    .dropdown-option {
        color: #000000;
        font-weight: lighter;
    }

    .dropdown-header {
        color: #FFFFFF;
        display: inline-flex;
    }

    .dropdown-caret {
        width: 18px;
    }

    .dropdown-caret-path {
        fill: #FFFFFF;
    }
    </style>
    
    <title>PyTorch vs Apache MXNet &#8212; Apache MXNet  documentation</title>

    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mxnet.css" />
    <link rel="stylesheet" href="../../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/google_analytics.js"></script>
    <script src="../../../_static/autodoc.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../../../_static/mxnet-icon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Gluon: from experiment to deployment" href="../gluon_from_experiment_to_deployment.html" />
    <link rel="prev" title="Moving to MXNet from Other Frameworks" href="index.html" /> 
  </head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
      <a class="site-title" rel="author" href="/versions/1.6.0/"><img
            src="../../../_static/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/versions/1.6.0/get_started">Get Started</a>
        <a class="page-link" href="/versions/1.6.0/blog">Blog</a>
        <a class="page-link" href="/versions/1.6.0/features">Features</a>
        <a class="page-link" href="/versions/1.6.0/ecosystem">Ecosystem</a>
        <a class="page-link page-current" href="/versions/1.6.0/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
        <div class="dropdown">
          <span class="dropdown-header">1.6.0
            <svg class="dropdown-caret" viewBox="0 0 32 32" class="icon icon-caret-bottom" aria-hidden="true"><path class="dropdown-caret-path" d="M24 11.305l-7.997 11.39L8 11.305z"></path></svg>
          </span>
          <div class="dropdown-content">
            <a class="dropdown-option" href="/">master</a><br>
            <a class="dropdown-option" href="/versions/1.7.0/">1.7.0</a><br>
            <a class="dropdown-option-active" href="/versions/1.6.0/">1.6.0</a><br>
            <a class="dropdown-option" href="/versions/1.5.0/">1.5.0</a><br>
            <a class="dropdown-option" href="/versions/1.4.1/">1.4.1</a><br>
            <a class="dropdown-option" href="/versions/1.3.1/">1.3.1</a><br>
            <a class="dropdown-option" href="/versions/1.2.1/">1.2.1</a><br>
            <a class="dropdown-option" href="/versions/1.1.0/">1.1.0</a><br>
            <a class="dropdown-option" href="/versions/1.0.0/">1.0.0</a><br>
            <a class="dropdown-option" href="/versions/0.12.1/">0.12.1</a><br>
            <a class="dropdown-option" href="/versions/0.11.0/">0.11.0</a>
          </div>
        </div>
      </div>
    </nav>
  </div>
</header>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../index.html">Python Tutorials</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="../index.html">Getting Started</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="index.html">Moving to MXNet from Other Frameworks</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">PyTorch vs Apache MXNet</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../../../_sources/tutorials/getting-started/to-mxnet/pytorch.ipynb" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">Getting Started</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/1-ndarray.html">Manipulate data with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/2-nn.html">Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/3-autograd.html">Automatic differentiation with <code class="docutils literal notranslate"><span class="pre">autograd</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/4-train.html">Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/5-predict.html">Predict with a pre-trained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/6-use_gpus.html">Use GPUs</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">Moving to MXNet from Other Frameworks</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/image-augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/pretrained_models.html">Using pre-trained models in MXNet</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/train.html">Train a Linear Regression Model with Sparse Symbols</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/super_resolution.html">Importing an ONNX model into MXNet</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/mkldnn/mkldnn_quantization.html">Quantize with MKL-DNN backend</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/tensorrt/index.html">TensorRT</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/tensorrt/tensorrt.html">Optimized GPU Inference</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/scala.html">Deploy into a Java or Scala Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/wine_detector.html">Real-time Object Detection with MXNet On The Raspberry Pi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../extend/custom_layer.html">Custom Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../api/gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../api/gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/metric/index.html">mxnet.metric</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/symbol.html">symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/mxnet/index.html">mxnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/attribute/index.html">mxnet.attribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/base/index.html">mxnet.base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/context/index.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/engine/index.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/executor/index.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/executor_manager/index.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/libinfo/index.html">mxnet.libinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/log/index.html">mxnet.log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/model/index.html">mxnet.model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/name/index.html">mxnet.name</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/notebook/index.html">mxnet.notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/operator/index.html">mxnet.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/random/index.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/registry/index.html">mxnet.registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/torch/index.html">mxnet.torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/util/index.html">mxnet.util</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../../../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">Getting Started</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../crash-course/index.html">Crash Course</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/1-ndarray.html">Manipulate data with <code class="docutils literal notranslate"><span class="pre">ndarray</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/2-nn.html">Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/3-autograd.html">Automatic differentiation with <code class="docutils literal notranslate"><span class="pre">autograd</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/4-train.html">Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/5-predict.html">Predict with a pre-trained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../crash-course/6-use_gpus.html">Use GPUs</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">Moving to MXNet from Other Frameworks</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/image-augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/image/pretrained_models.html">Using pre-trained models in MXNet</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/train.html">Train a Linear Regression Model with Sparse Symbols</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../packages/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../packages/onnx/super_resolution.html">Importing an ONNX model into MXNet</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/mkldnn/mkldnn_quantization.html">Quantize with MKL-DNN backend</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/tensorrt/index.html">TensorRT</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../performance/backend/tensorrt/tensorrt.html">Optimized GPU Inference</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/scala.html">Deploy into a Java or Scala Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/inference/wine_detector.html">Real-time Object Detection with MXNet On The Raspberry Pi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../extend/custom_layer.html">Custom Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/gluon/index.html">mxnet.gluon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/gluon/data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../api/gluon/data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../api/gluon/data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/nn/index.html">gluon.nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/rnn/index.html">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/gluon/utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/metric/index.html">mxnet.metric</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/symbol.html">symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/mxnet/index.html">mxnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/attribute/index.html">mxnet.attribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/base/index.html">mxnet.base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/context/index.html">mxnet.context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/engine/index.html">mxnet.engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/executor/index.html">mxnet.executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/executor_manager/index.html">mxnet.executor_manager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/libinfo/index.html">mxnet.libinfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/log/index.html">mxnet.log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/model/index.html">mxnet.model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/name/index.html">mxnet.name</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/notebook/index.html">mxnet.notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/operator/index.html">mxnet.operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/random/index.html">mxnet.random</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/registry/index.html">mxnet.registry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/torch/index.html">mxnet.torch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/util/index.html">mxnet.util</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/mxnet/visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<!--- Licensed to the Apache Software Foundation (ASF) under one --><!--- or more contributor license agreements.  See the NOTICE file --><!--- distributed with this work for additional information --><!--- regarding copyright ownership.  The ASF licenses this file --><!--- to you under the Apache License, Version 2.0 (the --><!--- "License"); you may not use this file except in compliance --><!--- with the License.  You may obtain a copy of the License at --><!---   http://www.apache.org/licenses/LICENSE-2.0 --><!--- Unless required by applicable law or agreed to in writing, --><!--- software distributed under the License is distributed on an --><!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY --><!--- KIND, either express or implied.  See the License for the --><!--- specific language governing permissions and limitations --><!--- under the License. --><div class="section" id="PyTorch-vs-Apache-MXNet">
<h1>PyTorch vs Apache MXNet<a class="headerlink" href="#PyTorch-vs-Apache-MXNet" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://pytorch.org/">PyTorch</a> is a popular deep learning framework due to its easy-to-understand API and its completely imperative approach. Apache MXNet includes the Gluon API which gives you the simplicity and flexibility of PyTorch and allows you to hybridize your network to leverage performance optimizations of the symbolic graph. As of April 2019, <a class="reference external" href="https://developer.nvidia.com/deep-learning-performance-training-inference">NVidia performance benchmarks</a> show that Apache MXNet
outperforms PyTorch by ~77% on training ResNet-50: 10,925 images per second vs.6,175.</p>
<p>In the next 10 minutes, well do a quick comparison between the two frameworks and show how small the learning curve can be when switching from PyTorch to Apache MXNet.</p>
<div class="section" id="Installation">
<h2>Installation<a class="headerlink" href="#Installation" title="Permalink to this headline"></a></h2>
<p>PyTorch uses conda for installation by default, for example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># !conda install pytorch-cpu -c pytorch</span>
</pre></div>
</div>
</div>
<p>For MXNet we use pip:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># !pip install mxnet</span>
</pre></div>
</div>
</div>
<p>To install Apache MXNet with GPU support, you need to specify CUDA version. For example, the snippet below will install Apache MXNet with CUDA 9.2 support:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># !pip install mxnet-cuda92</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data-manipulation">
<h2>Data manipulation<a class="headerlink" href="#Data-manipulation" title="Permalink to this headline"></a></h2>
<p>Both PyTorch and Apache MXNet relies on multidimensional matrices as a data sources. While PyTorch follows Torchs naming convention and refers to multidimensional matrices as tensors, Apache MXNet follows NumPys conventions and refers to them as NDArrays.</p>
<p>In the code snippets below, we create a two-dimensional matrix where each element is initialized to 1. We show how to add 1 to each element of matrices and print the results.</p>
<p><strong>PyTorch:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y</span>
</pre></div>
</div>
</div>
<p><strong>MXNet:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y</span>
</pre></div>
</div>
</div>
<p>The main difference apart from the package name is that the MXNets shape input parameter needs to be passed as a tuple enclosed in parentheses as in NumPy.</p>
<p>Both frameworks support multiple functions to create and manipulate tensors / NDArrays. You can find more of them in the documentation.</p>
</div>
<div class="section" id="Model-training">
<h2>Model training<a class="headerlink" href="#Model-training" title="Permalink to this headline"></a></h2>
<p>After covering the basics of data creation and manipulation, lets dive deep and compare how model training is done in both frameworks. In order to do so, we are going to solve image classification task on MNIST data set using Multilayer Perceptron (MLP) in both frameworks. We divide the task in 4 steps.</p>
<div class="section" id="1.-Read-data">
<h3>1. Read data<a class="headerlink" href="#1.-Read-data" title="Permalink to this headline"></a></h3>
<p>The first step is to obtain the data. We download the MNIST data set from the web and load it into memory so that we can read batches one by one.</p>
<p><strong>PyTorch:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">trans</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.13</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.31</span><span class="p">,))])</span>
<span class="n">pt_train_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trans</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>MXNet:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">gluon</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon.data.vision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="n">trans</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                            <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">)])</span>
<span class="n">mx_train_data</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">transform_first</span><span class="p">(</span><span class="n">trans</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Both frameworks allows you to download MNIST data set from their sources and specify that only training part of the data set is required.</p>
<p>The main difference between the code snippets is that MXNet uses <a class="reference external" href="https://mxnet.apache.org/api/python/docs/api/gluon/_autogen/mxnet.gluon.data.Dataset.html">transform_first</a> method to indicate that the data transformation is done on the first element of the data batch, the MNIST picture, rather than the second element, the label.</p>
</div>
<div class="section" id="2.-Creating-the-model">
<h3>2. Creating the model<a class="headerlink" href="#2.-Creating-the-model" title="Permalink to this headline"></a></h3>
<p>Below we define a Multilayer Perceptron (MLP) with a single hidden layer and 10 units in the output layer.</p>
<p><strong>PyTorch:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">pt_nn</span>

<span class="n">pt_net</span> <span class="o">=</span> <span class="n">pt_nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">pt_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="n">pt_nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">pt_nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p><strong>MXNet:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">mxnet.gluon.nn</span> <span class="k">as</span> <span class="nn">mx_nn</span>

<span class="n">mx_net</span> <span class="o">=</span> <span class="n">mx_nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">mx_net</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mx_nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
           <span class="n">mx_nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">mx_net</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>We used the Sequential container to stack layers one after the other in order to construct the neural network. Apache MXNet differs from PyTorch in the following ways:</p>
<ul class="simple">
<li><p>In PyTorch you have to specify the input size as the first argument of the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> object. Apache MXNet provides an extra flexibility to network structure by automatically inferring the input size after the first forward pass.</p></li>
<li><p>In Apache MXNet you can specify activation functions directly in fully connected and convolutional layers.</p></li>
<li><p>After the model structure is defined, Apache MXNet requires you to explicitly call the model initialization function.</p></li>
</ul>
<p>With a Sequential block, layers are executed one after the other. To have a different execution model, with PyTorch you can inherit from <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and then customize how the <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> function is executed. Similarly, in Apache MXNet you can inherit from <a class="reference external" href="https://mxnet.apache.org/api/python/docs/api/gluon/mxnet.gluon.nn.Block.html">nn.Block</a> to achieve similar results.</p>
</div>
<div class="section" id="3.-Loss-function-and-optimization-algorithm">
<h3>3. Loss function and optimization algorithm<a class="headerlink" href="#3.-Loss-function-and-optimization-algorithm" title="Permalink to this headline"></a></h3>
<p>The next step is to define the loss function and pick an optimization algorithm. Both PyTorch and Apache MXNet provide multiple options to chose from, and for our particular case we are going to use the cross-entropy loss function and the Stochastic Gradient Descent (SGD) optimization algorithm.</p>
<p><strong>PyTorch:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">pt_loss_fn</span> <span class="o">=</span> <span class="n">pt_nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">pt_trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">pt_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>MXNet:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="n">mx_loss_fn</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
<span class="n">mx_trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">mx_net</span><span class="o">.</span><span class="n">collect_params</span><span class="p">(),</span>
                           <span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>The code difference between frameworks is small. The main difference is that in Apache MXNet we use <a class="reference external" href="/versions/1.6.0/api/python/docs/api/gluon/trainer.html">Trainer</a> class, which accepts optimization algorithm as an argument. We also use <a class="reference external" href="/versions/1.6.0/api/python/docs/api/gluon/block.html#mxnet.gluon.Block.collect_params">.collect_params()</a> method to get parameters of the network.</p>
</div>
<div class="section" id="4.-Training">
<h3>4. Training<a class="headerlink" href="#4.-Training" title="Permalink to this headline"></a></h3>
<p>Finally, we implement the training algorithm. Note that the results for each run may vary because the weights will get different initialization values and the data will be read in a different order due to shuffling.</p>
<p><strong>PyTorch:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="o">.</span><span class="mi">0</span>
    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">pt_train_data</span><span class="p">:</span>
        <span class="n">pt_trainer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">pt_loss_fn</span><span class="p">(</span><span class="n">pt_net</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">pt_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">%d</span><span class="s1">, avg loss </span><span class="si">%.4f</span><span class="s1">, time </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> <span class="n">total_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">pt_train_data</span><span class="p">),</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p><strong>MXNet:</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">autograd</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="o">.</span><span class="mi">0</span>
    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mx_train_data</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">record</span><span class="p">():</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mx_loss_fn</span><span class="p">(</span><span class="n">mx_net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">mx_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">asscalar</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">%d</span><span class="s1">, avg loss </span><span class="si">%.4f</span><span class="s1">, time </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> <span class="n">total_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mx_train_data</span><span class="p">),</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Some of the differences in Apache MXNet when compared to PyTorch are as follows:</p>
<ul class="simple">
<li><p>In Apache MXNet, you dont need to flatten the 4-D input into 2-D when feeding the data into forward pass.</p></li>
<li><p>In Apache MXNet, you need to perform the calculation within the <a class="reference external" href="/versions/1.6.0/api/python/docs/api/autograd/index.html?autograd%20record#mxnet.autograd.record">autograd.record()</a> scope so that it can be automatically differentiated in the backward pass.</p></li>
<li><p>It is not necessary to clear the gradient every time as with PyTorchs <code class="docutils literal notranslate"><span class="pre">trainer.zero_grad()</span></code> because by default the new gradient is written in, not accumulated.</p></li>
<li><p>You need to specify the update step size (usually batch size) when performing <a class="reference external" href="/versions/1.6.0/api/python/docs/api/gluon/trainer.html?#mxnet.gluon.Trainer.step">step()</a> on the trainer.</p></li>
<li><p>You need to call <a class="reference external" href="/versions/1.6.0/api/python/docs/api/ndarray/ndarray.html?#mxnet.ndarray.NDArray.asscalar">.asscalar()</a> to turn a multidimensional array into a scalar.</p></li>
<li><p>In this sample, Apache MXNet is twice as fast as PyTorch. Though you need to be cautious with such toy comparisons.</p></li>
</ul>
</div>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline"></a></h2>
<p>As we saw above, Apache MXNet Gluon API and PyTorch have many similarities. The main difference lies in terminology (Tensor vs.NDArray) and behavior of accumulating gradients: gradients are accumulated in PyTorch and overwritten in Apache MXNet. The rest of the code is very similar, and it is quite straightforward to move code from one framework to the other.</p>
</div>
<div class="section" id="Recommended-Next-Steps">
<h2>Recommended Next Steps<a class="headerlink" href="#Recommended-Next-Steps" title="Permalink to this headline"></a></h2>
<p>While Apache MXNet Gluon API is very similar to PyTorch, there are some extra functionality that can make your code even faster.</p>
<ul class="simple">
<li><p>Check out <a class="reference external" href="/versions/1.6.0/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html">Hybridize tutorial</a> to learn how to write imperative code which can be converted to symbolic one.</p></li>
<li><p>Also, check out how to extend Apache MXNet with your own <a class="reference external" href="/versions/1.6.0/api/python/docs/tutorials/packages/gluon/blocks/custom-layer.html?custom_layers">custom layers</a>.</p></li>
</ul>
</div>
<div class="section" id="Appendix">
<h2>Appendix<a class="headerlink" href="#Appendix" title="Permalink to this headline"></a></h2>
<p>Below you can find a detailed comparison of various PyTorch functions and their equivalent in Gluon API of Apache MXNet.</p>
<div class="section" id="Tensor-operation">
<h3>Tensor operation<a class="headerlink" href="#Tensor-operation" title="Permalink to this headline"></a></h3>
<p>Here is the list of function names in PyTorch Tensor that are different from Apache MXNet NDArray.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 32%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Element-wise inverse cosine</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.acos()</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.acos(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.arccos(x)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Batch Matrix product and accumulation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.addbmm(M,</span> <span class="pre">batch1,</span> <span class="pre">batch2)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.linalg_gemm(M,</span> <span class="pre">batch1,</span> <span class="pre">batch2)</span></code> Leading n-2 dim are reduced</p></td>
</tr>
<tr class="row-even"><td><p>Element-wise division of t1, t2, multiply v, and add t</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.addcdiv(t,</span> <span class="pre">v,</span> <span class="pre">t1,</span> <span class="pre">t2)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">+</span> <span class="pre">v*(t1/t2)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Matrix product and accumulation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.addmm(M,</span> <span class="pre">mat1,</span> <span class="pre">mat2)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.linalg_gemm(M,</span> <span class="pre">mat1,</span> <span class="pre">mat2)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Outer-product of two vector add a matrix</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">m.addr(vec1,</span> <span class="pre">vec2)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>Element-wise applies function</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.apply_(calllable)</span></code></p></td>
<td><p>Not available, but there is <code class="docutils literal notranslate"><span class="pre">nd.custom(x,</span> <span class="pre">'op')</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Element-wise inverse sine</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.asin()</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.asin(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.arcsin(x)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Element-wise inverse tangent</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.atan()</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.atan(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.arctan(x)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Tangent of two tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.atan2(y)</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.atan2(x,</span> <span class="pre">y)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>batch matrix product</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.bmm(y)</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.bmm(x,</span> <span class="pre">x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.linalg_gemm2(x,</span> <span class="pre">y)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Draws a sample from bernoulli distribution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.bernoulli()</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>Fills a tensor with number drawn from Cauchy distribution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.cauchy_()</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Splits a tensor in a given dim</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.chunk(num_of_chunk)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.split(x,</span> <span class="pre">num_outputs=num_of_chunk)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Limits the values of a tensor to between min and max</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.clamp(min,</span> <span class="pre">max)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.clip(x,</span> <span class="pre">min,</span> <span class="pre">max)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Returns a copy of the tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.clone()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.copy()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Cross product</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.cross(y)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Cumulative product along an axis</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.cumprod(1)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>Cumulative sum along an axis</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.cumsum(1)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Address of the first element</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.data_ptr()</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>Creates a diagonal tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.diag()</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Computes norm of a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.dist()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.norm(x)</span></code> Only calculate L2 norm</p></td>
</tr>
<tr class="row-odd"><td><p>Computes Gauss error function</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.erf()</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Broadcasts/Expands tensor to new shape</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.expand(3,4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.broadcast_to([3,</span> <span class="pre">4])</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fills a tensor with samples drawn from exponential distribution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.exponential_()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.random_exponential()</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Element-wise mod</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.fmod(3)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.module(x,</span> <span class="pre">3)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Fractional portion of a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.frac()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">-</span> <span class="pre">nd.trunc(x)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Gathers values along an axis specified by dim</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.gather(x,</span> <span class="pre">1,</span>&#160; <span class="pre">torch.LongTensor([[0,0],[1,0]]))</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.gather_nd(x,</span> <span class="pre">nd.array([[[0,0],[1,1]],[[0,0],[1,0]]]))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Solves least square &amp; least norm</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">B.gels(A)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-even"><td><p>Draws from geometirc distribution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.geometric_(p)</span></code></p></td>
<td><p>Not available</p></td>
</tr>
<tr class="row-odd"><td><p>Device context of a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">print(x)</span></code> will print which device x is on</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.context</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Repeats tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.repeat(4,2)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.tile(4,2)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Data type of a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.type()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.dtype</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Scatter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.zeros(2,</span> <span class="pre">4).scatter_(1,</span> <span class="pre">torch.LongTensor([[2],</span> <span class="pre">[3]]),</span> <span class="pre">1.23)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.scatter_nd(nd.array([1.23,1.23]),</span> <span class="pre">nd.array([[0,1],[2,3]]),</span> <span class="pre">(2,4))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Returns the shape of a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.size()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.shape</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Number of elements in a tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.numel()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.size</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Returns this tensor as a NumPy ndarray</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.numpy()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.asnumpy()</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Eigendecomposition for symmetric matrix</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">e,</span> <span class="pre">v</span> <span class="pre">=</span> <span class="pre">a.symeig()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">v,</span> <span class="pre">e</span> <span class="pre">=</span> <span class="pre">nd.linalg.syevd(a)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Transpose</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.t()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.T</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sample uniformly</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uniform_()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.sample_uniform()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Inserts a new dimesion</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.unsqueeze()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nd.expand_dims(x)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Reshape</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.view(16)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.reshape((16,))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Veiw as a specified tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.view_as(y)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.reshape_like(y)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Returns a copy of the tensor after casting to a specified type</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.type(type)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.astype(dtype)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Copies the value of one tensor to another</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dst.copy_(src)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">src.copyto(dst)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Returns a zero tensor with specified shape</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">torch.zeros(2,3)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">nd.zeros((2,3))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Returns a one tensor with specified shape</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">torch.ones(2,3)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">nd.ones((2,3)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Returns a Tensor filled with the scalar value 1, with the same size as input</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">torch.ones_like(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">nd.ones_like(x)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Functional">
<h3>Functional<a class="headerlink" href="#Functional" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="GPU">
<h3>GPU<a class="headerlink" href="#GPU" title="Permalink to this headline"></a></h3>
<p>Just like Tensor, MXNet NDArray can be copied to and operated on GPU. This is done by specifying context.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Copy to GPU</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">torch.FloatTensor(1).cuda()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((1,),</span> <span class="pre">ctx=mx.gpu(0))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Convert to numpy array</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">y.cpu().numpy()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">y.asnumpy()</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Context scope</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.cuda.device(1):</span></code><code class="docutils literal notranslate"><span class="pre">y=</span> <span class="pre">torch.cuda.FloatTensor(1)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">mx.gpu(1):</span></code><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((3,5))</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Cross-device">
<h3>Cross-device<a class="headerlink" href="#Cross-device" title="Permalink to this headline"></a></h3>
<p>Just like Tensor, MXNet NDArray can be copied across multiple GPUs.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Copy from GPU 0 to GPU 1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">torch.cuda.FloatTensor(1)</span></code><code class="docutils literal notranslate"><span class="pre">y=x.cuda(1)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((1,),</span> <span class="pre">ctx=mx.gpu(0))</span></code><code class="docutils literal notranslate"><span class="pre">y=x.as_in_context(mx.gpu(1))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Copy Tensor/NDArray on different GPUs</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y.copy_(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x.copyto(y)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="Autograd">
<h2>Autograd<a class="headerlink" href="#Autograd" title="Permalink to this headline"></a></h2>
<div class="section" id="Variable-wrapper-vs-autograd-scope">
<h3>Variable wrapper vs autograd scope<a class="headerlink" href="#Variable-wrapper-vs-autograd-scope" title="Permalink to this headline"></a></h3>
<p>Autograd package of PyTorch/MXNet enables automatic differentiation of Tensor/NDArray.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Recording computation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">Variable(torch.FloatTensor(1),</span> <span class="pre">requires_grad=True)</span></code><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">*</span> <span class="pre">2</span></code><code class="docutils literal notranslate"><span class="pre">y.backward()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((1,))</span></code><code class="docutils literal notranslate"><span class="pre">x.attach_grad()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">mx.autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">*</span> <span class="pre">2</span></code><code class="docutils literal notranslate"><span class="pre">y.backward()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Scope-override-(pause,-train_mode,-predict_mode)">
<h3>Scope override (pause, train_mode, predict_mode)<a class="headerlink" href="#Scope-override-(pause,-train_mode,-predict_mode)" title="Permalink to this headline"></a></h3>
<p>Some operators (Dropout, BatchNorm, etc) behave differently in training and making predictions. This can be controlled with <code class="docutils literal notranslate"><span class="pre">train_mode</span></code> and <code class="docutils literal notranslate"><span class="pre">predict_mode</span></code> scope in MXNet. Pause scope is for code that does not need gradients to be calculated.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Scope override</p></td>
<td><p>Not available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((1,))</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.train_mode():</span></code><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mx.nd.Dropout(x)</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.predict_mode():</span></code><code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">mx.nd.Dropout(y)</span></code><code class="docutils literal notranslate"><span class="pre">w</span> <span class="pre">=</span> <span class="pre">mx.nd.ones((1,))</span></code><code class="docutils literal notranslate"><span class="pre">w.attach_grad()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">*</span> <span class="pre">w</span></code><code class="docutils literal notranslate"><span class="pre">y.backward()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.pause():</span></code><code class="docutils literal notranslate"><span class="pre">w</span> <span class="pre">+=</span> <span class="pre">w.grad</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Batch-end-synchronization-is-needed">
<h3>Batch-end synchronization is needed<a class="headerlink" href="#Batch-end-synchronization-is-needed" title="Permalink to this headline"></a></h3>
<p>Apache MXNet uses lazy evaluation to achieve superior performance. The Python thread just pushes the operations into the backend engine and then returns. In training phase batch-end synchronization is needed, e.g, <code class="docutils literal notranslate"><span class="pre">asnumpy()</span></code>, <code class="docutils literal notranslate"><span class="pre">wait_to_read()</span></code>, <code class="docutils literal notranslate"><span class="pre">metric.update(...)</span></code>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batch-end synchronization</p></td>
<td><p>Not available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">(data,</span> <span class="pre">label)</span> <span class="pre">in</span> <span class="pre">train_data:</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">net(data)</span></code><code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">=</span> <span class="pre">loss(output,</span> <span class="pre">label)</span></code><code class="docutils literal notranslate"><span class="pre">L.backward()</span></code><code class="docutils literal notranslate"><span class="pre">trainer.step(data.shape[0])</span></code><code class="docutils literal notranslate"><span class="pre">metric.update([label],</span> <span class="pre">[output])</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="PyTorch-module-and-Gluon-blocks">
<h2>PyTorch module and Gluon blocks<a class="headerlink" href="#PyTorch-module-and-Gluon-blocks" title="Permalink to this headline"></a></h2>
<div class="section" id="For-new-block-definition,-gluon-needs-name_scope">
<h3>For new block definition, gluon needs name_scope<a class="headerlink" href="#For-new-block-definition,-gluon-needs-name_scope" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">name_scope</span></code> coerces Gluon to give each parameter an appropriate name, indicating which model it belongs to.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>New block definition</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">Net(torch.nn.Module):</span></code><code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">D_in,</span> <span class="pre">D_out):</span></code><code class="docutils literal notranslate"><span class="pre">super(Net,</span> <span class="pre">self).__init__</span>
<span class="pre">()</span></code><code class="docutils literal notranslate"><span class="pre">self.linear</span> <span class="pre">=</span> <span class="pre">torch.nn.Linear(D_in,</span> <span class="pre">D_out)</span></code><code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">x):</span></code><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">self.linear(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cla</span>
<span class="pre">ss</span> <span class="pre">Net(mx.gluon.Block):</span></code><code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">__init__(self,</span> <span class="pre">D_in,</span> <span class="pre">D_out):</span></code><code class="docutils literal notranslate"><span class="pre">super(Net,</span> <span class="pre">self).__init__()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">self.name_scope():</span></code><code class="docutils literal notranslate"><span class="pre">self.dense=mx.gluon.nn.Dense(D_out,</span> <span class="pre">in_units=D_in)</span></code><code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">forward(self,</span> <span class="pre">x):</span></code><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">self.dense(x)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Parameter-and-Initializer">
<h3>Parameter and Initializer<a class="headerlink" href="#Parameter-and-Initializer" title="Permalink to this headline"></a></h3>
<p>When creating new layers in PyTorch, you do not need to specify its parameter initializer, and different layers have different default initializer. When you create new layers in Gluon API, you can specify its initializer or just leave it none. The parameters will finish initializing after calling <code class="docutils literal notranslate"><span class="pre">net.initialize(&lt;init</span> <span class="pre">method&gt;)</span></code> and all parameters will be initialized in <code class="docutils literal notranslate"><span class="pre">init</span> <span class="pre">method</span></code> except those layers whose initializer specified.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 29%" />
<col style="width: 35%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Get all parameters</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">net.parameters()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">net.collect_params()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Initialize network</p></td>
<td><p>Not Available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">net.initialize(mx.init.Xavier())</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Specify layer initializer</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">=</span> <span class="pre">torch.nn.Linear(20,</span> <span class="pre">10)</span></code> <code class="docutils literal notranslate"><span class="pre">torch.nn.init.normal(layer.weight,</span> <span class="pre">0,</span> <span class="pre">0.01)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">=</span> <span class="pre">mx.gluon.nn.Dense(10,</span> <span class="pre">weight_initializer=mx.init.Normal(0.01))</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Usage-of-existing-blocks-look-alike">
<h3>Usage of existing blocks look alike<a class="headerlink" href="#Usage-of-existing-blocks-look-alike" title="Permalink to this headline"></a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Usage of existing blocks</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y=net(x)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y=net(x)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="HybridBlock-can-be-hybridized,-and-allows-partial-shape-info">
<h3>HybridBlock can be hybridized, and allows partial-shape info<a class="headerlink" href="#HybridBlock-can-be-hybridized,-and-allows-partial-shape-info" title="Permalink to this headline"></a></h3>
<p>HybridBlock supports forwarding with both Symbol and NDArray. After hybridized, HybridBlock will create a symbolic graph representing the forward computation and cache it. Most of the built-in blocks (Dense, Conv2D, MaxPool2D, BatchNorm, etc.) are HybridBlocks.</p>
<p>Instead of explicitly declaring the number of inputs to a layer, we can simply state the number of outputs. The shape will be inferred on the fly once the network is provided with some input.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>partial-shape hybridized</p></td>
<td><p>Not Available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">net</span> <span class="pre">=</span> <span class="pre">mx.gluon.nn.HybridSequential()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">net.name_scope():</span></code><code class="docutils literal notranslate"><span class="pre">net.add(mx.gluon.nn.Dense(10))</span></code><code class="docutils literal notranslate"><span class="pre">net.hybridize()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="SymbolBlock">
<h3>SymbolBlock<a class="headerlink" href="#SymbolBlock" title="Permalink to this headline"></a></h3>
<p>SymbolBlock can construct block from symbol. This is useful for using pre-trained models as feature extractors.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SymbolBlock</p></td>
<td><p>Not Available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">alexnet</span> <span class="pre">=</span> <span class="pre">mx.gluon</span>
<span class="pre">.model_zoo.vision.alexnet(pretrained=True,</span> <span class="pre">prefix='model_')</span></code><code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">alexnet(inputs)</span></code><code class="docutils literal notranslate"><span class="pre">internals</span> <span class="pre">=</span> <span class="pre">out.get_internals()</span></code><code class="docutils literal notranslate"><span class="pre">outputs</span> <span class="pre">=</span> <span class="pre">[internals['model_dense0_relu_fwd_output']]</span></code><code class="docutils literal notranslate"><span class="pre">feat_model</span> <span class="pre">=</span> <span class="pre">gluon.SymbolBlock(outputs,</span> <span class="pre">inputs,</span> <span class="pre">params=alexnet.collect_params())</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="PyTorch-optimizer-vs-Gluon-Trainer">
<h2>PyTorch optimizer vs Gluon Trainer<a class="headerlink" href="#PyTorch-optimizer-vs-Gluon-Trainer" title="Permalink to this headline"></a></h2>
<div class="section" id="For-Gluon-API-calling-zero_grad-is-not-necessary-most-of-the-time">
<h3>For Gluon API calling zero_grad is not necessary most of the time<a class="headerlink" href="#For-Gluon-API-calling-zero_grad-is-not-necessary-most-of-the-time" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">zero_grad</span></code> in optimizer (PyTorch) or Trainer (Gluon API) clears the gradients of all parameters. In Gluon API, there is no need to clear the gradients every batch if <code class="docutils literal notranslate"><span class="pre">grad_req</span> <span class="pre">=</span> <span class="pre">'write'</span></code>(default).</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>clear the gradients</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optm</span> <span class="pre">=</span> <span class="pre">torch.optim.SGD(model.parameters(),</span> <span class="pre">lr=0.1)</span></code><code class="docutils literal notranslate"><span class="pre">optm.zero_grad()</span></code><code class="docutils literal notranslate"><span class="pre">loss_fn(model(input),</span> <span class="pre">target).backward()</span></code><code class="docutils literal notranslate"><span class="pre">optm.step()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">tr</span>
<span class="pre">ainer</span> <span class="pre">=</span> <span class="pre">gluon.Trainer(net.collect_params(),</span> <span class="pre">'sgd',</span> <span class="pre">{'learning_rate':</span> <span class="pre">0.1})</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">loss_fn(net(data),</span> <span class="pre">label)</span></code><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code><code class="docutils literal notranslate"><span class="pre">trainer.step(batch_size)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Multi-GPU-training">
<h3>Multi-GPU training<a class="headerlink" href="#Multi-GPU-training" title="Permalink to this headline"></a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>data parallelism</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">net</span> <span class="pre">=</span> <span class="pre">torch.nn.DataParallel(model,</span> <span class="pre">device_ids=[0,</span> <span class="pre">1,</span> <span class="pre">2])</span></code><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">net(data)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ctx</span> <span class="pre">=</span> <span class="pre">[mx.gpu(i)</span> <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(3)]</span></code><code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">=</span> <span class="pre">gluon.utils.split_and_load(data</span>
<span class="pre">,</span> <span class="pre">ctx)</span></code><code class="docutils literal notranslate"><span class="pre">label</span> <span class="pre">=</span> <span class="pre">gluon.utils.split_and_load(label,</span> <span class="pre">ctx)</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">losses</span> <span class="pre">=</span> <span class="pre">[loss(net(X),</span> <span class="pre">Y)</span> <span class="pre">for</span> <span class="pre">X,</span> <span class="pre">Y</span> <span class="pre">in</span> <span class="pre">zip(data,</span> <span class="pre">label)]</span></code><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">l</span> <span class="pre">in</span> <span class="pre">losses:</span></code><code class="docutils literal notranslate"><span class="pre">l.backward()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Distributed-training">
<h3>Distributed training<a class="headerlink" href="#Distributed-training" title="Permalink to this headline"></a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Pytorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>distributed data parallelism</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.distributed.init_process_group(...)</span></code><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">torch.nn.parallel.distributedDataParallel(model,</span> <span class="pre">...)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">store</span> <span class="pre">=</span> <span class="pre">kv.create('dist')</span></code><code class="docutils literal notranslate"><span class="pre">trainer</span> <span class="pre">=</span> <span class="pre">gluon.Trainer(net.collect_params(),</span> <span class="pre">...,</span> <span class="pre">kvstore=store)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="Monitoring">
<h2>Monitoring<a class="headerlink" href="#Monitoring" title="Permalink to this headline"></a></h2>
<div class="section" id="Apache-MXNet-has-pre-defined-metrics">
<h3>Apache MXNet has pre-defined metrics<a class="headerlink" href="#Apache-MXNet-has-pre-defined-metrics" title="Permalink to this headline"></a></h3>
<p>Gluon provide several predefined metrics which can online evaluate the performance of a learned model.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>metric</p></td>
<td><p>Not available</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">metric</span> <span class="pre">=</span> <span class="pre">mx.metric.Accura</span>
<span class="pre">cy()</span></code><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">autograd.record():</span></code><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">net(data)</span></code><code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">=</span> <span class="pre">loss(ouput,</span> <span class="pre">label)</span></code><code class="docutils literal notranslate"><span class="pre">loss(ouput,</span> <span class="pre">label).backward()</span></code><code class="docutils literal notranslate"><span class="pre">trainer.step(batch_size)</span></code><code class="docutils literal notranslate"><span class="pre">metric.update(label,</span> <span class="pre">output)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Data-visualization">
<h3>Data visualization<a class="headerlink" href="#Data-visualization" title="Permalink to this headline"></a></h3>
<p>TensorboardX (PyTorch) and <a class="reference external" href="https://github.com/awslabs/mxboard">MXBoard</a> (MXNet) can be used to visualize your network and plot quantitative metrics about the execution of your graph.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sw</span> <span class="pre">=</span> <span class="pre">tensorboardX.SummaryWriter()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sw</span> <span class="pre">=</span> <span class="pre">mxboard.SummaryWriter()</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">...</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">...</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">name,</span> <span class="pre">param</span> <span class="pre">in</span> <span class="pre">model.named_parameters():</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">name,</span> <span class="pre">param</span> <span class="pre">in</span> <span class="pre">net.collect_params():</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">=</span> <span class="pre">param.clone().cpu().data.numpy()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">=</span> <span class="pre">param.grad.asnumpy().flatten()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sw.add_histogram(name,</span> <span class="pre">grad,</span> <span class="pre">n_iter)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sw.add_histogram(tag=str(param),</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">...</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">values=grad,</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sw.close()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">bins=200,</span></code></p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">global_step=i)</span></code></p></td>
</tr>
<tr class="row-even"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">...</span></code></p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sw.close()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="I/O-and-deploy">
<h2>I/O and deploy<a class="headerlink" href="#I/O-and-deploy" title="Permalink to this headline"></a></h2>
<div class="section" id="Data-loading">
<h3>Data loading<a class="headerlink" href="#Data-loading" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> are the basic components for loading data.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dataset holding arrays</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.TensorDataset(data_tensor,</span> <span class="pre">label_tensor)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gluon.data.ArrayDataset(data_array,</span> <span class="pre">label_array)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Data loader</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">to</span>
<span class="pre">rch.utils.data.DataLoader(dataset,</span> <span class="pre">batch_size=1,</span> <span class="pre">shuffle=False,</span> <span class="pre">sampler=None,</span> <span class="pre">batch_sampler=None,</span> <span class="pre">num_workers=0,</span> <span class="pre">collate_fn=&lt;function</span> <span class="pre">default_collate&gt;,</span> <span class="pre">drop_last=False)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gluon.data.DataLoader(dataset,</span> <span class="pre">batch_size=None,</span> <span class="pre">shuffle=False,</span> <span class="pre">sampler=None,</span> <span class="pre">last_batch='keep',</span> <span class="pre">batch_sampler=None,</span> <span class="pre">batchify_fn=None,</span> <span class="pre">num_workers=0)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sequentially applied sampler</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.sampler.SequentialSampler(data_source)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gluon.data.SequentialSampler(length)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Random order sampler</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.sampler.RandomSampler(data_source)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">gluon.data.RandomSampler(length)</span></code></p></td>
</tr>
</tbody>
</table>
<p>Some commonly used datasets for computer vision are provided in <code class="docutils literal notranslate"><span class="pre">mx.gluon.data.vision</span></code> package.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MNIST handwritten digits dataset.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.MNIST</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mx.gluon.data.vision.MNIST</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>CIFAR10 Dataset.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.CIFAR10</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mx.gluon.data.vision.CIFAR10</span></code></p></td>
</tr>
<tr class="row-even"><td><p>CIFAR100 Dataset.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.CIFAR100</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mx.gluon.data.vision.CIFAR100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>A generic data loader where the images are arranged in folders.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder(root,</span> <span class="pre">transform=None,</span> <span class="pre">target_transform=None,</span> <span class="pre">loader=&lt;function</span> <span class="pre">default_loader&gt;)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mx.gluon.data.vision.ImageFolderDataset(root,</span> <span class="pre">flag,</span> <span class="pre">transform=None)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Serialization">
<h3>Serialization<a class="headerlink" href="#Serialization" title="Permalink to this headline"></a></h3>
<p>Serialization and deserialization are achieved by calling <code class="docutils literal notranslate"><span class="pre">save_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">load_parameters</span></code>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 35%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MXNet Gluon</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Save model parameters</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.save(the_model.state_dict(),</span> <span class="pre">filename)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.save_parameters(filename)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Load parameters</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">the_model.load_state_dict(torch.load(PATH))</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.load_parameters(filename,</span> <span class="pre">ctx,</span> <span class="pre">allow_missing=False,</span> <span class="pre">ignore_extra=False)</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">PyTorch vs Apache MXNet</a><ul>
<li><a class="reference internal" href="#Installation">Installation</a></li>
<li><a class="reference internal" href="#Data-manipulation">Data manipulation</a></li>
<li><a class="reference internal" href="#Model-training">Model training</a><ul>
<li><a class="reference internal" href="#1.-Read-data">1. Read data</a></li>
<li><a class="reference internal" href="#2.-Creating-the-model">2. Creating the model</a></li>
<li><a class="reference internal" href="#3.-Loss-function-and-optimization-algorithm">3. Loss function and optimization algorithm</a></li>
<li><a class="reference internal" href="#4.-Training">4. Training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#Recommended-Next-Steps">Recommended Next Steps</a></li>
<li><a class="reference internal" href="#Appendix">Appendix</a><ul>
<li><a class="reference internal" href="#Tensor-operation">Tensor operation</a></li>
<li><a class="reference internal" href="#Functional">Functional</a></li>
<li><a class="reference internal" href="#GPU">GPU</a></li>
<li><a class="reference internal" href="#Cross-device">Cross-device</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Autograd">Autograd</a><ul>
<li><a class="reference internal" href="#Variable-wrapper-vs-autograd-scope">Variable wrapper vs autograd scope</a></li>
<li><a class="reference internal" href="#Scope-override-(pause,-train_mode,-predict_mode)">Scope override (pause, train_mode, predict_mode)</a></li>
<li><a class="reference internal" href="#Batch-end-synchronization-is-needed">Batch-end synchronization is needed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#PyTorch-module-and-Gluon-blocks">PyTorch module and Gluon blocks</a><ul>
<li><a class="reference internal" href="#For-new-block-definition,-gluon-needs-name_scope">For new block definition, gluon needs name_scope</a></li>
<li><a class="reference internal" href="#Parameter-and-Initializer">Parameter and Initializer</a></li>
<li><a class="reference internal" href="#Usage-of-existing-blocks-look-alike">Usage of existing blocks look alike</a></li>
<li><a class="reference internal" href="#HybridBlock-can-be-hybridized,-and-allows-partial-shape-info">HybridBlock can be hybridized, and allows partial-shape info</a></li>
<li><a class="reference internal" href="#SymbolBlock">SymbolBlock</a></li>
</ul>
</li>
<li><a class="reference internal" href="#PyTorch-optimizer-vs-Gluon-Trainer">PyTorch optimizer vs Gluon Trainer</a><ul>
<li><a class="reference internal" href="#For-Gluon-API-calling-zero_grad-is-not-necessary-most-of-the-time">For Gluon API calling zero_grad is not necessary most of the time</a></li>
<li><a class="reference internal" href="#Multi-GPU-training">Multi-GPU training</a></li>
<li><a class="reference internal" href="#Distributed-training">Distributed training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Monitoring">Monitoring</a><ul>
<li><a class="reference internal" href="#Apache-MXNet-has-pre-defined-metrics">Apache MXNet has pre-defined metrics</a></li>
<li><a class="reference internal" href="#Data-visualization">Data visualization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#I/O-and-deploy">I/O and deploy</a><ul>
<li><a class="reference internal" href="#Data-loading">Data loading</a></li>
<li><a class="reference internal" href="#Serialization">Serialization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>                    

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>Moving to MXNet from Other Frameworks</div>
         </div>
     </a>
     <a id="button-next" href="../gluon_from_experiment_to_deployment.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>Gluon: from experiment to deployment</div>
        </div>
     </a>
  </div>
            <footer class="site-footer h-card">
    <div class="wrapper">
        <div class="row">
            <div class="col-4">
                <h4 class="footer-category-title">Resources</h4>
                <ul class="contact-list">
                    <li><a class="u-email" href="mailto:dev@mxnet.apache.org">Dev list</a></li>
                    <li><a class="u-email" href="mailto:user@mxnet.apache.org">User mailing list</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="https://issues.apache.org/jira/projects/MXNET/issues">Jira Tracker</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/labels/Roadmap">Github Roadmap</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>
                    <li><a href="/versions/1.6.0/community/contribute">Contribute To MXNet</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>

<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="../../../_static/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright  2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>
        
  </body>
</html>