<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <style>
    .dropdown {
        position: relative;
        display: inline-block;
    }

    .dropdown-content {
        display: none;
        position: absolute;
        background-color: #f9f9f9;
        min-width: 160px;
        box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
        padding: 12px 16px;
        z-index: 1;
        text-align: left;
    }

    .dropdown:hover .dropdown-content {
        display: block;
    }

    .dropdown-option:hover {
        color: #FF4500;
    }

    .dropdown-option-active {
        color: #FF4500;
        font-weight: lighter;
    }

    .dropdown-option {
        color: #000000;
        font-weight: lighter;
    }

    .dropdown-header {
        color: #FFFFFF;
        display: inline-flex;
    }

    .dropdown-caret {
        width: 18px;
        height: 54px;
    }

    .dropdown-caret-path {
        fill: #FFFFFF;
    }
    </style>
    
    <title>gluon.rnn &#8212; Apache MXNet  documentation</title>

    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mxnet.css" />
    <link rel="stylesheet" href="../../../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/feedback.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/google_analytics.js"></script>
    <script src="../../../_static/autodoc.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../../_static/sphinx_materialdesign_theme.js"></script>
    <link rel="shortcut icon" href="../../../_static/mxnet-icon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="gluon.utils" href="../utils/index.html" />
    <link rel="prev" title="gluon.nn" href="../nn/index.html" /> 
  </head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
      <a class="site-title" rel="author" href="/"><img
            src="../../../_static/mxnet_logo.png" class="site-header-logo"></a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger"/>
      <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/get_started">Get Started</a>
        <a class="page-link" href="/blog">Blog</a>
        <a class="page-link" href="/features">Features</a>
        <a class="page-link" href="/ecosystem">Ecosystem</a>
        <a class="page-link page-current" href="/api">Docs & Tutorials</a>
        <a class="page-link" href="https://github.com/apache/incubator-mxnet">GitHub</a>
        <div class="dropdown">
          <span class="dropdown-header">master
            <svg class="dropdown-caret" viewBox="0 0 32 32" class="icon icon-caret-bottom" aria-hidden="true"><path class="dropdown-caret-path" d="M24 11.305l-7.997 11.39L8 11.305z"></path></svg>
          </span>
          <div class="dropdown-content">
            <a class="dropdown-option-active" href="/">master</a><br>
            <a class="dropdown-option" href="/versions/1.7/">1.7</a><br>
            <a class="dropdown-option" href="/versions/1.6/">1.6</a><br>
            <a class="dropdown-option" href="/versions/1.5.0/">1.5.0</a><br>
            <a class="dropdown-option" href="/versions/1.4.1/">1.4.1</a><br>
            <a class="dropdown-option" href="/versions/1.3.1/">1.3.1</a><br>
            <a class="dropdown-option" href="/versions/1.2.1/">1.2.1</a><br>
            <a class="dropdown-option" href="/versions/1.1.0/">1.1.0</a><br>
            <a class="dropdown-option" href="/versions/1.0.0/">1.0.0</a><br>
            <a class="dropdown-option" href="/versions/0.12.1/">0.12.1</a><br>
            <a class="dropdown-option" href="/versions/0.11.0/">0.11.0</a>
          </div>
        </div>
      </div>
    </nav>
  </div>
</header>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../index.html">Python API</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link" href="../index.html">mxnet.gluon</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">gluon.rnn</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-github"
    href="https://github.com/apache/mxnet/edit/master/docs/python_docs/python/api/gluon/rnn/index.rst" class="mdl-button mdl-js-button mdl-button--icon">
<i class="material-icons">edit</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-github">
Edit on Github
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/index.html">Getting started with NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/1-ndarray.html">Step 1: Manipulate data with NP on MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/2-nn.html">Step 2: Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/3-autograd.html">Step 3: Automatic differentiation with autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/4-train.html">Step 4: Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/5-predict.html">Step 5: Predict with a pretrained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/6-use_gpus.html">Step 6: Use GPUs to increase efficiency</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/data/index.html">Data Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Spatial-Augmentation">Spatial Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Color-Augmentation">Color Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Composed-Augmentations">Composed Augmentations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html">Gluon <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-custom-Datasets">Using own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader">Appendix: Upgrading from Module <code class="docutils literal notranslate"><span class="pre">DataIter</span></code> to Gluon <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/info_gan.html">Image similarity search with InfoGAN</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/legacy/index.html">Legacy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/np/index.html">What is NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/np/cheat-sheet.html">The NP on MXNet cheat sheet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/np/np-vs-numpy.html">Differences between NP on MXNet and NumPy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/index.html">TensorRT</a><ul class="simple">
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/image_classification_jetson.html">Image Classication using pretrained ResNet-50 model on Jetson module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/using_rtc">Using RTC for CUDA kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../np/index.html">mxnet.np</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../np/arrays.html">Array objects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../np/arrays.ndarray.html">The N-dimensional array (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.html">mxnet.np.ndarray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.shape.html">mxnet.np.ndarray.shape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.ndim.html">mxnet.np.ndarray.ndim</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.size.html">mxnet.np.ndarray.size</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.dtype.html">mxnet.np.ndarray.dtype</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.item.html">mxnet.np.ndarray.item</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.copy.html">mxnet.np.ndarray.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.tolist.html">mxnet.np.ndarray.tolist</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.astype.html">mxnet.np.ndarray.astype</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.reshape.html">mxnet.np.ndarray.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.transpose.html">mxnet.np.ndarray.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.swapaxes.html">mxnet.np.ndarray.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.squeeze.html">mxnet.np.ndarray.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.nonzero.html">mxnet.np.ndarray.nonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.take.html">mxnet.np.ndarray.take</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.repeat.html">mxnet.np.ndarray.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.max.html">mxnet.np.ndarray.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.argmax.html">mxnet.np.ndarray.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.min.html">mxnet.np.ndarray.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.argmin.html">mxnet.np.ndarray.argmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.clip.html">mxnet.np.ndarray.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.sum.html">mxnet.np.ndarray.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.mean.html">mxnet.np.ndarray.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.prod.html">mxnet.np.ndarray.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.cumsum.html">mxnet.np.ndarray.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.var.html">mxnet.np.ndarray.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.std.html">mxnet.np.ndarray.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__lt__.html">mxnet.np.ndarray.__lt__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__le__.html">mxnet.np.ndarray.__le__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__gt__.html">mxnet.np.ndarray.__gt__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__ge__.html">mxnet.np.ndarray.__ge__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__eq__.html">mxnet.np.ndarray.__eq__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__ne__.html">mxnet.np.ndarray.__ne__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__bool__.html">mxnet.np.ndarray.__bool__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__neg__.html">mxnet.np.ndarray.__neg__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__add__.html">mxnet.np.ndarray.__add__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__sub__.html">mxnet.np.ndarray.__sub__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__mul__.html">mxnet.np.ndarray.__mul__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__truediv__.html">mxnet.np.ndarray.__truediv__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__mod__.html">mxnet.np.ndarray.__mod__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__pow__.html">mxnet.np.ndarray.__pow__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__iadd__.html">mxnet.np.ndarray.__iadd__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__isub__.html">mxnet.np.ndarray.__isub__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__imul__.html">mxnet.np.ndarray.__imul__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__itruediv__.html">mxnet.np.ndarray.__itruediv__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__imod__.html">mxnet.np.ndarray.__imod__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__reduce__.html">mxnet.np.ndarray.__reduce__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__setstate__.html">mxnet.np.ndarray.__setstate__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__len__.html">mxnet.np.ndarray.__len__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__getitem__.html">mxnet.np.ndarray.__getitem__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__setitem__.html">mxnet.np.ndarray.__setitem__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__int__.html">mxnet.np.ndarray.__int__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__float__.html">mxnet.np.ndarray.__float__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__str__.html">mxnet.np.ndarray.__str__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__repr__.html">mxnet.np.ndarray.__repr__</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/arrays.indexing.html">Indexing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../np/routines.html">Routines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.array-creation.html">Array creation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.eye.html">mxnet.np.eye</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.empty.html">mxnet.np.empty</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.full.html">mxnet.np.full</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.identity.html">mxnet.np.identity</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ones.html">mxnet.np.ones</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ones_like.html">mxnet.np.ones_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.zeros.html">mxnet.np.zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.zeros_like.html">mxnet.np.zeros_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.array.html">mxnet.np.array</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.copy.html">mxnet.np.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arange.html">mxnet.np.arange</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linspace.html">mxnet.np.linspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.logspace.html">mxnet.np.logspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.meshgrid.html">mxnet.np.meshgrid</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tril.html">mxnet.np.tril</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.array-manipulation.html">Array manipulation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ravel.html">mxnet.np.ravel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.swapaxes.html">mxnet.np.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.transpose.html">mxnet.np.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.moveaxis.html">mxnet.np.moveaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.expand_dims.html">mxnet.np.expand_dims</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.squeeze.html">mxnet.np.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.broadcast_to.html">mxnet.np.broadcast_to</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.broadcast_arrays.html">mxnet.np.broadcast_arrays</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.concatenate.html">mxnet.np.concatenate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.stack.html">mxnet.np.stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.dstack.html">mxnet.np.dstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vstack.html">mxnet.np.vstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.split.html">mxnet.np.split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.hsplit.html">mxnet.np.hsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vsplit.html">mxnet.np.vsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tile.html">mxnet.np.tile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.repeat.html">mxnet.np.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.unique.html">mxnet.np.unique</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.flip.html">mxnet.np.flip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.roll.html">mxnet.np.roll</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rot90.html">mxnet.np.rot90</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.io.html">Input and output</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.genfromtxt.html">mxnet.np.genfromtxt</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.linalg.html">Linear algebra (<code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.linalg</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.dot.html">mxnet.np.dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vdot.html">mxnet.np.vdot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.inner.html">mxnet.np.inner</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.outer.html">mxnet.np.outer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tensordot.html">mxnet.np.tensordot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.einsum.html">mxnet.np.einsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linalg.svd.html">mxnet.np.linalg.svd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linalg.norm.html">mxnet.np.linalg.norm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.trace.html">mxnet.np.trace</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.math.html">Mathematical functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sin.html">mxnet.np.sin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cos.html">mxnet.np.cos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tan.html">mxnet.np.tan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arcsin.html">mxnet.np.arcsin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arccos.html">mxnet.np.arccos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctan.html">mxnet.np.arctan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.degrees.html">mxnet.np.degrees</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.radians.html">mxnet.np.radians</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.hypot.html">mxnet.np.hypot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctan2.html">mxnet.np.arctan2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.deg2rad.html">mxnet.np.deg2rad</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rad2deg.html">mxnet.np.rad2deg</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sinh.html">mxnet.np.sinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cosh.html">mxnet.np.cosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tanh.html">mxnet.np.tanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arcsinh.html">mxnet.np.arcsinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arccosh.html">mxnet.np.arccosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctanh.html">mxnet.np.arctanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rint.html">mxnet.np.rint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.fix.html">mxnet.np.fix</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.floor.html">mxnet.np.floor</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ceil.html">mxnet.np.ceil</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.trunc.html">mxnet.np.trunc</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.around.html">mxnet.np.around</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sum.html">mxnet.np.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.prod.html">mxnet.np.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cumsum.html">mxnet.np.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.exp.html">mxnet.np.exp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.expm1.html">mxnet.np.expm1</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log.html">mxnet.np.log</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log10.html">mxnet.np.log10</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log2.html">mxnet.np.log2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log1p.html">mxnet.np.log1p</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ldexp.html">mxnet.np.ldexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.lcm.html">mxnet.np.lcm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.add.html">mxnet.np.add</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reciprocal.html">mxnet.np.reciprocal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.negative.html">mxnet.np.negative</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.divide.html">mxnet.np.divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.power.html">mxnet.np.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.subtract.html">mxnet.np.subtract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.mod.html">mxnet.np.mod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.multiply.html">mxnet.np.multiply</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.true_divide.html">mxnet.np.true_divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.remainder.html">mxnet.np.remainder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.clip.html">mxnet.np.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sqrt.html">mxnet.np.sqrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cbrt.html">mxnet.np.cbrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.square.html">mxnet.np.square</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.absolute.html">mxnet.np.absolute</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sign.html">mxnet.np.sign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.maximum.html">mxnet.np.maximum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.minimum.html">mxnet.np.minimum</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/random/index.html">np.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.sort.html">Sorting, searching, and counting</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.argmax.html">mxnet.np.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.argmin.html">mxnet.np.argmin</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.statistics.html">Statistics</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.min.html">mxnet.np.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.max.html">mxnet.np.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.mean.html">mxnet.np.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.std.html">mxnet.np.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.var.html">mxnet.np.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.histogram.html">mxnet.np.histogram</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../npx/index.html">NPX: NumPy Neural Network Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.set_np.html">mxnet.npx.set_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.reset_np.html">mxnet.npx.reset_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.cpu.html">mxnet.npx.cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.cpu_pinned.html">mxnet.npx.cpu_pinned</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gpu.html">mxnet.npx.gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gpu_memory_info.html">mxnet.npx.gpu_memory_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.current_context.html">mxnet.npx.current_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.num_gpus.html">mxnet.npx.num_gpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.activation.html">mxnet.npx.activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_norm.html">mxnet.npx.batch_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.convolution.html">mxnet.npx.convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.dropout.html">mxnet.npx.dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.embedding.html">mxnet.npx.embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.fully_connected.html">mxnet.npx.fully_connected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.layer_norm.html">mxnet.npx.layer_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.pooling.html">mxnet.npx.pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.rnn.html">mxnet.npx.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.leaky_relu.html">mxnet.npx.leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_detection.html">mxnet.npx.multibox_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_prior.html">mxnet.npx.multibox_prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_target.html">mxnet.npx.multibox_target</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.roi_pooling.html">mxnet.npx.roi_pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.sigmoid.html">mxnet.npx.sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.smooth_l1.html">mxnet.npx.smooth_l1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.softmax.html">mxnet.npx.softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.topk.html">mxnet.npx.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.waitall.html">mxnet.npx.waitall</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.load.html">mxnet.npx.load</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.save.html">mxnet.npx.save</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.one_hot.html">mxnet.npx.one_hot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.pick.html">mxnet.npx.pick</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.reshape_like.html">mxnet.npx.reshape_like</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_flatten.html">mxnet.npx.batch_flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_dot.html">mxnet.npx.batch_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gamma.html">mxnet.npx.gamma</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.sequence_mask.html">mxnet.npx.sequence_mask</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">mxnet.gluon</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../metric/index.html">gluon.metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nn/index.html">gluon.nn</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../legacy/index.html">Legacy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/symbol.html">symbol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../context/index.html">mxnet.context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../runtime/index.html">mxnet.runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../util/index.html">mxnet.util</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">
<header class="mdl-layout__drawer">      
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/getting-started/index.html">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/index.html">Getting started with NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/1-ndarray.html">Step 1: Manipulate data with NP on MXNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/2-nn.html">Step 2: Create a neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/3-autograd.html">Step 3: Automatic differentiation with autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/4-train.html">Step 4: Train the neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/5-predict.html">Step 5: Predict with a pretrained model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/crash-course/6-use_gpus.html">Step 6: Use GPUs to increase efficiency</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/index.html">Moving to MXNet from Other Frameworks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/getting-started/to-mxnet/pytorch.html">PyTorch vs Apache MXNet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/gluon_from_experiment_to_deployment.html">Gluon: from experiment to deployment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/getting-started/logistic_regression_explained.html">Logistic regression explained</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html">MNIST</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/packages/index.html">Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/autograd/index.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/gluon/index.html">Gluon</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/index.html">Blocks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom-layer.html">Custom Layers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/custom_layer_beginners.html">Customer Layers (Beginners)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/hybridize.html">Hybridize</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/init.html">Initialization</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/naming.html">Parameter and Block Naming</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/nn.html">Layers and Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/parameters.html">Parameter Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/save_load_params.html">Saving and Loading Gluon Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/blocks/activations/activations.html">Activation Blocks</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/data/index.html">Data Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html">Image Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Spatial-Augmentation">Spatial Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Color-Augmentation">Color Augmentation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/data_augmentation.html#Composed-Augmentations">Composed Augmentations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html">Gluon <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-included-Datasets">Using own data with included <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Using-own-data-with-custom-Datasets">Using own data with custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader">Appendix: Upgrading from Module <code class="docutils literal notranslate"><span class="pre">DataIter</span></code> to Gluon <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/image/index.html">Image Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/info_gan.html">Image similarity search with InfoGAN</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/image/mnist.html">Handwritten Digit Recognition</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/index.html">Losses</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/custom-loss.html">Custom Loss Blocks</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/kl_divergence.html">Kullback-Leibler (KL) Divergence</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/loss/loss.html">Loss functions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/text/index.html">Text Tutorials</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/gnmt.html">Google Neural Machine Translation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/text/transformer.html">Machine Translation with Transformer</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/gluon/training/index.html">Training</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/fit_api_tutorial.html">MXNet Gluon Fit API</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/trainer.html">Trainer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/index.html">Learning Rates</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_finder.html">Learning Rate Finder</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules.html">Learning Rate Schedules</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/gluon/training/learning_rates/learning_rate_schedules_advanced.html">Advanced Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/gluon/training/normalization/index.html">Normalization Blocks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/kvstore/index.html">KVStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/kvstore/kvstore.html">Distributed Key-Value Store</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/legacy/index.html">Legacy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/index.html">NDArray</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/01-ndarray-intro.html">An Intro: Manipulate Data the MXNet Way with NDArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/02-ndarray-operations.html">NDArray Operations</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/03-ndarray-contexts.html">NDArray Contexts</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/gotchas_numpy_in_mxnet.html">Gotchas using NumPy in Apache MXNet</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/index.html">Tutorials</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/csr.html">CSRNDArray - NDArray in Compressed Sparse Row Storage Format</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/row_sparse.html">RowSparseNDArray - NDArray for Sparse Gradient Updates</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../tutorials/packages/legacy/ndarray/sparse/train_gluon.html">Sparse NDArrays with Gluon</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/np/index.html">What is NP on MXNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/np/cheat-sheet.html">The NP on MXNet cheat sheet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/np/np-vs-numpy.html">Differences between NP on MXNet and NumPy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/onnx/index.html">ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/fine_tuning_gluon.html">Fine-tuning an ONNX model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/packages/onnx/inference_on_onnx_model.html">Running inference on MXNet/Gluon from an ONNX model</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/deploy/export/onnx.html">Export ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/optimizer/index.html">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/packages/viz/index.html">Visualization</a><ul>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/visualize_graph">Visualize networks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/performance/index.html">Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/compression/index.html">Compression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/compression/int8.html">Deploy with int-8</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/float16">Float16</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/faq/gradient_compression">Gradient Compression</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html">GluonCV with Quantized Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/performance/backend/index.html">Accelerated Backend Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/index.html">Intel MKL-DNN</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../tutorials/performance/backend/mkldnn/mkldnn_readme.html">Install MXNet with MKL-DNN</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tensorrt/index.html">TensorRT</a><ul class="simple">
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/tvm.html">Use TVM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/profiler.html">Profiling MXNet Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/performance/backend/amp.html">Using AMP: Automatic Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/deploy/index.html">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/export/index.html">Export</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/export/onnx.html">Exporting to ONNX format</a></li>
<li class="toctree-l4"><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_deployment/export_network.html">Export Gluon CV Models</a></li>
<li class="toctree-l4"><a class="reference external" href="https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html">Save / Load Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/inference/index.html">Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/cpp.html">Deploy into C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/inference/image_classification_jetson.html">Image Classication using pretrained ResNet-50 model on Jetson module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/index.html">Run on AWS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_ec2.html">Run on an EC2 Instance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/use_sagemaker.html">Run on Amazon SageMaker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tutorials/deploy/run-on-aws/cloud.html">MXNet on the Cloud</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/extend/index.html">Extend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/extend/customop.html">Custom Numpy Operators</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/new_op">New Operator Creation</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/add_op_in_backend">New Operator in MXNet Backend</a></li>
<li class="toctree-l3"><a class="reference external" href="https://mxnet.apache.org/api/faq/using_rtc">Using RTC for CUDA kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Python API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../np/index.html">mxnet.np</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../np/arrays.html">Array objects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../np/arrays.ndarray.html">The N-dimensional array (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.html">mxnet.np.ndarray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.shape.html">mxnet.np.ndarray.shape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.ndim.html">mxnet.np.ndarray.ndim</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.size.html">mxnet.np.ndarray.size</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.dtype.html">mxnet.np.ndarray.dtype</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.item.html">mxnet.np.ndarray.item</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.copy.html">mxnet.np.ndarray.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.tolist.html">mxnet.np.ndarray.tolist</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.astype.html">mxnet.np.ndarray.astype</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.reshape.html">mxnet.np.ndarray.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.transpose.html">mxnet.np.ndarray.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.swapaxes.html">mxnet.np.ndarray.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.squeeze.html">mxnet.np.ndarray.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.nonzero.html">mxnet.np.ndarray.nonzero</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.take.html">mxnet.np.ndarray.take</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.repeat.html">mxnet.np.ndarray.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.max.html">mxnet.np.ndarray.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.argmax.html">mxnet.np.ndarray.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.min.html">mxnet.np.ndarray.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.argmin.html">mxnet.np.ndarray.argmin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.clip.html">mxnet.np.ndarray.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.sum.html">mxnet.np.ndarray.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.mean.html">mxnet.np.ndarray.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.prod.html">mxnet.np.ndarray.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.cumsum.html">mxnet.np.ndarray.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.var.html">mxnet.np.ndarray.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.std.html">mxnet.np.ndarray.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__lt__.html">mxnet.np.ndarray.__lt__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__le__.html">mxnet.np.ndarray.__le__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__gt__.html">mxnet.np.ndarray.__gt__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__ge__.html">mxnet.np.ndarray.__ge__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__eq__.html">mxnet.np.ndarray.__eq__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__ne__.html">mxnet.np.ndarray.__ne__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__bool__.html">mxnet.np.ndarray.__bool__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__neg__.html">mxnet.np.ndarray.__neg__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__add__.html">mxnet.np.ndarray.__add__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__sub__.html">mxnet.np.ndarray.__sub__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__mul__.html">mxnet.np.ndarray.__mul__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__truediv__.html">mxnet.np.ndarray.__truediv__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__mod__.html">mxnet.np.ndarray.__mod__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__pow__.html">mxnet.np.ndarray.__pow__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__iadd__.html">mxnet.np.ndarray.__iadd__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__isub__.html">mxnet.np.ndarray.__isub__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__imul__.html">mxnet.np.ndarray.__imul__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__itruediv__.html">mxnet.np.ndarray.__itruediv__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__imod__.html">mxnet.np.ndarray.__imod__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__reduce__.html">mxnet.np.ndarray.__reduce__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__setstate__.html">mxnet.np.ndarray.__setstate__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__len__.html">mxnet.np.ndarray.__len__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__getitem__.html">mxnet.np.ndarray.__getitem__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__setitem__.html">mxnet.np.ndarray.__setitem__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__int__.html">mxnet.np.ndarray.__int__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__float__.html">mxnet.np.ndarray.__float__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__str__.html">mxnet.np.ndarray.__str__</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.__repr__.html">mxnet.np.ndarray.__repr__</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/arrays.indexing.html">Indexing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../np/routines.html">Routines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.array-creation.html">Array creation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.eye.html">mxnet.np.eye</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.empty.html">mxnet.np.empty</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.full.html">mxnet.np.full</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.identity.html">mxnet.np.identity</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ones.html">mxnet.np.ones</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ones_like.html">mxnet.np.ones_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.zeros.html">mxnet.np.zeros</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.zeros_like.html">mxnet.np.zeros_like</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.array.html">mxnet.np.array</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.copy.html">mxnet.np.copy</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arange.html">mxnet.np.arange</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linspace.html">mxnet.np.linspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.logspace.html">mxnet.np.logspace</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.meshgrid.html">mxnet.np.meshgrid</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tril.html">mxnet.np.tril</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.array-manipulation.html">Array manipulation routines</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ravel.html">mxnet.np.ravel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.flatten.html">mxnet.np.ndarray.flatten</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.swapaxes.html">mxnet.np.swapaxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ndarray.T.html">mxnet.np.ndarray.T</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.transpose.html">mxnet.np.transpose</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.moveaxis.html">mxnet.np.moveaxis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.expand_dims.html">mxnet.np.expand_dims</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.squeeze.html">mxnet.np.squeeze</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.broadcast_to.html">mxnet.np.broadcast_to</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.broadcast_arrays.html">mxnet.np.broadcast_arrays</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.concatenate.html">mxnet.np.concatenate</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.stack.html">mxnet.np.stack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.dstack.html">mxnet.np.dstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vstack.html">mxnet.np.vstack</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.split.html">mxnet.np.split</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.hsplit.html">mxnet.np.hsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vsplit.html">mxnet.np.vsplit</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tile.html">mxnet.np.tile</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.repeat.html">mxnet.np.repeat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.unique.html">mxnet.np.unique</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reshape.html">mxnet.np.reshape</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.flip.html">mxnet.np.flip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.roll.html">mxnet.np.roll</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rot90.html">mxnet.np.rot90</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.io.html">Input and output</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.genfromtxt.html">mxnet.np.genfromtxt</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.linalg.html">Linear algebra (<code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy.linalg</span></code>)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.dot.html">mxnet.np.dot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.vdot.html">mxnet.np.vdot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.inner.html">mxnet.np.inner</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.outer.html">mxnet.np.outer</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tensordot.html">mxnet.np.tensordot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.einsum.html">mxnet.np.einsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linalg.svd.html">mxnet.np.linalg.svd</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.linalg.norm.html">mxnet.np.linalg.norm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.trace.html">mxnet.np.trace</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.math.html">Mathematical functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sin.html">mxnet.np.sin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cos.html">mxnet.np.cos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tan.html">mxnet.np.tan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arcsin.html">mxnet.np.arcsin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arccos.html">mxnet.np.arccos</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctan.html">mxnet.np.arctan</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.degrees.html">mxnet.np.degrees</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.radians.html">mxnet.np.radians</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.hypot.html">mxnet.np.hypot</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctan2.html">mxnet.np.arctan2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.deg2rad.html">mxnet.np.deg2rad</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rad2deg.html">mxnet.np.rad2deg</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sinh.html">mxnet.np.sinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cosh.html">mxnet.np.cosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.tanh.html">mxnet.np.tanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arcsinh.html">mxnet.np.arcsinh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arccosh.html">mxnet.np.arccosh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.arctanh.html">mxnet.np.arctanh</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.rint.html">mxnet.np.rint</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.fix.html">mxnet.np.fix</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.floor.html">mxnet.np.floor</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ceil.html">mxnet.np.ceil</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.trunc.html">mxnet.np.trunc</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.around.html">mxnet.np.around</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sum.html">mxnet.np.sum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.prod.html">mxnet.np.prod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cumsum.html">mxnet.np.cumsum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.exp.html">mxnet.np.exp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.expm1.html">mxnet.np.expm1</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log.html">mxnet.np.log</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log10.html">mxnet.np.log10</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log2.html">mxnet.np.log2</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.log1p.html">mxnet.np.log1p</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.ldexp.html">mxnet.np.ldexp</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.lcm.html">mxnet.np.lcm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.add.html">mxnet.np.add</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.reciprocal.html">mxnet.np.reciprocal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.negative.html">mxnet.np.negative</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.divide.html">mxnet.np.divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.power.html">mxnet.np.power</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.subtract.html">mxnet.np.subtract</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.mod.html">mxnet.np.mod</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.multiply.html">mxnet.np.multiply</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.true_divide.html">mxnet.np.true_divide</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.remainder.html">mxnet.np.remainder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.clip.html">mxnet.np.clip</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sqrt.html">mxnet.np.sqrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.cbrt.html">mxnet.np.cbrt</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.square.html">mxnet.np.square</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.absolute.html">mxnet.np.absolute</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.sign.html">mxnet.np.sign</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.maximum.html">mxnet.np.maximum</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.minimum.html">mxnet.np.minimum</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/random/index.html">np.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.sort.html">Sorting, searching, and counting</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.argmax.html">mxnet.np.argmax</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.argmin.html">mxnet.np.argmin</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../np/routines.statistics.html">Statistics</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.min.html">mxnet.np.min</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.max.html">mxnet.np.max</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.mean.html">mxnet.np.mean</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.std.html">mxnet.np.std</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.var.html">mxnet.np.var</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../np/generated/mxnet.np.histogram.html">mxnet.np.histogram</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../npx/index.html">NPX: NumPy Neural Network Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.set_np.html">mxnet.npx.set_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.reset_np.html">mxnet.npx.reset_np</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.cpu.html">mxnet.npx.cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.cpu_pinned.html">mxnet.npx.cpu_pinned</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gpu.html">mxnet.npx.gpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gpu_memory_info.html">mxnet.npx.gpu_memory_info</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.current_context.html">mxnet.npx.current_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.num_gpus.html">mxnet.npx.num_gpus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.activation.html">mxnet.npx.activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_norm.html">mxnet.npx.batch_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.convolution.html">mxnet.npx.convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.dropout.html">mxnet.npx.dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.embedding.html">mxnet.npx.embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.fully_connected.html">mxnet.npx.fully_connected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.layer_norm.html">mxnet.npx.layer_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.pooling.html">mxnet.npx.pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.rnn.html">mxnet.npx.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.leaky_relu.html">mxnet.npx.leaky_relu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_detection.html">mxnet.npx.multibox_detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_prior.html">mxnet.npx.multibox_prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.multibox_target.html">mxnet.npx.multibox_target</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.roi_pooling.html">mxnet.npx.roi_pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.sigmoid.html">mxnet.npx.sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.smooth_l1.html">mxnet.npx.smooth_l1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.softmax.html">mxnet.npx.softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.topk.html">mxnet.npx.topk</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.waitall.html">mxnet.npx.waitall</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.load.html">mxnet.npx.load</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.save.html">mxnet.npx.save</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.one_hot.html">mxnet.npx.one_hot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.pick.html">mxnet.npx.pick</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.reshape_like.html">mxnet.npx.reshape_like</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_flatten.html">mxnet.npx.batch_flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.batch_dot.html">mxnet.npx.batch_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.gamma.html">mxnet.npx.gamma</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../npx/generated/mxnet.npx.sequence_mask.html">mxnet.npx.sequence_mask</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">mxnet.gluon</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../block.html">gluon.Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hybrid_block.html">gluon.HybridBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../symbol_block.html">gluon.SymbolBlock</a></li>
<li class="toctree-l3"><a class="reference internal" href="../constant.html">gluon.Constant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../parameter.html">gluon.Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../parameter_dict.html">gluon.ParameterDict</a></li>
<li class="toctree-l3"><a class="reference internal" href="../trainer.html">gluon.Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contrib/index.html">gluon.contrib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data/index.html">gluon.data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../data/vision/index.html">data.vision</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../data/vision/datasets/index.html">vision.datasets</a></li>
<li class="toctree-l5"><a class="reference internal" href="../data/vision/transforms/index.html">vision.transforms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../loss/index.html">gluon.loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../metric/index.html">gluon.metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo/index.html">gluon.model_zoo.vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nn/index.html">gluon.nn</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">gluon.rnn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../utils/index.html">gluon.utils</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../autograd/index.html">mxnet.autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../initializer/index.html">mxnet.initializer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../optimizer/index.html">mxnet.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lr_scheduler/index.html">mxnet.lr_scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore/index.html">mxnet.kvstore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../module/index.html">mxnet.module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../contrib/index.html">mxnet.contrib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/autograd/index.html">contrib.autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/io/index.html">contrib.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/ndarray/index.html">contrib.ndarray</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/onnx/index.html">contrib.onnx</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/quantization/index.html">contrib.quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/symbol/index.html">contrib.symbol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorboard/index.html">contrib.tensorboard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/tensorrt/index.html">contrib.tensorrt</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../contrib/text/index.html">contrib.text</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../legacy/index.html">Legacy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/callback/index.html">mxnet.callback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/image/index.html">mxnet.image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/io/index.html">mxnet.io</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/monitor/index.html">mxnet.monitor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/ndarray/index.html">mxnet.ndarray</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/contrib/index.html">ndarray.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/image/index.html">ndarray.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/linalg/index.html">ndarray.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/op/index.html">ndarray.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/random/index.html">ndarray.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/register/index.html">ndarray.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/sparse/index.html">ndarray.sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/ndarray/utils/index.html">ndarray.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/recordio/index.html">mxnet.recordio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/symbol/index.html">mxnet.symbol</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/symbol.html">symbol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/contrib/index.html">symbol.contrib</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/image/index.html">symbol.image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/linalg/index.html">symbol.linalg</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/op/index.html">symbol.op</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/random/index.html">symbol.random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/register/index.html">symbol.register</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../legacy/symbol/sparse/index.html">symbol.sparse</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../legacy/visualization/index.html">mxnet.visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../context/index.html">mxnet.context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../engine/index.html">mxnet.engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../executor/index.html">mxnet.executor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kvstore_server/index.html">mxnet.kvstore_server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../profiler/index.html">mxnet.profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rtc/index.html">mxnet.rtc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../runtime/index.html">mxnet.runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../test_utils/index.html">mxnet.test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../util/index.html">mxnet.util</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="gluon-rnn">
<h1>gluon.rnn<a class="headerlink" href="#gluon-rnn" title="Permalink to this headline"></a></h1>
<p>Build-in recurrent neural network layers are provided in the following two modules:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#module-mxnet.gluon.rnn" title="mxnet.gluon.rnn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mxnet.gluon.rnn</span></code></a></p></td>
<td><p>Recurrent neural network module.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">mxnet.gluon.contrib.rnn</span></code></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<div class="section" id="recurrent-cells">
<h2>Recurrent Cells<a class="headerlink" href="#recurrent-cells" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMCell" title="mxnet.gluon.rnn.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.LSTMCell</span></code></a></p></td>
<td><p>Long-Short Term Memory (LSTM) network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRUCell" title="mxnet.gluon.rnn.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.GRUCell</span></code></a></p></td>
<td><p>Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.RecurrentCell</span></code></a></p></td>
<td><p>Abstract base class for RNN cells</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMPCell" title="mxnet.gluon.rnn.LSTMPCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.LSTMPCell</span></code></a></p></td>
<td><p>Long-Short Term Memory Projected (LSTMP) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell" title="mxnet.gluon.rnn.SequentialRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.SequentialRNNCell</span></code></a></p></td>
<td><p>Sequentially stacking multiple RNN cells.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.BidirectionalCell" title="mxnet.gluon.rnn.BidirectionalCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.BidirectionalCell</span></code></a></p></td>
<td><p>Bidirectional RNN cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.DropoutCell" title="mxnet.gluon.rnn.DropoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.DropoutCell</span></code></a></p></td>
<td><p>Applies dropout on input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.VariationalDropoutCell" title="mxnet.gluon.rnn.VariationalDropoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.VariationalDropoutCell</span></code></a></p></td>
<td><p>Applies Variational Dropout on base cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ZoneoutCell" title="mxnet.gluon.rnn.ZoneoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.ZoneoutCell</span></code></a></p></td>
<td><p>Applies Zoneout on base cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ResidualCell" title="mxnet.gluon.rnn.ResidualCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.ResidualCell</span></code></a></p></td>
<td><p>Adds residual connection as described in Wu et al, 2016 (<a class="reference external" href="https://arxiv.org/abs/1609.08144">https://arxiv.org/abs/1609.08144</a>).</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="convolutional-recurrent-cells">
<h2>Convolutional Recurrent Cells<a class="headerlink" href="#convolutional-recurrent-cells" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DLSTMCell" title="mxnet.gluon.rnn.Conv1DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv1DLSTMCell</span></code></a></p></td>
<td><p>1D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DLSTMCell" title="mxnet.gluon.rnn.Conv2DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv2DLSTMCell</span></code></a></p></td>
<td><p>2D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DLSTMCell" title="mxnet.gluon.rnn.Conv3DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv3DLSTMCell</span></code></a></p></td>
<td><p>3D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DGRUCell" title="mxnet.gluon.rnn.Conv1DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv1DGRUCell</span></code></a></p></td>
<td><p>1D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DGRUCell" title="mxnet.gluon.rnn.Conv2DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv2DGRUCell</span></code></a></p></td>
<td><p>2D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DGRUCell" title="mxnet.gluon.rnn.Conv3DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv3DGRUCell</span></code></a></p></td>
<td><p>3D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DRNNCell" title="mxnet.gluon.rnn.Conv1DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv1DRNNCell</span></code></a></p></td>
<td><p>1D Convolutional RNN cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DRNNCell" title="mxnet.gluon.rnn.Conv2DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv2DRNNCell</span></code></a></p></td>
<td><p>2D Convolutional RNN cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DRNNCell" title="mxnet.gluon.rnn.Conv3DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.Conv3DRNNCell</span></code></a></p></td>
<td><p>3D Convolutional RNN cells</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="recurrent-layers">
<h2>Recurrent Layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RNN" title="mxnet.gluon.rnn.RNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.RNN</span></code></a></p></td>
<td><p>Applies a multi-layer Elman RNN with <cite>tanh</cite> or <cite>ReLU</cite> non-linearity to an input sequence.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTM" title="mxnet.gluon.rnn.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.LSTM</span></code></a></p></td>
<td><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRU" title="mxnet.gluon.rnn.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rnn.GRU</span></code></a></p></td>
<td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-mxnet.gluon.rnn">
<span id="api-reference"></span><h2>API Reference<a class="headerlink" href="#module-mxnet.gluon.rnn" title="Permalink to this headline"></a></h2>
<p>Recurrent neural network module.</p>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.BidirectionalCell" title="mxnet.gluon.rnn.BidirectionalCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BidirectionalCell</span></code></a>(l_cell,r_cell)</p></td>
<td><p>Bidirectional RNN cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DGRUCell" title="mxnet.gluon.rnn.Conv1DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1DGRUCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>1D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DLSTMCell" title="mxnet.gluon.rnn.Conv1DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1DLSTMCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>1D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv1DRNNCell" title="mxnet.gluon.rnn.Conv1DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1DRNNCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>1D Convolutional RNN cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DGRUCell" title="mxnet.gluon.rnn.Conv2DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2DGRUCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>2D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DLSTMCell" title="mxnet.gluon.rnn.Conv2DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2DLSTMCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>2D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv2DRNNCell" title="mxnet.gluon.rnn.Conv2DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2DRNNCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>2D Convolutional RNN cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DGRUCell" title="mxnet.gluon.rnn.Conv3DGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3DGRUCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>3D Convolutional Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DLSTMCell" title="mxnet.gluon.rnn.Conv3DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3DLSTMCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>3D Convolutional LSTM network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.Conv3DRNNCell" title="mxnet.gluon.rnn.Conv3DRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3DRNNCell</span></code></a>(input_shape,hidden_channels,)</p></td>
<td><p>3D Convolutional RNN cells</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.DropoutCell" title="mxnet.gluon.rnn.DropoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DropoutCell</span></code></a>(rate[,axes])</p></td>
<td><p>Applies dropout on input.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRU" title="mxnet.gluon.rnn.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRU</span></code></a>(hidden_size[,num_layers,layout,])</p></td>
<td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRUCell" title="mxnet.gluon.rnn.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRUCell</span></code></a>(hidden_size[,])</p></td>
<td><p>Gated Rectified Unit (GRU) network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridRecurrentCell" title="mxnet.gluon.rnn.HybridRecurrentCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HybridRecurrentCell</span></code></a>()</p></td>
<td><p>HybridRecurrentCell supports hybridize.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell" title="mxnet.gluon.rnn.HybridSequentialRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HybridSequentialRNNCell</span></code></a>()</p></td>
<td><p>Sequentially stacking multiple HybridRNN cells.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTM" title="mxnet.gluon.rnn.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a>(hidden_size[,num_layers,layout,])</p></td>
<td><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMCell" title="mxnet.gluon.rnn.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>(hidden_size[,])</p></td>
<td><p>Long-Short Term Memory (LSTM) network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMPCell" title="mxnet.gluon.rnn.LSTMPCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTMPCell</span></code></a>(hidden_size,projection_size[,])</p></td>
<td><p>Long-Short Term Memory Projected (LSTMP) network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ModifierCell" title="mxnet.gluon.rnn.ModifierCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ModifierCell</span></code></a>(base_cell)</p></td>
<td><p>Base class for modifier cells.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RNN" title="mxnet.gluon.rnn.RNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RNN</span></code></a>(hidden_size[,num_layers,activation,])</p></td>
<td><p>Applies a multi-layer Elman RNN with <cite>tanh</cite> or <cite>ReLU</cite> non-linearity to an input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RNNCell" title="mxnet.gluon.rnn.RNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RNNCell</span></code></a>(hidden_size[,activation,])</p></td>
<td><p>Elman RNN recurrent neural network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RecurrentCell</span></code></a>()</p></td>
<td><p>Abstract base class for RNN cells</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ResidualCell" title="mxnet.gluon.rnn.ResidualCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ResidualCell</span></code></a>(base_cell)</p></td>
<td><p>Adds residual connection as described in Wu et al, 2016 (<a class="reference external" href="https://arxiv.org/abs/1609.08144">https://arxiv.org/abs/1609.08144</a>).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell" title="mxnet.gluon.rnn.SequentialRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SequentialRNNCell</span></code></a>()</p></td>
<td><p>Sequentially stacking multiple RNN cells.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.VariationalDropoutCell" title="mxnet.gluon.rnn.VariationalDropoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VariationalDropoutCell</span></code></a>(base_cell[,])</p></td>
<td><p>Applies Variational Dropout on base cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ZoneoutCell" title="mxnet.gluon.rnn.ZoneoutCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ZoneoutCell</span></code></a>(base_cell[,zoneout_outputs,])</p></td>
<td><p>Applies Zoneout on base cell.</p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="mxnet.gluon.rnn.BidirectionalCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">BidirectionalCell</code><span class="sig-paren">(</span><em class="sig-param">l_cell</em>, <em class="sig-param">r_cell</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#BidirectionalCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.BidirectionalCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Bidirectional RNN cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>l_cell</strong> (<a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>)  Cell for forward unrolling</p></li>
<li><p><strong>r_cell</strong> (<a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>)  Cell for backward unrolling</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.BidirectionalCell.begin_state" title="mxnet.gluon.rnn.BidirectionalCell.begin_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">begin_state</span></code></a>(**kwargs)</p></td>
<td><p>Initial state for this cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.BidirectionalCell.state_info" title="mxnet.gluon.rnn.BidirectionalCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.BidirectionalCell.unroll" title="mxnet.gluon.rnn.BidirectionalCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.BidirectionalCell.begin_state">
<code class="sig-name descname">begin_state</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#BidirectionalCell.begin_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.BidirectionalCell.begin_state" title="Permalink to this definition"></a></dt>
<dd><p>Initial state for this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em>, </em><em>default symbol.zeros</em>)  <p>Function for creating initial state.</p>
<p>For Symbol API, func can be <cite>symbol.zeros</cite>, <cite>symbol.uniform</cite>,
<cite>symbol.var etc</cite>. Use <cite>symbol.var</cite> if you want to directly
feed input as states.</p>
<p>For NDArray API, func can be <cite>ndarray.zeros</cite>, <cite>ndarray.ones</cite>, etc.</p>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  Only required for NDArray API. Size of the batch (N in layout)
dimension of input.</p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments passed to func. For example
<cite>mean</cite>, <cite>std</cite>, <cite>dtype</cite>, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>states</strong>  Starting states for the first RNN step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nested list of Symbol</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.BidirectionalCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#BidirectionalCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.BidirectionalCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.BidirectionalCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#BidirectionalCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.BidirectionalCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv1DGRUCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv1DGRUCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv1DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv1DGRUCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvGRUCell</span></code></p>
<p>1D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCW the shape should be (C, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCW and NWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in n_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv1DLSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv1DLSTMCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv1DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv1DLSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvLSTMCell</span></code></p>
<p>1D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a> paper. Xingjian et al. NIPS2015</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCW the shape should be (C, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCW and NWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in c^prime_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv1DRNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv1DRNNCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv1DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv1DRNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvRNNCell</span></code></p>
<p>1D Convolutional RNN cell.</p>
<div class="math notranslate nohighlight">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCW the shape should be (C, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>,</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>,</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCW and NWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv2DGRUCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv2DGRUCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv2DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv2DGRUCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvGRUCell</span></code></p>
<p>2D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCHW the shape should be (C, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCHW and NHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in n_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv2DLSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv2DLSTMCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv2DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv2DLSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvLSTMCell</span></code></p>
<p>2D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a> paper. Xingjian et al. NIPS2015</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCHW the shape should be (C, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCHW and NHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in c^prime_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv2DRNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv2DRNNCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv2DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv2DRNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvRNNCell</span></code></p>
<p>2D Convolutional RNN cell.</p>
<div class="math notranslate nohighlight">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCHW the shape should be (C, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCHW and NHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv3DGRUCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv3DGRUCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCDHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv3DGRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv3DGRUCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvGRUCell</span></code></p>
<p>3D Convolutional Gated Rectified Unit (GRU) network cell.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = \sigma(W_r \ast x_t + R_r \ast h_{t-1} + b_r) \\
z_t = \sigma(W_z \ast x_t + R_z \ast h_{t-1} + b_z) \\
n_t = tanh(W_i \ast x_t + b_i + r_t \circ (R_n \ast h_{t-1} + b_n)) \\
h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCDHW the shape should be (C, D, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCDHW and NDHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in n_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv3DLSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv3DLSTMCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCDHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv3DLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv3DLSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvLSTMCell</span></code></p>
<p>3D Convolutional LSTM network cell.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1506.04214">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a> paper. Xingjian et al. NIPS2015</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = \sigma(W_i \ast x_t + R_i \ast h_{t-1} + b_i) \\
f_t = \sigma(W_f \ast x_t + R_f \ast h_{t-1} + b_f) \\
o_t = \sigma(W_o \ast x_t + R_o \ast h_{t-1} + b_o) \\
c^\prime_t = tanh(W_c \ast x_t + R_c \ast h_{t-1} + b_c) \\
c_t = f_t \circ c_{t-1} + i_t \circ c^\prime_t \\
h_t = o_t \circ tanh(c_t) \\
\end{array}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCDHW the shape should be (C, D, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCDHW and NDHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function used in c^prime_t.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.Conv3DRNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">Conv3DRNNCell</code><span class="sig-paren">(</span><em class="sig-param">input_shape</em>, <em class="sig-param">hidden_channels</em>, <em class="sig-param">i2h_kernel</em>, <em class="sig-param">h2h_kernel</em>, <em class="sig-param">i2h_pad=(0</em>, <em class="sig-param">0</em>, <em class="sig-param">0)</em>, <em class="sig-param">i2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">h2h_dilate=(1</em>, <em class="sig-param">1</em>, <em class="sig-param">1)</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">conv_layout='NCDHW'</em>, <em class="sig-param">activation='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/conv_rnn_cell.html#Conv3DRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.Conv3DRNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.conv_rnn_cell._ConvRNNCell</span></code></p>
<p>3D Convolutional RNN cells</p>
<div class="math notranslate nohighlight">
\[h_t = tanh(W_i \ast x_t + R_i \ast h_{t-1} + b_i)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple of int</em>)  Input tensor shape at each time step for each sample, excluding dimension of the batch size
and sequence length. Must be consistent with <cite>conv_layout</cite>.
For example, for layout NCDHW the shape should be (C, D, H, W).</p></li>
<li><p><strong>hidden_channels</strong> (<em>int</em>)  Number of output channels.</p></li>
<li><p><strong>i2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Input convolution kernel sizes.</p></li>
<li><p><strong>h2h_kernel</strong> (<em>int</em><em> or </em><em>tuple of int</em>)  Recurrent convolution kernel sizes. Only odd-numbered sizes are supported.</p></li>
<li><p><strong>i2h_pad</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>0</em><em>, </em><em>0</em><em>, </em><em>0</em><em>)</em>)  Pad for input convolution.</p></li>
<li><p><strong>i2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Input convolution dilate.</p></li>
<li><p><strong>h2h_dilate</strong> (<em>int</em><em> or </em><em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>1</em><em>, </em><em>1</em><em>, </em><em>1</em><em>)</em>)  Recurrent convolution dilate.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the input convolutions.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the input convolutions.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the input convolution bias vectors.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default zeros</em>)  Initializer for the recurrent convolution bias vectors.</p></li>
<li><p><strong>conv_layout</strong> (<em>str</em><em>, </em><em>default 'NCDHW'</em>)  Layout for all convolution inputs, outputs and weights. Options are NCDHW and NDHWC.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../block.html#mxnet.gluon.Block" title="mxnet.gluon.Block"><em>gluon.Block</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function.
If argument type is string, its equivalent to nn.Activation(act_type=str). See
<a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.Activation" title="mxnet.ndarray.Activation"><code class="xref py py-func docutils literal notranslate"><span class="pre">Activation()</span></code></a> for available choices.
Alternatively, other activation blocks such as nn.LeakyReLU can be used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.DropoutCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">DropoutCell</code><span class="sig-paren">(</span><em class="sig-param">rate</em>, <em class="sig-param">axes=()</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#DropoutCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.DropoutCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Applies dropout on input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rate</strong> (<em>float</em>)  Percentage of elements to drop out, which
is 1 - percentage to retain.</p></li>
<li><p><strong>axes</strong> (<em>tuple of int</em><em>, </em><em>default</em><em> (</em><em>)</em>)  The axes on which dropout mask is shared. If empty, regular dropout is applied.</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.DropoutCell.hybrid_forward" title="mxnet.gluon.rnn.DropoutCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.DropoutCell.state_info" title="mxnet.gluon.rnn.DropoutCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.DropoutCell.unroll" title="mxnet.gluon.rnn.DropoutCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(batch_size, size)</cite>.</p></li>
<li><p><strong>states</strong>: a list of recurrent state tensors.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(batch_size, size)</cite>.</p></li>
<li><p><strong>next_states</strong>: returns input <cite>states</cite> directly.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mxnet.gluon.rnn.DropoutCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#DropoutCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.DropoutCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.DropoutCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#DropoutCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.DropoutCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.DropoutCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#DropoutCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.DropoutCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.GRU">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">GRU</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">layout='TNC'</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">input_size=0</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">dtype='float32'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_layer.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.GRU" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_layer._RNNLayer</span></code></p>
<p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.
Note: this is an implementation of the cuDNN version of GRUs
(slight modification compared to Cho et al. 2014; the reset gate <span class="math notranslate nohighlight">\(r_t\)</span>
is applied after matrix multiplication).</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\
h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer,
and <span class="math notranslate nohighlight">\(r_t\)</span>, <span class="math notranslate nohighlight">\(i_t\)</span>, <span class="math notranslate nohighlight">\(n_t\)</span> are the reset, input, and new gates, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  The number of features in the hidden state h</p></li>
<li><p><strong>num_layers</strong> (<em>int</em><em>, </em><em>default 1</em>)  Number of recurrent layers.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>default 'TNC'</em>)  The format of input and output tensors. T, N and C stand for
sequence length, batch size, and feature dimensions respectively.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>default 0</em>)  If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em><em>, </em><em>default False</em>)  If True, becomes a bidirectional RNN.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em>, </em><em>default 'float32'</em>)  Type to initialize the parameters and default states to</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(sequence_length, batch_size, input_size)</cite>
when <cite>layout</cite> is TNC. For other layouts, dimensions are permuted accordingly
using transpose() operator which adds performance overhead. Consider creating
batches in TNC layout during data batching step.</p></li>
<li><p><strong>states</strong>: initial recurrent state tensor with shape
<cite>(num_layers, batch_size, num_hidden)</cite>. If <cite>bidirectional</cite> is True,
shape will instead be <cite>(2*num_layers, batch_size, num_hidden)</cite>. If
<cite>states</cite> is None, zeros will be used as default begin states.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(sequence_length, batch_size, num_hidden)</cite>
when <cite>layout</cite> is TNC. If <cite>bidirectional</cite> is True, output shape will instead
be <cite>(sequence_length, batch_size, 2*num_hidden)</cite></p></li>
<li><p><strong>out_states</strong>: output recurrent state tensor with the same shape as <cite>states</cite>.
If <cite>states</cite> is None <cite>out_states</cite> will not be returned.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># by default zeros are used as begin state</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># manually specify begin state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.GRUCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">GRUCell</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">input_size=0</em>, <em class="sig-param">activation='tanh'</em>, <em class="sig-param">recurrent_activation='sigmoid'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#GRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.GRUCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Gated Rectified Unit (GRU) network cell.
Note: this is an implementation of the cuDNN version of GRUs
(slight modification compared to Cho et al. 2014; the reset gate <span class="math notranslate nohighlight">\(r_t\)</span>
is applied after matrix multiplication).</p>
<p>Each call computes the following function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\
h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\
\end{array}\end{split}\]</div>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRUCell.hybrid_forward" title="mxnet.gluon.rnn.GRUCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states,)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.GRUCell.state_info" title="mxnet.gluon.rnn.GRUCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer,
and <span class="math notranslate nohighlight">\(r_t\)</span>, <span class="math notranslate nohighlight">\(i_t\)</span>, <span class="math notranslate nohighlight">\(n_t\)</span> are the reset, input, and new gates, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  Number of units in output symbol.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em>, </em><em>default 'tanh'</em>)  Activation type to use. See nd/symbol Activation
for supported types.</p></li>
<li><p><strong>recurrent_activation</strong> (<em>str</em><em>, </em><em>default 'sigmoid'</em>)  Activation type to use for the recurrent step. See nd/symbol Activation
for supported types.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</p></li>
<li><p><strong>states</strong>: a list of one initial recurrent state tensor with shape
<cite>(batch_size, num_hidden)</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</p></li>
<li><p><strong>next_states</strong>: a list of one output recurrent state tensor with the
same shape as <cite>states</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mxnet.gluon.rnn.GRUCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em>, <em class="sig-param">i2h_weight</em>, <em class="sig-param">h2h_weight</em>, <em class="sig-param">i2h_bias</em>, <em class="sig-param">h2h_bias</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#GRUCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.GRUCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.GRUCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#GRUCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.GRUCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.HybridRecurrentCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">HybridRecurrentCell</code><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridRecurrentCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridRecurrentCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.RecurrentCell</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.block.HybridBlock</span></code></p>
<p>HybridRecurrentCell supports hybridize.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridRecurrentCell.hybrid_forward" title="mxnet.gluon.rnn.HybridRecurrentCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,x,*args,**kwargs)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.HybridRecurrentCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">x</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridRecurrentCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridRecurrentCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">HybridSequentialRNNCell</code><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Sequentially stacking multiple HybridRNN cells.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.add" title="mxnet.gluon.rnn.HybridSequentialRNNCell.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a>(cell)</p></td>
<td><p>Appends a cell into the stack.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.begin_state" title="mxnet.gluon.rnn.HybridSequentialRNNCell.begin_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">begin_state</span></code></a>(**kwargs)</p></td>
<td><p>Initial state for this cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.hybrid_forward" title="mxnet.gluon.rnn.HybridSequentialRNNCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.state_info" title="mxnet.gluon.rnn.HybridSequentialRNNCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.unroll" title="mxnet.gluon.rnn.HybridSequentialRNNCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">cell</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.add" title="Permalink to this definition"></a></dt>
<dd><p>Appends a cell into the stack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cell</strong> (<a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>)  The cell to add.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell.begin_state">
<code class="sig-name descname">begin_state</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell.begin_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.begin_state" title="Permalink to this definition"></a></dt>
<dd><p>Initial state for this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em>, </em><em>default symbol.zeros</em>)  <p>Function for creating initial state.</p>
<p>For Symbol API, func can be <cite>symbol.zeros</cite>, <cite>symbol.uniform</cite>,
<cite>symbol.var etc</cite>. Use <cite>symbol.var</cite> if you want to directly
feed input as states.</p>
<p>For NDArray API, func can be <cite>ndarray.zeros</cite>, <cite>ndarray.ones</cite>, etc.</p>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  Only required for NDArray API. Size of the batch (N in layout)
dimension of input.</p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments passed to func. For example
<cite>mean</cite>, <cite>std</cite>, <cite>dtype</cite>, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>states</strong>  Starting states for the first RNN step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nested list of Symbol</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.HybridSequentialRNNCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#HybridSequentialRNNCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.HybridSequentialRNNCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.LSTM">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">LSTM</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">layout='TNC'</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">input_size=0</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">projection_size=None</em>, <em class="sig-param">h2r_weight_initializer=None</em>, <em class="sig-param">state_clip_min=None</em>, <em class="sig-param">state_clip_max=None</em>, <em class="sig-param">state_clip_nan=False</em>, <em class="sig-param">dtype='float32'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_layer.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_layer._RNNLayer</span></code></p>
<p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\
o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(c_t\)</span> is the
cell state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the hidden state of the previous
layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer, and <span class="math notranslate nohighlight">\(i_t\)</span>,
<span class="math notranslate nohighlight">\(f_t\)</span>, <span class="math notranslate nohighlight">\(g_t\)</span>, <span class="math notranslate nohighlight">\(o_t\)</span> are the input, forget, cell, and
out gates, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  The number of features in the hidden state h.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em><em>, </em><em>default 1</em>)  Number of recurrent layers.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>default 'TNC'</em>)  The format of input and output tensors. T, N and C stand for
sequence length, batch size, and feature dimensions respectively.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>default 0</em>)  If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer.</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em><em>, </em><em>default False</em>)  If <cite>True</cite>, becomes a bidirectional RNN.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'lstmbias'</em>)  Initializer for the bias vector. By default, bias for the forget
gate is initialized to 1 while all other biases are initialized
to zero.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>projection_size</strong> (<em>int</em><em>, </em><em>default None</em>)  The number of features after projection.</p></li>
<li><p><strong>h2r_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default None</em>)  Initializer for the projected recurrent weights matrix, used for the linear
transformation of the recurrent state to the projected space.</p></li>
<li><p><strong>state_clip_min</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>default None</em>)  Minimum clip value of LSTM states. This option must be used together with
state_clip_max. If None, clipping is not applied.</p></li>
<li><p><strong>state_clip_max</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>default None</em>)  Maximum clip value of LSTM states. This option must be used together with
state_clip_min. If None, clipping is not applied.</p></li>
<li><p><strong>state_clip_nan</strong> (<em>boolean</em><em>, </em><em>default False</em>)  Whether to stop NaN from propagating in state by clipping it to min/max.
If the clipping range is not specified, this option is ignored.</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em>, </em><em>default 'float32'</em>)  Type to initialize the parameters and default states to</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(sequence_length, batch_size, input_size)</cite>
when <cite>layout</cite> is TNC. For other layouts, dimensions are permuted accordingly
using transpose() operator which adds performance overhead. Consider creating
batches in TNC layout during data batching step.</p></li>
<li><p><strong>states</strong>: a list of two initial recurrent state tensors. Each has shape
<cite>(num_layers, batch_size, num_hidden)</cite>. If <cite>bidirectional</cite> is True,
shape will instead be <cite>(2*num_layers, batch_size, num_hidden)</cite>. If
<cite>states</cite> is None, zeros will be used as default begin states.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(sequence_length, batch_size, num_hidden)</cite>
when <cite>layout</cite> is TNC. If <cite>bidirectional</cite> is True, output shape will instead
be <cite>(sequence_length, batch_size, 2*num_hidden)</cite></p></li>
<li><p><strong>out_states</strong>: a list of two output recurrent state tensors with the same
shape as in <cite>states</cite>. If <cite>states</cite> is None <cite>out_states</cite> will not be returned.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># by default zeros are used as begin state</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># manually specify begin state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">[</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.LSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">LSTMCell</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">input_size=0</em>, <em class="sig-param">activation='tanh'</em>, <em class="sig-param">recurrent_activation='sigmoid'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Long-Short Term Memory (LSTM) network cell.</p>
<p>Each call computes the following function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\
o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}\]</div>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMCell.hybrid_forward" title="mxnet.gluon.rnn.LSTMCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states,)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMCell.state_info" title="mxnet.gluon.rnn.LSTMCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(c_t\)</span> is the
cell state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the hidden state of the previous
layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer, and <span class="math notranslate nohighlight">\(i_t\)</span>,
<span class="math notranslate nohighlight">\(f_t\)</span>, <span class="math notranslate nohighlight">\(g_t\)</span>, <span class="math notranslate nohighlight">\(o_t\)</span> are the input, forget, cell, and
out gates, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  Number of units in output symbol.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
<li><p><strong>activation</strong> (<em>str</em><em>, </em><em>default 'tanh'</em>)  Activation type to use. See nd/symbol Activation
for supported types.</p></li>
<li><p><strong>recurrent_activation</strong> (<em>str</em><em>, </em><em>default 'sigmoid'</em>)  Activation type to use for the recurrent step. See nd/symbol Activation
for supported types.</p></li>
<li><p><strong>Inputs</strong>  <ul>
<li><p><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</p></li>
<li><p><strong>states</strong>: a list of two initial recurrent state tensors. Each has shape
<cite>(batch_size, num_hidden)</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>Outputs</strong>  <ul>
<li><p><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</p></li>
<li><p><strong>next_states</strong>: a list of two output recurrent state tensors. Each has
the same shape as <cite>states</cite>.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mxnet.gluon.rnn.LSTMCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em>, <em class="sig-param">i2h_weight</em>, <em class="sig-param">h2h_weight</em>, <em class="sig-param">i2h_bias</em>, <em class="sig-param">h2h_bias</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.LSTMCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.LSTMPCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">LSTMPCell</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">projection_size</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">h2r_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">input_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMPCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMPCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Long-Short Term Memory Projected (LSTMP) network cell.
(<a class="reference external" href="https://arxiv.org/abs/1402.1128">https://arxiv.org/abs/1402.1128</a>)</p>
<p>Each call computes the following function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{ri} r_{(t-1)} + b_{ri}) \\
f_t = sigmoid(W_{if} x_t + b_{if} + W_{rf} r_{(t-1)} + b_{rf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{rc} r_{(t-1)} + b_{rg}) \\
o_t = sigmoid(W_{io} x_t + b_{io} + W_{ro} r_{(t-1)} + b_{ro}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t) \\
r_t = W_{hr} h_t
\end{array}\end{split}\]</div>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMPCell.hybrid_forward" title="mxnet.gluon.rnn.LSTMPCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states,)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.LSTMPCell.state_info" title="mxnet.gluon.rnn.LSTMPCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(r_t\)</span> is the projected recurrent activation at time <cite>t</cite>,
<span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(c_t\)</span> is the
cell state at time <cite>t</cite>, <span class="math notranslate nohighlight">\(x_t\)</span> is the input at time <cite>t</cite>, and <span class="math notranslate nohighlight">\(i_t\)</span>,
<span class="math notranslate nohighlight">\(f_t\)</span>, <span class="math notranslate nohighlight">\(g_t\)</span>, <span class="math notranslate nohighlight">\(o_t\)</span> are the input, forget, cell, and
out gates, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  Number of units in cell state symbol.</p></li>
<li><p><strong>projection_size</strong> (<em>int</em>)  Number of units in output symbol.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the hidden state.</p></li>
<li><p><strong>h2r_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the projection weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'lstmbias'</em>)  Initializer for the bias vector. By default, bias for the forget
gate is initialized to 1 while all other biases are initialized
to zero.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>Inputs</strong>  <ul>
<li><p><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</p></li>
<li><p><strong>states</strong>: a list of two initial recurrent state tensors, with shape
<cite>(batch_size, projection_size)</cite> and <cite>(batch_size, hidden_size)</cite> respectively.</p></li>
</ul>
</p></li>
<li><p><strong>Outputs</strong>  <ul>
<li><p><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</p></li>
<li><p><strong>next_states</strong>: a list of two output recurrent state tensors. Each has
the same shape as <cite>states</cite>.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mxnet.gluon.rnn.LSTMPCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em>, <em class="sig-param">i2h_weight</em>, <em class="sig-param">h2h_weight</em>, <em class="sig-param">h2r_weight</em>, <em class="sig-param">i2h_bias</em>, <em class="sig-param">h2h_bias</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMPCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMPCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.LSTMPCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#LSTMPCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.LSTMPCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.ModifierCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">ModifierCell</code><span class="sig-paren">(</span><em class="sig-param">base_cell</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ModifierCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ModifierCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Base class for modifier cells. A modifier
cell takes a base cell, apply modifications
on it (e.g. Zoneout), and returns a new cell.</p>
<p>After applying modifiers the base cell should
no longer be called directly. The modifier cell
should be used instead.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ModifierCell.begin_state" title="mxnet.gluon.rnn.ModifierCell.begin_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">begin_state</span></code></a>([func])</p></td>
<td><p>Initial state for this cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ModifierCell.hybrid_forward" title="mxnet.gluon.rnn.ModifierCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ModifierCell.state_info" title="mxnet.gluon.rnn.ModifierCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ModifierCell.params" title="mxnet.gluon.rnn.ModifierCell.params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">params</span></code></a></p></td>
<td><p>Returns this <code class="xref py py-class docutils literal notranslate"><span class="pre">Block</span></code>s parameter dictionary (does not include its childrens parameters).</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.ModifierCell.begin_state">
<code class="sig-name descname">begin_state</code><span class="sig-paren">(</span><em class="sig-param">func=&lt;function zeros&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ModifierCell.begin_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ModifierCell.begin_state" title="Permalink to this definition"></a></dt>
<dd><p>Initial state for this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em>, </em><em>default symbol.zeros</em>)  <p>Function for creating initial state.</p>
<p>For Symbol API, func can be <cite>symbol.zeros</cite>, <cite>symbol.uniform</cite>,
<cite>symbol.var etc</cite>. Use <cite>symbol.var</cite> if you want to directly
feed input as states.</p>
<p>For NDArray API, func can be <cite>ndarray.zeros</cite>, <cite>ndarray.ones</cite>, etc.</p>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  Only required for NDArray API. Size of the batch (N in layout)
dimension of input.</p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments passed to func. For example
<cite>mean</cite>, <cite>std</cite>, <cite>dtype</cite>, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>states</strong>  Starting states for the first RNN step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nested list of Symbol</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.ModifierCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ModifierCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ModifierCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.ModifierCell.params">
<em class="property">property </em><code class="sig-name descname">params</code><a class="headerlink" href="#mxnet.gluon.rnn.ModifierCell.params" title="Permalink to this definition"></a></dt>
<dd><p>Returns this <code class="xref py py-class docutils literal notranslate"><span class="pre">Block</span></code>s parameter dictionary (does not include its
childrens parameters).</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.ModifierCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ModifierCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ModifierCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.RNN">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">RNN</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">activation='relu'</em>, <em class="sig-param">layout='TNC'</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">input_size=0</em>, <em class="sig-param">dtype='float32'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_layer.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RNN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_layer._RNNLayer</span></code></p>
<p>Applies a multi-layer Elman RNN with <cite>tanh</cite> or <cite>ReLU</cite> non-linearity to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math notranslate nohighlight">
\[h_t = \tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\]</div>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, and <span class="math notranslate nohighlight">\(x_t\)</span> is the output
of the previous layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer.
If nonlinearity=relu, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  The number of features in the hidden state h.</p></li>
<li><p><strong>num_layers</strong> (<em>int</em><em>, </em><em>default 1</em>)  Number of recurrent layers.</p></li>
<li><p><strong>activation</strong> (<em>{'relu'</em><em> or </em><em>'tanh'}</em><em>, </em><em>default 'relu'</em>)  The activation function to use.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>default 'TNC'</em>)  The format of input and output tensors. T, N and C stand for
sequence length, batch size, and feature dimensions respectively.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>default 0</em>)  If non-zero, introduces a dropout layer on the outputs of each
RNN layer except the last layer.</p></li>
<li><p><strong>bidirectional</strong> (<em>bool</em><em>, </em><em>default False</em>)  If <cite>True</cite>, becomes a bidirectional RNN.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the bias vector.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em>, </em><em>default 'float32'</em>)  Type to initialize the parameters and default states to</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(sequence_length, batch_size, input_size)</cite>
when <cite>layout</cite> is TNC. For other layouts, dimensions are permuted accordingly
using transpose() operator which adds performance overhead. Consider creating
batches in TNC layout during data batching step.</p></li>
<li><p><strong>states</strong>: initial recurrent state tensor with shape
<cite>(num_layers, batch_size, num_hidden)</cite>. If <cite>bidirectional</cite> is True,
shape will instead be <cite>(2*num_layers, batch_size, num_hidden)</cite>. If
<cite>states</cite> is None, zeros will be used as default begin states.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(sequence_length, batch_size, num_hidden)</cite>
when <cite>layout</cite> is TNC. If <cite>bidirectional</cite> is True, output shape will instead
be <cite>(sequence_length, batch_size, 2*num_hidden)</cite></p></li>
<li><p><strong>out_states</strong>: output recurrent state tensor with the same shape as <cite>states</cite>.
If <cite>states</cite> is None <cite>out_states</cite> will not be returned.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># by default zeros are used as begin state</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># manually specify begin state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.RNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">RNNCell</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">activation='tanh'</em>, <em class="sig-param">i2h_weight_initializer=None</em>, <em class="sig-param">h2h_weight_initializer=None</em>, <em class="sig-param">i2h_bias_initializer='zeros'</em>, <em class="sig-param">h2h_bias_initializer='zeros'</em>, <em class="sig-param">input_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.HybridRecurrentCell</span></code></p>
<p>Elman RNN recurrent neural network cell.</p>
<p>Each call computes the following function:</p>
<div class="math notranslate nohighlight">
\[h_t = \tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\]</div>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RNNCell.hybrid_forward" title="mxnet.gluon.rnn.RNNCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states,)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RNNCell.state_info" title="mxnet.gluon.rnn.RNNCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(h_t\)</span> is the hidden state at time <cite>t</cite>, and <span class="math notranslate nohighlight">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math notranslate nohighlight">\(input_t\)</span> for the first layer.
If nonlinearity=relu, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<em>int</em>)  Number of units in output symbol</p></li>
<li><p><strong>activation</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>default 'tanh'</em>)  Type of activation function.</p></li>
<li><p><strong>i2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the input weights matrix, used for the linear
transformation of the inputs.</p></li>
<li><p><strong>h2h_weight_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a>)  Initializer for the recurrent weights matrix, used for the linear
transformation of the recurrent state.</p></li>
<li><p><strong>i2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>h2h_bias_initializer</strong> (<em>str</em><em> or </em><a class="reference internal" href="../../initializer/index.html#mxnet.initializer.Initializer" title="mxnet.initializer.Initializer"><em>Initializer</em></a><em>, </em><em>default 'zeros'</em>)  Initializer for the bias vector.</p></li>
<li><p><strong>input_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  The number of expected features in the input x.
If not specified, it will be inferred from input.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong>: input tensor with shape <cite>(batch_size, input_size)</cite>.</p></li>
<li><p><strong>states</strong>: a list of one initial recurrent state tensor with shape
<cite>(batch_size, num_hidden)</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>out</strong>: output tensor with shape <cite>(batch_size, num_hidden)</cite>.</p></li>
<li><p><strong>next_states</strong>: a list of one output recurrent state tensor with the
same shape as <cite>states</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mxnet.gluon.rnn.RNNCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em>, <em class="sig-param">i2h_weight</em>, <em class="sig-param">h2h_weight</em>, <em class="sig-param">i2h_bias</em>, <em class="sig-param">h2h_bias</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RNNCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RNNCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.RNNCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RNNCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RNNCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.RecurrentCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">RecurrentCell</code><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.block.Block</span></code></p>
<p>Abstract base class for RNN cells</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.begin_state" title="mxnet.gluon.rnn.RecurrentCell.begin_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">begin_state</span></code></a>([batch_size,func])</p></td>
<td><p>Initial state for this cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.forward" title="mxnet.gluon.rnn.RecurrentCell.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(inputs,states)</p></td>
<td><p>Unrolls the recurrent cell for one time step.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.reset" title="mxnet.gluon.rnn.RecurrentCell.reset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset</span></code></a>()</p></td>
<td><p>Reset before re-using the cell for another graph.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.state_info" title="mxnet.gluon.rnn.RecurrentCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.unroll" title="mxnet.gluon.rnn.RecurrentCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.RecurrentCell.begin_state">
<code class="sig-name descname">begin_state</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em>, <em class="sig-param">func=&lt;function zeros&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell.begin_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell.begin_state" title="Permalink to this definition"></a></dt>
<dd><p>Initial state for this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em>, </em><em>default symbol.zeros</em>)  <p>Function for creating initial state.</p>
<p>For Symbol API, func can be <cite>symbol.zeros</cite>, <cite>symbol.uniform</cite>,
<cite>symbol.var etc</cite>. Use <cite>symbol.var</cite> if you want to directly
feed input as states.</p>
<p>For NDArray API, func can be <cite>ndarray.zeros</cite>, <cite>ndarray.ones</cite>, etc.</p>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  Only required for NDArray API. Size of the batch (N in layout)
dimension of input.</p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments passed to func. For example
<cite>mean</cite>, <cite>std</cite>, <cite>dtype</cite>, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>states</strong>  Starting states for the first RNN step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nested list of Symbol</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.RecurrentCell.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell.forward" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls the recurrent cell for one time step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>sym.Variable</em>)  Input symbol, 2D, of shape (batch_size * num_units).</p></li>
<li><p><strong>states</strong> (<em>list of sym.Variable</em>)  RNN state from previous step or the output of begin_state().</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>output</strong> (<em>Symbol</em>)  Symbol corresponding to the output from the RNN when unrolling
for a single time step.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.
This can be used as an input state to the next time step
of this RNN.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.begin_state" title="mxnet.gluon.rnn.RecurrentCell.begin_state"><code class="xref py py-meth docutils literal notranslate"><span class="pre">begin_state()</span></code></a></dt><dd><p>This function can provide the states for the first time step.</p>
</dd>
<dt><a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell.unroll" title="mxnet.gluon.rnn.RecurrentCell.unroll"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unroll()</span></code></a></dt><dd><p>This function unrolls an RNN for a given number of (&gt;=1) time steps.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.RecurrentCell.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell.reset" title="Permalink to this definition"></a></dt>
<dd><p>Reset before re-using the cell for another graph.</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.RecurrentCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.RecurrentCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#RecurrentCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.RecurrentCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.ResidualCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">ResidualCell</code><span class="sig-paren">(</span><em class="sig-param">base_cell</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ResidualCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ResidualCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.ModifierCell</span></code></p>
<p>Adds residual connection as described in Wu et al, 2016
(<a class="reference external" href="https://arxiv.org/abs/1609.08144">https://arxiv.org/abs/1609.08144</a>).
Output of the cell is output of the base cell plus input.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ResidualCell.hybrid_forward" title="mxnet.gluon.rnn.ResidualCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ResidualCell.unroll" title="mxnet.gluon.rnn.ResidualCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.ResidualCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ResidualCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ResidualCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.ResidualCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ResidualCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ResidualCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.SequentialRNNCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">SequentialRNNCell</code><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#SequentialRNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.SequentialRNNCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.RecurrentCell</span></code></p>
<p>Sequentially stacking multiple RNN cells.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell.add" title="mxnet.gluon.rnn.SequentialRNNCell.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a>(cell)</p></td>
<td><p>Appends a cell into the stack.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell.begin_state" title="mxnet.gluon.rnn.SequentialRNNCell.begin_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">begin_state</span></code></a>(**kwargs)</p></td>
<td><p>Initial state for this cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell.state_info" title="mxnet.gluon.rnn.SequentialRNNCell.state_info"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_info</span></code></a>([batch_size])</p></td>
<td><p>shape and layout information of states</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.SequentialRNNCell.unroll" title="mxnet.gluon.rnn.SequentialRNNCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.SequentialRNNCell.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">cell</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#SequentialRNNCell.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.SequentialRNNCell.add" title="Permalink to this definition"></a></dt>
<dd><p>Appends a cell into the stack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cell</strong> (<a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>)  The cell to add.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.SequentialRNNCell.begin_state">
<code class="sig-name descname">begin_state</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#SequentialRNNCell.begin_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.SequentialRNNCell.begin_state" title="Permalink to this definition"></a></dt>
<dd><p>Initial state for this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em>, </em><em>default symbol.zeros</em>)  <p>Function for creating initial state.</p>
<p>For Symbol API, func can be <cite>symbol.zeros</cite>, <cite>symbol.uniform</cite>,
<cite>symbol.var etc</cite>. Use <cite>symbol.var</cite> if you want to directly
feed input as states.</p>
<p>For NDArray API, func can be <cite>ndarray.zeros</cite>, <cite>ndarray.ones</cite>, etc.</p>
</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default 0</em>)  Only required for NDArray API. Size of the batch (N in layout)
dimension of input.</p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments passed to func. For example
<cite>mean</cite>, <cite>std</cite>, <cite>dtype</cite>, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>states</strong>  Starting states for the first RNN step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>nested list of Symbol</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.SequentialRNNCell.state_info">
<code class="sig-name descname">state_info</code><span class="sig-paren">(</span><em class="sig-param">batch_size=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#SequentialRNNCell.state_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.SequentialRNNCell.state_info" title="Permalink to this definition"></a></dt>
<dd><p>shape and layout information of states</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.SequentialRNNCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#SequentialRNNCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.SequentialRNNCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.VariationalDropoutCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">VariationalDropoutCell</code><span class="sig-paren">(</span><em class="sig-param">base_cell</em>, <em class="sig-param">drop_inputs=0.0</em>, <em class="sig-param">drop_states=0.0</em>, <em class="sig-param">drop_outputs=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#VariationalDropoutCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.VariationalDropoutCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.ModifierCell</span></code></p>
<p>Applies Variational Dropout on base cell.
<a class="reference external" href="https://arxiv.org/pdf/1512.05287.pdf">https://arxiv.org/pdf/1512.05287.pdf</a></p>
<p>Variational dropout uses the same dropout mask across time-steps. It can be applied to RNN
inputs, outputs, and states. The masks for them are not shared.</p>
<p>The dropout mask is initialized when stepping forward for the first time and will remain
the same until .reset() is called. Thus, if using the cell and stepping manually without calling
.unroll(), the .reset() should be called after each sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_cell</strong> (<a class="reference internal" href="#mxnet.gluon.rnn.RecurrentCell" title="mxnet.gluon.rnn.RecurrentCell"><em>RecurrentCell</em></a>)  The cell on which to perform variational dropout.</p></li>
<li><p><strong>drop_inputs</strong> (<em>float</em><em>, </em><em>default 0.</em>)  The dropout rate for inputs. Wont apply dropout if it equals 0.</p></li>
<li><p><strong>drop_states</strong> (<em>float</em><em>, </em><em>default 0.</em>)  The dropout rate for state inputs on the first state channel.
Wont apply dropout if it equals 0.</p></li>
<li><p><strong>drop_outputs</strong> (<em>float</em><em>, </em><em>default 0.</em>)  The dropout rate for outputs. Wont apply dropout if it equals 0.</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.VariationalDropoutCell.hybrid_forward" title="mxnet.gluon.rnn.VariationalDropoutCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.VariationalDropoutCell.reset" title="mxnet.gluon.rnn.VariationalDropoutCell.reset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset</span></code></a>()</p></td>
<td><p>Reset before re-using the cell for another graph.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.VariationalDropoutCell.unroll" title="mxnet.gluon.rnn.VariationalDropoutCell.unroll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unroll</span></code></a>(length,inputs[,begin_state,])</p></td>
<td><p>Unrolls an RNN cell across time steps.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.VariationalDropoutCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#VariationalDropoutCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.VariationalDropoutCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.VariationalDropoutCell.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#VariationalDropoutCell.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.VariationalDropoutCell.reset" title="Permalink to this definition"></a></dt>
<dd><p>Reset before re-using the cell for another graph.</p>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.VariationalDropoutCell.unroll">
<code class="sig-name descname">unroll</code><span class="sig-paren">(</span><em class="sig-param">length</em>, <em class="sig-param">inputs</em>, <em class="sig-param">begin_state=None</em>, <em class="sig-param">layout='NTC'</em>, <em class="sig-param">merge_outputs=None</em>, <em class="sig-param">valid_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#VariationalDropoutCell.unroll"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.VariationalDropoutCell.unroll" title="Permalink to this definition"></a></dt>
<dd><p>Unrolls an RNN cell across time steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>length</strong> (<em>int</em>)  Number of steps to unroll.</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><em>list of Symbol</em><em>, or </em><em>None</em>)  <p>If <cite>inputs</cite> is a single Symbol (usually the output
of Embedding symbol), it should have shape
(batch_size, length, ) if <cite>layout</cite> is NTC,
or (length, batch_size, ) if <cite>layout</cite> is TNC.</p>
<p>If <cite>inputs</cite> is a list of symbols (usually output of
previous unroll), they should all have shape
(batch_size, ).</p>
</p></li>
<li><p><strong>begin_state</strong> (<em>nested list of Symbol</em><em>, </em><em>optional</em>)  Input states created by <cite>begin_state()</cite>
or output state of another cell.
Created from <cite>begin_state()</cite> if <cite>None</cite>.</p></li>
<li><p><strong>layout</strong> (<em>str</em><em>, </em><em>optional</em>)  <cite>layout</cite> of input symbol. Only used if inputs
is a single Symbol.</p></li>
<li><p><strong>merge_outputs</strong> (<em>bool</em><em>, </em><em>optional</em>)  If <cite>False</cite>, returns outputs as a list of Symbols.
If <cite>True</cite>, concatenates output across time steps
and returns a single symbol with shape
(batch_size, length, ) if layout is NTC,
or (length, batch_size, ) if layout is TNC.
If <cite>None</cite>, output whatever is faster.</p></li>
<li><p><strong>valid_length</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em>, </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a><em> or </em><em>None</em>)  <cite>valid_length</cite> specifies the length of the sequences in the batch without padding.
This option is especially useful for building sequence-to-sequence models where
the input and output sequences would potentially be padded.
If <cite>valid_length</cite> is None, all sequences are assumed to have the same length.
If <cite>valid_length</cite> is a Symbol or NDArray, it should have shape (batch_size,).
The ith element will be the length of the ith sequence in the batch.
The last valid state will be return and the padded outputs will be masked with 0.
Note that <cite>valid_length</cite> must be smaller or equal to <cite>length</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>outputs</strong> (<em>list of Symbol or Symbol</em>)  Symbol (if <cite>merge_outputs</cite> is True) or list of Symbols
(if <cite>merge_outputs</cite> is False) corresponding to the output from
the RNN from this unrolling.</p></li>
<li><p><strong>states</strong> (<em>list of Symbol</em>)  The new state of this RNN after this unrolling.
The type of this symbol is same as the output of <cite>begin_state()</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mxnet.gluon.rnn.ZoneoutCell">
<em class="property">class </em><code class="sig-prename descclassname">mxnet.gluon.rnn.</code><code class="sig-name descname">ZoneoutCell</code><span class="sig-paren">(</span><em class="sig-param">base_cell</em>, <em class="sig-param">zoneout_outputs=0.0</em>, <em class="sig-param">zoneout_states=0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ZoneoutCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ZoneoutCell" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mxnet.gluon.rnn.rnn_cell.ModifierCell</span></code></p>
<p>Applies Zoneout on base cell.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ZoneoutCell.hybrid_forward" title="mxnet.gluon.rnn.ZoneoutCell.hybrid_forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hybrid_forward</span></code></a>(F,inputs,states)</p></td>
<td><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#mxnet.gluon.rnn.ZoneoutCell.reset" title="mxnet.gluon.rnn.ZoneoutCell.reset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset</span></code></a>()</p></td>
<td><p>Reset before re-using the cell for another graph.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="mxnet.gluon.rnn.ZoneoutCell.hybrid_forward">
<code class="sig-name descname">hybrid_forward</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">inputs</em>, <em class="sig-param">states</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ZoneoutCell.hybrid_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ZoneoutCell.hybrid_forward" title="Permalink to this definition"></a></dt>
<dd><p>Overrides to construct symbolic graph for this <cite>Block</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="../../legacy/symbol/symbol.html#mxnet.symbol.Symbol" title="mxnet.symbol.Symbol"><em>Symbol</em></a><em> or </em><a class="reference internal" href="../../legacy/ndarray/ndarray.html#mxnet.ndarray.NDArray" title="mxnet.ndarray.NDArray"><em>NDArray</em></a>)  The first input tensor.</p></li>
<li><p><strong>*args</strong> (<em>list of Symbol</em><em> or </em><em>list of NDArray</em>)  Additional input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mxnet.gluon.rnn.ZoneoutCell.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mxnet/gluon/rnn/rnn_cell.html#ZoneoutCell.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mxnet.gluon.rnn.ZoneoutCell.reset" title="Permalink to this definition"></a></dt>
<dd><p>Reset before re-using the cell for another graph.</p>
</dd></dl>

</dd></dl>

</div>
</div>


        <hr class="feedback-hr-top" />
<div class="feedback-container">
    <div class="feedback-question">Did this page help you?</div>
    <div class="feedback-answer-container">
        <div class="feedback-answer yes-link" data-response="yes">Yes</div>
        <div class="feedback-answer no-link" data-response="no">No</div>
    </div>
    <div class="feedback-thank-you">Thanks for your feedback!</div>
</div>
<hr class="feedback-hr-bottom" />
        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">gluon.rnn</a><ul>
<li><a class="reference internal" href="#recurrent-cells">Recurrent Cells</a></li>
<li><a class="reference internal" href="#convolutional-recurrent-cells">Convolutional Recurrent Cells</a></li>
<li><a class="reference internal" href="#recurrent-layers">Recurrent Layers</a></li>
<li><a class="reference internal" href="#module-mxnet.gluon.rnn">API Reference</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>                    

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../nn/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>gluon.nn</div>
         </div>
     </a>
     <a id="button-next" href="../utils/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>gluon.utils</div>
        </div>
     </a>
  </div>
            <footer class="site-footer h-card">
    <div class="wrapper">
        <div class="row">
            <div class="col-4">
                <h4 class="footer-category-title">Resources</h4>
                <ul class="contact-list">
                    <li><a
                            href="https://lists.apache.org/list.html?dev@mxnet.apache.org">Mailing list</a> <a class="u-email" href="mailto:dev-subscribe@mxnet.apache.org">(subscribe)</a></li>
                    <li><a href="https://discuss.mxnet.io">MXNet Discuss forum</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/issues">Github Issues</a></li>
                    <li><a href="https://github.com/apache/incubator-mxnet/projects">Projects</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+Home">Developer Wiki</a></li>
                    <li><a href="/community">Contribute To MXNet</a></li>

                </ul>
            </div>

            <div class="col-4"><ul class="social-media-list"><li><a href="https://github.com/apache/incubator-mxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#github"></use></svg> <span class="username">apache/incubator-mxnet</span></a></li><li><a href="https://www.twitter.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#twitter"></use></svg> <span class="username">apachemxnet</span></a></li><li><a href="https://youtube.com/apachemxnet"><svg class="svg-icon"><use xlink:href="../../../_static/minima-social-icons.svg#youtube"></use></svg> <span class="username">apachemxnet</span></a></li></ul>
</div>

            <div class="col-4 footer-text">
                <p>A flexible and efficient library for deep learning.</p>
            </div>
        </div>
    </div>
</footer>

<footer class="site-footer2">
    <div class="wrapper">
        <div class="row">
            <div class="col-3">
                <img src="../../../_static/apache_incubator_logo.png" class="footer-logo col-2">
            </div>
            <div class="footer-bottom-warning col-9">
                <p>Apache MXNet is an effort undergoing incubation at The Apache Software Foundation (ASF), <span style="font-weight:bold">sponsored by the <i>Apache Incubator</i></span>. Incubation is required
                    of all newly accepted projects until a further review indicates that the infrastructure,
                    communications, and decision making process have stabilized in a manner consistent with other
                    successful ASF projects. While incubation status is not necessarily a reflection of the completeness
                    or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
                </p><p>"Copyright  2017-2018, The Apache Software Foundation Apache MXNet, MXNet, Apache, the Apache
                    feather, and the Apache MXNet project logo are either registered trademarks or trademarks of the
                    Apache Software Foundation."</p>
            </div>
        </div>
    </div>
</footer>
        
  </body>
</html>